<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.7.3">Jekyll</generator><link href="https://www.lytzen.name/feed.xml" rel="self" type="application/atom+xml" /><link href="https://www.lytzen.name/" rel="alternate" type="text/html" /><updated>2019-05-28T12:53:37+01:00</updated><id>https://www.lytzen.name/</id><title type="html">Frans’ Randomness</title><subtitle>My very infrequent thoughts on the world of software development</subtitle><entry><title type="html">Find SELECT N+1 with Application Insights</title><link href="https://www.lytzen.name/2019/05/20/find-select-nplus1-with-app-insights.html" rel="alternate" type="text/html" title="Find SELECT N+1 with Application Insights" /><published>2019-05-20T00:00:00+01:00</published><updated>2019-05-20T00:00:00+01:00</updated><id>https://www.lytzen.name/2019/05/20/find-select-nplus1-with-app-insights</id><content type="html" xml:base="https://www.lytzen.name/2019/05/20/find-select-nplus1-with-app-insights.html">&lt;p&gt;ORMs like Entity Framework are good for many things - but they also make it easy to write “SELECT N+1 queries”. This is when you thought you just did one SQL call but you make one call to retrieve the list of records and then (at least) one more call for each row in the resultset, because you loop over the data and inadvertently auto-expand a property. You rarely spot it in development and even in production it is sometimes hard to see; It may well be that a particular operation causes your code to call SQL 500 times, but that may take less than a second so the operation may not even show up at the top of your “performance offenders” list. But, it’s still a lot of traffic to send to SQL and, combined with the fact that SELECT N+1 problems are usually very easy to fix, it’s worthwhile hunting them down and sorting them out.&lt;/p&gt;

&lt;p&gt;By default Application Insights log every Request made to your web/api server and log every &lt;em&gt;dependency call&lt;/em&gt;, such as database and micro service call you do. It connects the two together by giving the dependency call a &lt;code class=&quot;highlighter-rouge&quot;&gt;operation_ParentId&lt;/code&gt; that is the same as the &lt;code class=&quot;highlighter-rouge&quot;&gt;id&lt;/code&gt; of the request.
In short, you can join the &lt;code class=&quot;highlighter-rouge&quot;&gt;requests&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;dependencies&lt;/code&gt; to list each web/api request and show how many SQL calls it generated and how long they took to run in aggregate like this:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;requests 
| project name, id, duration
| join kind = leftouter (
   dependencies 
   | where type  == 'SQL'
   | project sqlid = id, operation_ParentId , sqlduration = duration 
) on $left.id == $right.operation_ParentId 
| summarize sqlopscount = countif(isnotempty(sqlid)), sqlopsduration = sum(sqlduration) by name, id, duration
| order by sqlopscount  desc 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note that this query will show each individual request made. You may want to roll that up further to see the &lt;em&gt;type&lt;/em&gt; of call - but I find that quite often it is helpful to know the exact call to make it easier to reproduce it.&lt;br /&gt;
I am currently working through a legacy app that has a lot of these kind of problems; I look at the result of this query and then set to work fixing them. It’s a very easy way to find major causes of stress on the database.&lt;/p&gt;

&lt;h3 id=&quot;cosmosdb&quot;&gt;CosmosDb&lt;/h3&gt;
&lt;p&gt;You can use the same query, just change &lt;code class=&quot;highlighter-rouge&quot;&gt;type&lt;/code&gt; to “Azure DocumentDB”.&lt;/p&gt;

&lt;h3 id=&quot;what-about-other-dependencies---or-if-my-sql-calls-are-not-logged&quot;&gt;What about other dependencies - or if my SQL calls are not logged?&lt;/h3&gt;
&lt;p&gt;When running ASP.Net (Core or otherwise) on Azure Web Apps, your SQL calls are automatically logged as dependencies. In other scenarios, this may not happen automatically. You may need to install certain extensions to pick them up - or you may have to write your own. Incidentally, writing your own can also be very helpful if you want to do the same statistics for something other than SQL.&lt;br /&gt;
In short, you can write code to wrap dependency operations in calls to &lt;code class=&quot;highlighter-rouge&quot;&gt;TelemetryClient.StartOperation&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;TelemetryClient.StopOperation&lt;/code&gt; - see the &lt;a href=&quot;https://docs.microsoft.com/en-us/azure/azure-monitor/app/custom-operations-tracking&quot;&gt;Telemetry Client documentation&lt;/a&gt;.&lt;/p&gt;</content><author><name>Frans Lytzen</name></author><category term="C#" /><summary type="html">With ORMs it's very easy to write code that calls SQL many times to serve a single web requests, and it can be hard to find it. Application Insights can help.</summary></entry><entry><title type="html">Everything I thought I knew about Async in C# was wrong</title><link href="https://www.lytzen.name/2019/04/29/Everything-I-thought-I-knew-about-async-was-wrong.html" rel="alternate" type="text/html" title="Everything I thought I knew about Async in C# was wrong" /><published>2019-04-29T00:00:00+01:00</published><updated>2019-04-29T00:00:00+01:00</updated><id>https://www.lytzen.name/2019/04/29/Everything-I-thought-I-knew-about-async-was-wrong</id><content type="html" xml:base="https://www.lytzen.name/2019/04/29/Everything-I-thought-I-knew-about-async-was-wrong.html">&lt;p&gt;When I first agreed to do a &lt;a href=&quot;/talks/csharp_async_deep_dive&quot;&gt;talk&lt;/a&gt; about how Async in C# really works, I thought I was an expert. I had written lots of high-throughput, high-performance async code so it should be really easy to write this talk.&lt;/p&gt;

&lt;p&gt;As soon as I began writing the code to prove the things I knew - it turned out that I knew nothing at all and all my assumptions were wrong. The result was a much better talk where I describe the fundamentals of how Async in C# works and explain a lot of the misconceptions people, such as myself, have. So far I have delivered the talk to my colleagues at work, at &lt;a href=&quot;https://www.meetup.com/dotnetoxford/&quot;&gt;Dotnet Oxford&lt;/a&gt; and at &lt;a href=&quot;https://dddsouthwest.com/&quot;&gt;DDD South West&lt;/a&gt; - thanks for having me!&lt;/p&gt;

&lt;p&gt;The presentation and the code samples &lt;a href=&quot;https://github.com/flytzen/Async.Presentation&quot;&gt;are on GitHub&lt;/a&gt; and I have also recorded it as a video. For practical reasons, the video was recorded in five parts; you can see them all &lt;a href=&quot;https://www.youtube.com/watch?v=UzVMzBEpuJg&amp;amp;list=PLbwbf2ZiIT4N7G08GcCd1Z64N8tfgKfIn&quot;&gt;in one playlist&lt;/a&gt; or see the individual parts below.&lt;/p&gt;

&lt;h2 id=&quot;part-1---introduction&quot;&gt;Part 1 - Introduction&lt;/h2&gt;
&lt;p&gt;There are lots of reasons to use async - some of them are true, others are not. In the introduction we have a quick look at these.&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/UzVMzBEpuJg&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;h2 id=&quot;part-2---brain-teasers&quot;&gt;Part 2 - Brain Teasers&lt;/h2&gt;
&lt;p&gt;A quick run through some code examples that may give you different results than you expect. It sets the stage for the next video about how it works.&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/s4cvgyZ0kUM&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;h2 id=&quot;part-3---how-it-works&quot;&gt;Part 3 - How it works&lt;/h2&gt;
&lt;p&gt;The meat of the presentation and by far the longest video at nearly 20 minutes. It goes into detail about what really happens when you run async code and explains where threads come into play.&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/RBFJoPbbvTk&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;h2 id=&quot;part-4---deadlocks&quot;&gt;Part 4 - Deadlocks&lt;/h2&gt;
&lt;p&gt;If you work with ASP.Net “old” (i.e. not Core) or you write Desktop apps then you have probably heard that using &lt;code class=&quot;highlighter-rouge&quot;&gt;.Wait&lt;/code&gt; or &lt;code class=&quot;highlighter-rouge&quot;&gt;.Result&lt;/code&gt; can result in deadlocks. And you may have experienced that some code will only deadlock sometimes. This video builds on part 3 to explain why the deadlocks occur. It does primarily focus on ASP.Net.&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/eRuGnEAya8M&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;h2 id=&quot;part-5---tips&quot;&gt;Part 5 - Tips&lt;/h2&gt;
&lt;p&gt;To sum up I have collected some tips that you can use, including some on how to utilise Async to easily parallelise network writes and other async work in a way that can speed up the throughput of your code very substantially. After the last time I gave this talk, &lt;a href=&quot;https://twitter.com/tjrobinson&quot;&gt;Tom Robinson&lt;/a&gt; kindly pointed me to &lt;a href=&quot;https://github.com/davidfowl/AspNetCoreDiagnosticScenarios/blob/master/AsyncGuidance.md#prefer-asyncawait-over-directly-returning-task&quot;&gt;these comments&lt;/a&gt; by &lt;a href=&quot;https://twitter.com/davidfowl&quot;&gt;David Fowler&lt;/a&gt; which put a bit more nuance around my tip to “just return the task”; consider your circumstances and make the choices that are right for you.&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/UtF_0gfZ48Y&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;</content><author><name>Frans Lytzen</name></author><category term="C#" /><summary type="html">The way Async is being described, it sounds like it will make your code faster and more scalable, whilst solving all your problems and achieving world peace - all before lunch. Async certainly can help you do more I/O in parallel and may in some circumstances help you scale. But did you know Async code can sometimes also use more memory, make your code slower and can introduce subtle bugs that may only appear in production?</summary></entry><entry><title type="html">Add new user to all my Azure subscriptions</title><link href="https://www.lytzen.name/2019/04/11/add-user-to-all-azure-subs.html" rel="alternate" type="text/html" title="Add new user to all my Azure subscriptions" /><published>2019-04-11T00:00:00+01:00</published><updated>2019-04-11T00:00:00+01:00</updated><id>https://www.lytzen.name/2019/04/11/add-user-to-all-azure-subs</id><content type="html" xml:base="https://www.lytzen.name/2019/04/11/add-user-to-all-azure-subs.html">&lt;p&gt;I found myself needing to add the same user as an owner to all my Azure subscriptions. If I could ever retain Powershell syntax, I could write a clever script that would loop through all the subscriptions and do what was needed. Alas, that forever eludes me so I just opted for a few Azure CLI commands with a bit of editing in an editor. Still much faster than going throgh the UI repeatedly.&lt;/p&gt;

&lt;h2 id=&quot;multiple-tenants&quot;&gt;Multiple tenants&lt;/h2&gt;
&lt;p&gt;For most people, this doesn’t apply so feel free to skip this section if it is irrelevant.&lt;br /&gt;
When you log in with Azure CLI, you can &lt;em&gt;see&lt;/em&gt; all your subscriptions across tenants - but the command to add a user to a subscription only works for the tenant you are currently “logged in to”. It can be confusing.&lt;br /&gt;
To see the tenant IDs for all your subscriptions you can run this:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; az account list --query &quot;[].{tenantId:tenantId, name:name} | sort_by([],&amp;amp;tenantId)&quot;  --output table
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In order to “login” to a specific tenant you need to :&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;az login --tenant &amp;lt;tenant id&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Then do the stuff below for each tenant id&lt;/p&gt;

&lt;h2 id=&quot;getting-the-list-of-subscription-ids&quot;&gt;Getting the list of subscription IDs&lt;/h2&gt;
&lt;p&gt;First, get the list of subscription ids for a given tenant id:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;az account list --query &quot;[] | [?tenantId == 'foo.onmicrosoft.com'].id&quot; --output table
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Note that the tenant id may be in this form or may be a guid - not sure why.&lt;br /&gt;
Alternatively, if you only have a single tenant, just do:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;az account list --query &quot;[].id&quot; --output table
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;add-the-user-to-all-the-subscriptions&quot;&gt;Add the user to all the subscriptions&lt;/h2&gt;

&lt;p&gt;Copy the list of subscription IDs into a text editor and modify each line to look like this (111… being the subscription id):&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;az role assignment create --role &quot;Owner&quot; --assignee foo@bar.com --scope /subscriptions/11111111-1111-1111-1111-111111111111
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Copy those lines into your console and shortly after your new user will be an owner of all your subscriptions.&lt;/p&gt;</content><author><name>Frans Lytzen</name></author><category term="DevOps" /><category term="Azure" /><summary type="html">Add a new user to all my Azure subscriptions using Azure CLI</summary></entry><entry><title type="html">Publish Nuget packages with Azure Dev Ops</title><link href="https://www.lytzen.name/2018/10/03/publish-nuget-packages-with-azure-devops.html" rel="alternate" type="text/html" title="Publish Nuget packages with Azure Dev Ops" /><published>2018-10-03T00:00:00+01:00</published><updated>2018-10-03T00:00:00+01:00</updated><id>https://www.lytzen.name/2018/10/03/publish-nuget-packages-with-azure-devops</id><content type="html" xml:base="https://www.lytzen.name/2018/10/03/publish-nuget-packages-with-azure-devops.html">&lt;p&gt;Whenever I decide to create a Nuget package, whether for OSS or to publish on our internal MyGet feed I end up spending an inordinate amount of time trying to figure out a flow that works for testing and publishing. I guess it’s one of those things that, once you have figured it out, becomes easy but it has eluded me until recently.&lt;/p&gt;

&lt;p&gt;My requirements are quite specific and may not be to everyone’s liking;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;I want to use &lt;a href=&quot;https://datasift.github.io/gitflow/IntroducingGitFlow.html&quot;&gt;GitFlow&lt;/a&gt; to control my branches, including using Pull Requests etc.&lt;/li&gt;
  &lt;li&gt;Whenever a commit is made to &lt;code class=&quot;highlighter-rouge&quot;&gt;develop&lt;/code&gt; (or a PR is merged in), I want to publish that package with a “&lt;code class=&quot;highlighter-rouge&quot;&gt;-pre.123&lt;/code&gt;” suffix as per &lt;a href=&quot;https://semver.org/&quot;&gt;SemVer&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Whenever the same happens to &lt;code class=&quot;highlighter-rouge&quot;&gt;master&lt;/code&gt; I want to publish a “full” release.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I haven’t explicitly covered it here, but it would also be nice to have packages sat in a &lt;code class=&quot;highlighter-rouge&quot;&gt;release/*&lt;/code&gt; branch be published with a &lt;code class=&quot;highlighter-rouge&quot;&gt;-beta.1&lt;/code&gt; suffix - but you can easily extend it to cover that scenario as well.&lt;/p&gt;

&lt;p&gt;In this post I will show how to set up GitHub with Azure DevOps to do this for us.&lt;/p&gt;

&lt;p&gt;I am basing this on “modern” (i.e. 2017) csproj files, the ones where the package references are in the .csproj files. This came in with .Net Core but works fine with Full Framework projects.&lt;/p&gt;

&lt;h2 id=&quot;version-numbers&quot;&gt;Version numbers&lt;/h2&gt;
&lt;p&gt;When you create and publish Nuget packages you can specify the version number you want to use on the command line and there is ample of documentation about how to do that with Azure DevOps and other builder services, including MyGet build services, which I used previously.&lt;br /&gt;
However, I really like more control so I like to control the version number in my .csproj file - but I want the build service to automatically append &lt;code class=&quot;highlighter-rouge&quot;&gt;-pre.nnn&lt;/code&gt; when it published from the &lt;code class=&quot;highlighter-rouge&quot;&gt;develop&lt;/code&gt; branch.&lt;/p&gt;

&lt;p&gt;The first thing to understand is that there are two ways you can specify the version number in your &lt;code class=&quot;highlighter-rouge&quot;&gt;.csproj&lt;/code&gt; file:&lt;/p&gt;
&lt;div class=&quot;language-xml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nt&quot;&gt;&amp;lt;version&amp;gt;&lt;/span&gt;1.2.3-pre.987&lt;span class=&quot;nt&quot;&gt;&amp;lt;/version&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;or&lt;/p&gt;
&lt;div class=&quot;language-xml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nt&quot;&gt;&amp;lt;VersionPrefix&amp;gt;&lt;/span&gt;1.2.3&lt;span class=&quot;nt&quot;&gt;&amp;lt;/VersionPrefix&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;VersionSuffix&amp;gt;&lt;/span&gt;pre.987&lt;span class=&quot;nt&quot;&gt;&amp;lt;/VersionSuffix&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Both the above will create packages with version &lt;code class=&quot;highlighter-rouge&quot;&gt;1.2.3-pre.987&lt;/code&gt;. The naming of the Prefix and Suffix threw me for the longest time - I thought they were meant to interact with the &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;Version&amp;gt;&lt;/code&gt; attribute somehow, but Prefix and Suffix is more like “main part” and “extra bit” and you should &lt;em&gt;either&lt;/em&gt; those &lt;em&gt;or&lt;/em&gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;Version&amp;gt;&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The second thing to understand is that you can use conditionals and environment variables in the attributes. 
For my purposes, this is what I ended up with:&lt;/p&gt;

&lt;div class=&quot;language-xml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nt&quot;&gt;&amp;lt;VersionPrefix&amp;gt;&lt;/span&gt;1.2.3&lt;span class=&quot;nt&quot;&gt;&amp;lt;/VersionPrefix&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;VersionSuffix&amp;gt;&amp;lt;/VersionSuffix&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;VersionSuffix&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;Condition=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot; '$(Configuration)' == 'Debug' &quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;debug&lt;span class=&quot;nt&quot;&gt;&amp;lt;/VersionSuffix&amp;gt;&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;&amp;lt;!-- For local/debug builds --&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;VersionSuffix&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;Condition=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot; '$(Build_SourceBranch)' == 'refs/heads/develop' &quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;pre.$(Build_BuildID)&lt;span class=&quot;nt&quot;&gt;&amp;lt;/VersionSuffix&amp;gt;&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;&amp;lt;!-- This is using variables that are specific to Azure Dev Ops Pipelines --&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;My &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;VersionPrefix&amp;gt;&lt;/code&gt; here is really the proper version I want my package to have.&lt;br /&gt;
I have an empty &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;VersionSuffix&amp;gt;&lt;/code&gt; as default. I probably don’t actually need that tag, but it helps make it clearer in my mind.&lt;/p&gt;

&lt;p&gt;The next &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;VersionSuffix&amp;gt;&lt;/code&gt; uses the &lt;code class=&quot;highlighter-rouge&quot;&gt;Configuration&lt;/code&gt; variable that is provided by the dotnet build process; if I build in Debug mode, the package version will become &lt;code class=&quot;highlighter-rouge&quot;&gt;1.2.3-debug&lt;/code&gt;. This is mainly useful for local scenarios as I will always build in Release mode for publishing.&lt;/p&gt;

&lt;p&gt;The &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;VersionSuffix&amp;gt;&lt;/code&gt; after that looks at an environment variable provided by Azure DevOps when you are running in the pipeline. This means that if I am building from the &lt;code class=&quot;highlighter-rouge&quot;&gt;develop&lt;/code&gt; branch in an Azure Pipeline then it will set the Suffix. &lt;code class=&quot;highlighter-rouge&quot;&gt;Build_BuildID&lt;/code&gt; is another environment variable provided by Azure Dev Ops to the pipeline, which will always increment. So, in the example here I may end up with a version number of &lt;code class=&quot;highlighter-rouge&quot;&gt;1.2.3-pre.6239&lt;/code&gt;. As long as that last number reliably increments (which it does) you are fine for package control.&lt;br /&gt;
There is another variable called &lt;code class=&quot;highlighter-rouge&quot;&gt;Build_BuildNumber&lt;/code&gt; which you may be tempted to use instead. However, I found some scenarios where that variable would have the name of the pipeline instead of a number, which causes the build to fail.&lt;/p&gt;

&lt;p&gt;For more advanced scenarios you can invent your own attributes, which become variables in their own right, which you can then re-combine in other ways.&lt;/p&gt;

&lt;h2 id=&quot;publish-symbols&quot;&gt;Publish Symbols&lt;/h2&gt;
&lt;p&gt;Traditionally, when you create a Nuget package, it &lt;em&gt;won’t&lt;/em&gt; include the &lt;code class=&quot;highlighter-rouge&quot;&gt;pdb&lt;/code&gt; files (the debug symbols). In the past, the answer was to &lt;code class=&quot;highlighter-rouge&quot;&gt;--include-symbols&lt;/code&gt; when building your Nuget pacakge. This will create &lt;em&gt;two&lt;/em&gt; Nuget packages, one with the &lt;code class=&quot;highlighter-rouge&quot;&gt;pdb&lt;/code&gt; files and one without. Up until a few years ago, you could publish both of these together to Nuget, but then that changed and now you have to publish the symbols package to a different server with a different API key and a different command. It becomes a real headache, especially because of the inconsistent and out of date documentation. Hence why I have included it in this guide; I either need to tell you how to publish symbols from within the pipeline or tell you how to avoid it.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/dotnet/sourcelink&quot;&gt;SourceLink&lt;/a&gt; to the rescue. SourceLink provides a way to link your package to a specific commit on, say, GitHub or elsewhere. I do recommend using SourceLink as it does so much more than just give you the PDB file - but even if you can’t or won’t, there is a gem hidden in the documentation, namely this line to add to your &lt;code class=&quot;highlighter-rouge&quot;&gt;.csproj&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&quot;language-xml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nt&quot;&gt;&amp;lt;AllowedOutputExtensionsInPackageBuildOutputFolder&amp;gt;&lt;/span&gt;
  $(AllowedOutputExtensionsInPackageBuildOutputFolder);.pdb
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/AllowedOutputExtensionsInPackageBuildOutputFolder&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;What this will do is include the &lt;code class=&quot;highlighter-rouge&quot;&gt;pdb&lt;/code&gt; file in your main Nuget package, meaning you don’t need a separate symbols package at all. Of course, using the full SourceLink is much better. Incidentally, this also works for private repos without sharing the source publicly.&lt;/p&gt;

&lt;h2 id=&quot;variables&quot;&gt;Variables&lt;/h2&gt;
&lt;p&gt;When you are looking at the documentation for Azure DevOps there are lists of variables scattered in different places. You will probably also find that the same variable in some context is referred to as Build.BuildId and in another as BUILD_BUILDID etc. Sometimes you have to reference it as %BUILD_BUILDID%, other times as $(Build.BuildId) and yet other times as $(Build_BuildID). It does sort of make sense, but as a good starting point, when designing your YAML file, I recommend adding this task somewhere:&lt;/p&gt;
&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;script&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;set&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;displayName&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;show variables&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;It just dumps all the environment variables to the log, so you can have a look through to see what is actually available for you to reference.&lt;/p&gt;

&lt;h2 id=&quot;setting-up-a-pipeline&quot;&gt;Setting up a pipeline.&lt;/h2&gt;
&lt;p&gt;The easiest way to set up a build pipeline on Azure DevOps from GitHub is to add the &lt;a href=&quot;https://github.com/marketplace/azure-pipelines&quot;&gt;Azure Pipelines&lt;/a&gt; GitHub App to your Github account. When you connect it to a repository, it will walk you through setting up a default pipeline; just choose the “empty” option. This pipeline will save a YAML file into your repository and will set up two triggers. One is a simple trigger to run the pipeline for any commit on any branch, the other is a specific integration into Pull Requests; essentially any pull request will be run through the pipeline and if it fails it will block the PR from being merged.&lt;/p&gt;

&lt;p&gt;This is the YAML file I ended up:&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# This only runs for master and develop. Plus a seperate trigger is run for PR validation. This means commits to branches not in a PR won't get tested. Choices, choices...&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;NewOrbit.NewOrbit.AddOne - build and test&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;trigger&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;master&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;develop&lt;/span&gt;

&lt;span class=&quot;na&quot;&gt;variables&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;buildConfiguration&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Release&lt;/span&gt;

&lt;span class=&quot;na&quot;&gt;pool&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;vmImage&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;vs2017-win2016'&lt;/span&gt;

&lt;span class=&quot;na&quot;&gt;steps&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;

&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;script&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;set&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;displayName&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;show variables&lt;/span&gt;

&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;script&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;dotnet restore&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;displayName&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;dotnet restore&lt;/span&gt;

&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;script&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;dotnet build --configuration $(buildConfiguration) --no-restore&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;displayName&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;build&lt;/span&gt;

&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;task&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;DotNetCoreCLI@2&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;displayName&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;test&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;inputs&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;command&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;test&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;projects&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;**/*tests/*.csproj'&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;arguments&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;--configuration&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;$(buildConfiguration)'&lt;/span&gt;

&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;script&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;dotnet pack --configuration $(buildConfiguration) --no-build --output %Build_ArtifactStagingDirectory%&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;condition&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;and(succeeded(), or(eq(variables['Build.SourceBranchName'], 'master'),eq(variables['Build.SourceBranchName'], 'develop')))&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;displayName&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;pack&lt;/span&gt;

&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;task&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;NuGetCommand@2&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;displayName&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;publish&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;condition&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;and(succeeded(), or(eq(variables['Build.SourceBranchName'], 'master'),eq(variables['Build.SourceBranchName'], 'develop')))&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;inputs&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;command&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;push&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;nuGetFeedType&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;external&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;publishFeedCredentials&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;NewOrbit&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;MyGet&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Nuget'&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;packagesToPush&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;$(Build.ArtifactStagingDirectory)/**/*.nupkg'&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The &lt;code class=&quot;highlighter-rouge&quot;&gt;trigger&lt;/code&gt; part limits this to only run on checkins to &lt;code class=&quot;highlighter-rouge&quot;&gt;develop&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;master&lt;/code&gt; (note, some of the documentation has a more verbose syntax that seems to not work). The Pull Request trigger still works so all Pull Request and all commits into an open Pull Request will be run through this pipeline. But, for me, I don’t need CI to run on every commit on every feature branch. That’s just me - if you want the pipeline to run for every commit, just delete the &lt;code class=&quot;highlighter-rouge&quot;&gt;trigger&lt;/code&gt; section altogether.&lt;/p&gt;

&lt;p&gt;The steps through restore and build should be obvious. The step after that uses a special Azure DevOps task to run the unit tests, which ensures that the results are reported in a nice way in the pipeline.&lt;/p&gt;

&lt;p&gt;The &lt;code class=&quot;highlighter-rouge&quot;&gt;script: dotnet pack&lt;/code&gt; packs the Nuget package and outputs the package to a particular holding area. To be honest, I could probably forgo the &lt;code class=&quot;highlighter-rouge&quot;&gt;output&lt;/code&gt; parameter but it helps to understand what is going on.&lt;br /&gt;
The key thing here is the &lt;code class=&quot;highlighter-rouge&quot;&gt;condition&lt;/code&gt; line. This will ensure that a Nuget package is &lt;em&gt;only&lt;/em&gt; created if the build is of either the develop or the master branch. If you wanted to publish “beta” versions from &lt;code class=&quot;highlighter-rouge&quot;&gt;release/*&lt;/code&gt; branches, it should be straight forward to extend the condition accordingly.&lt;br /&gt;
Incidentally, there is an Azure DevOps task for creating the Nuget package but I couldn’t get it to work so used &lt;code class=&quot;highlighter-rouge&quot;&gt;dotnet pack&lt;/code&gt; instead.&lt;/p&gt;

&lt;p&gt;The final task publishes the created nuget package to Nuget. In this case I am publishing it to Myget; In order to do this, you first need to go to your &lt;em&gt;project&lt;/em&gt; in Azure DevOps, go to Project Settings and then select Service Connections (it’s well hidden). Then add a connection to Nuget or MyGet or whatever Nuget feed you want to publish to. You put the &lt;em&gt;name&lt;/em&gt; of that service connection in the &lt;code class=&quot;highlighter-rouge&quot;&gt;publishFeedCredentials&lt;/code&gt; property in the YAML file.&lt;/p&gt;

&lt;p&gt;if you wanted to publish packages from your develop branch to MyGet and the ones from Master to NuGet you can hopefully see how you can just duplicate the last task and change the &lt;code class=&quot;highlighter-rouge&quot;&gt;condition&lt;/code&gt; statements to suit your needs.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt; There is a bug in Azure DevOps that may result in an error saying something like that your pipeline doesn’t have the right permissiom to use the service connection. It’s easy to fix by following the guidance &lt;a href=&quot;https://docs.microsoft.com/en-us/azure/devops/pipelines/process/resources?view=vsts&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;approvals&quot;&gt;Approvals&lt;/h2&gt;
&lt;p&gt;The approach described above will publish packages immediately. If you wanted, you can easily set it up so you have to manually approve the publish. In short, you need to replace the final publish task in the YAML above with a Publish Artifacts task.&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;task&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;PublishBuildArtifacts@1&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;inputs&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;artifactName&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;package'&lt;/span&gt; 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This will copy whatever is in the &lt;code class=&quot;highlighter-rouge&quot;&gt;%Build_ArtifactStagingDirectory%&lt;/code&gt; directory (where we put the Nuget package before) and make it available as an artefact of the build. Once you run the pipeline, look at the build and you will see an Artefact. If you click on that, Azure DevOps will take you through a wizard to set up a release pipeline, which you can then use to add manual approval before you publish the package to Nuget.&lt;/p&gt;</content><author><name>Frans Lytzen</name></author><category term="DevOps" /><category term="Azure" /><summary type="html">Use GitFlow and Azure Devops to automatically publish Nuget packages with sensible version numbers</summary></entry><entry><title type="html">Hierarchy IDs for Fun and Performance</title><link href="https://www.lytzen.name/2018/06/27/hierarchyid-for-fun-and-performance.html" rel="alternate" type="text/html" title="Hierarchy IDs for Fun and Performance" /><published>2018-06-27T00:00:00+01:00</published><updated>2018-06-27T00:00:00+01:00</updated><id>https://www.lytzen.name/2018/06/27/hierarchyid-for-fun-and-performance</id><content type="html" xml:base="https://www.lytzen.name/2018/06/27/hierarchyid-for-fun-and-performance.html">&lt;p&gt;Many systems have a &lt;em&gt;hierarchy&lt;/em&gt; in the data. This may be an organisation hierarchy, or maybe a hierarchy caused by a multi-tenant system or a combination thereof.&lt;br /&gt;
It’s relatively easy to model a hierarchy in a relational or document database - it is much harder to effectively filter which part of the tree a given user can see or act on. You may find yourself traversing up or down the tree or making multi-table joins.&lt;br /&gt;
A simpler solution is to use a Hierarchy ID. In a relational database, you would implement this as a string field with a delimiter-separated list of its parents, for example &lt;code class=&quot;highlighter-rouge&quot;&gt;120/23/47/19&lt;/code&gt;. If a user is allowed to see everything from &lt;code class=&quot;highlighter-rouge&quot;&gt;120/23&lt;/code&gt; downwards, you can easily search for all the records where the Hierarchy ID starts with &lt;code class=&quot;highlighter-rouge&quot;&gt;120/23&lt;/code&gt; (indexes will work well with that). In document databases it ma be more optimal to have an array of all the Ancestor IDs instead of a string value.&lt;/p&gt;

&lt;p&gt;This post explores the use of Hierarchy IDs to make filtering easy and performant. In addition, it has some pointers about how to set up hierarchical configuration using the same Hierarchy ID you are implementing anyway. Of course, you could just use a graph database and then it’s a different story altogether.&lt;/p&gt;

&lt;h1 id=&quot;the-problem&quot;&gt;The problem&lt;/h1&gt;

&lt;p&gt;A multi-tenant hierarchy may look like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.lytzen.name/assets/fixedhierarchy.png&quot; alt=&quot;Fixed Hierarchy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We’ll call this is a &lt;em&gt;fixed&lt;/em&gt; hierarchy as it has a fixed depth and different types at different levels.&lt;/p&gt;

&lt;p&gt;An organisational hierarchy may look like this;
&lt;img src=&quot;https://www.lytzen.name/assets/selfreferrentialhierarchy.png&quot; alt=&quot;Self-referential Hierarchy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We’ll call this a &lt;em&gt;self-referential&lt;/em&gt; hierarchy as each layer in the hierarchy refers to parent/children of the same type.&lt;/p&gt;

&lt;p&gt;At &lt;a href=&quot;https://neworbit.co.uk&quot;&gt;NewOrbit&lt;/a&gt; we sometimes build multi-tenanted systems which has resellers, who have system customers, who in turn have organisational hierarchies so it can quickly become a very deep hierarchy.&lt;/p&gt;

&lt;p&gt;When you are creating your data structure, whether in a relational database such as Azure SQL or a document database such as Mongo or CosmosDb, you will typically have a Parent ID on records in the hierarchy as illustrated above. This is easy at write time and makes it relatively easy to traverse the hierarchy both up and down. 
However, when you need to get access to a only a part of the tree or indeed multiple subsets of the tree then it becomes very hard to query efficiently and performantly.&lt;/p&gt;

&lt;p&gt;As an example with the self-referential hierarchy, imagine in the illustration above that a given user is allowed to see the details of anybody who is in Bob’s hierarchy; you’d need to write a recursive function to keep drilling down the layers until there are no more subordinates. That means running many different SQL queries or using UDFs (which in turn will run a recursive function).&lt;/p&gt;

&lt;p&gt;Alternatively in the fixed hierarchy, imagine if a given user has access to see all the projects for three separate branches and they want to see a list of all tasks across all projects they have access to. Or imagine that a Reseller user has access to see all the details for all their Customers, but not for any other Customer - and they want to see a list of all Projects (okay, maybe not the best example in the world, but you get the idea). In that scenario you’d probably write some code that JOINs all the way up the hierarchy and out to various permissions tables, such as “are they in the list that can see the project or in the list that can see the branch or in the list that can see the customer or in the list that can see the reseller”. It may not be that hard to write the code, but it’s easy to end up with a 20-table &lt;code class=&quot;highlighter-rouge&quot;&gt;JOIN&lt;/code&gt;, which is expensive on a relational database - and impossible if you use CosmosSB which does not support &lt;code class=&quot;highlighter-rouge&quot;&gt;JOIN&lt;/code&gt;s.&lt;/p&gt;

&lt;p&gt;In this post I am primarily focusing on how to &lt;em&gt;filter&lt;/em&gt; the data so a user can only see the data they are allowed in an efficient manner. There are other hierarchy scenarios, in particular around set based operations and there are other patterns that are better suited to those than what I am showing here. I highly recommend &lt;a href=&quot;https://www.amazon.co.uk/Hierarchies-Smarties-Kaufmann-Management-Systems/dp/0123877334&quot;&gt;Joe Celko’s Trees and Hierarchies in SQL for Smarties&lt;/a&gt; for understanding more about this. In fact, the lessons that I am expounding on in this post are based on what I learnt from that book many moons ago. Even if you use a document database, it’s still a good read to understand the patterns.&lt;/p&gt;

&lt;h1 id=&quot;how-to-store-hierarchy-ids&quot;&gt;How to store Hierarchy IDs&lt;/h1&gt;
&lt;p&gt;As discussed above, you will typically have some kind of Parent ID on each node in the hierarchy (though it’s usually called something more meaningful, such as CustomerID or ManagerID). 
The trick to efficient querying is to maintain a &lt;em&gt;Hierarchy ID&lt;/em&gt; on each record that has all the parent IDs all the way up the tree.&lt;br /&gt;
In a relational database, you would store it as a text string like this (for the Task in the fixed hierarchy example above);
&lt;code class=&quot;highlighter-rouge&quot;&gt;10/20/57/2/1047&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;In a Document Database you can do the same or use a slightly more efficient approach, which is to store an &lt;code class=&quot;highlighter-rouge&quot;&gt;array&lt;/code&gt; of Ancestor IDs. If your documents use &lt;code class=&quot;highlighter-rouge&quot;&gt;GUID&lt;/code&gt;s for IDs, then you can just put all the Ancestor IDs in the array. Otherwise (and only in the fixed hierarchy) you could prefix each ID with its record type - but that starts making it too blurry in my opinion.&lt;/p&gt;

&lt;p&gt;SQL Server has a native Hierarchy ID data type that gives you a few convenience functions, but it is not mapped in Entity Framework so if you use EF you may prefer to just use a normal string.&lt;/p&gt;

&lt;h1 id=&quot;querying&quot;&gt;Querying&lt;/h1&gt;
&lt;p&gt;In order to query the database you need to be able to search on Hierarchy IDs so ensure there is an index on the field for performance. Next you need to know which Hierarchy IDs a user is allowed to see.&lt;/p&gt;

&lt;p&gt;In the case of a &lt;strong&gt;relational database&lt;/strong&gt; you have a couple of options;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;If a user will only have access to a few Hierarchy IDs, then you can simply have those on listed on whatever user object (such as a ClaimsPrincipal) you are passing around in your code and you can then do some SQL along the lines of &lt;code class=&quot;highlighter-rouge&quot;&gt;SELECT FROM xx WHERE HierarchyID LIKE &quot;123/23%&quot; OR HierarchyID LIKE &quot;516/67/43/109%&quot; &lt;/code&gt;. 
If you have many Hierarchy IDs per user then this will end up with a lot of &lt;code class=&quot;highlighter-rouge&quot;&gt;OR&lt;/code&gt; statements which can really hurt performance, so be careful.&lt;/li&gt;
  &lt;li&gt;If a user has access to many Hierarchy IDs it may be better to maintain a Hierarchy ID table in the database, with each row having a user ID and a Hierarchy ID and then &lt;code class=&quot;highlighter-rouge&quot;&gt;INNER JOIN&lt;/code&gt; or &lt;code class=&quot;highlighter-rouge&quot;&gt;APPLY&lt;/code&gt; to that table in your query.&lt;br /&gt;
Just be mindful that if a user is allowed two Hierarchy IDs where one is a subset of the other, you will get duplicate records so you need to either use &lt;code class=&quot;highlighter-rouge&quot;&gt;DISTINCT&lt;/code&gt; or remove such “duplicates” from the User-HierarchyID table before &lt;code class=&quot;highlighter-rouge&quot;&gt;JOIN&lt;/code&gt;ing.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In the case of a &lt;strong&gt;document database&lt;/strong&gt; that doesn’t support &lt;code class=&quot;highlighter-rouge&quot;&gt;JOIN&lt;/code&gt;s your only option is to keep that list of allowed Hierarchy IDs or Ancestor IDs and then use OR statements.&lt;/p&gt;

&lt;h1 id=&quot;mutability-of-the-hierarchy&quot;&gt;Mutability of the hierarchy&lt;/h1&gt;
&lt;p&gt;Some hierarchies are essentially immutable, others can change at times. For example, in the example above with customers and projects, it is exceedingly unlikely that a customer will move between resellers or a project will move between customers. In that scenario you can probably treat the Hierarchy ID as write-only and just set it once when you create the records. In the rare circumstance where you may have to change it, you can deal with that as a one-off and handle it manually.&lt;/p&gt;

&lt;p&gt;Organisation hierarchies, in particular, have an annoying habit of changing over time. If someone’s manager changes, you have to update the Hierarchy IDs all the way down the tree. Depending on the potential size of the tree and your security requirements, this may be done in different ways. Bear in mind that when the CEO of a 100,000 person company changes, that’s a lot of records to update.
In most scenarios you can just have a simple function that recurses through all the affected records and updates each one in turn. You may implement this at the database level or in application code, depending on your requirements. In a relational database you may be tempted to wrap this entire thing in a transaction, but be mindful that this may escalate to a table lock, which may effectively lock your whole system up. Alternatively, updating each record in turn may mean that for a few minutes (for a very large change) some users will see a mixture of the records they used to be able to see and the records they are going to be able to see. The user should never see records they weren’t meant to see; it may just take a few minutes to remove all the records they used to be able to see and add all the new records. 
Users may also need to re-login if you are caching their list of allowed Hierarchy IDs in a session object of some kind. Mostly, these changes are infrequent - at least changes that affect many records - so it’s usually not something to worry too much about, as long as you understand it for your system.&lt;/p&gt;

&lt;h1 id=&quot;ease-of-writing-vs-ease-of-reading&quot;&gt;Ease of Writing vs ease of Reading&lt;/h1&gt;
&lt;p&gt;When you implement a Hierarchy ID, you are making it more complex to &lt;em&gt;write&lt;/em&gt; data; Whenever you add a node in the hierarchy you now have to set it’s Hierarchy ID. If your tree is mutable, you also need to handle updating the Hierarchy IDs of all children whenever nodes are moved - something that can take considerable time and capacity if a high-level node is moved. Similarly, you may need to write code to maintain a list of “allowed hierarchy IDs” per user.  In other words, Hierarchy IDs adds extra complexity to your system.&lt;br /&gt;
On the other hand, once Hierarchy IDs are in place, your data filtering/security code becomes much easier to write and your database queries will be much more performant.&lt;/p&gt;

&lt;p&gt;Whether Hierarchy IDs are right for a given solution will, as always, depend on the needs of that system. Some of the key indicators that you may need it are;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Deep hierarchies with access rights determined at multiple levels&lt;/li&gt;
  &lt;li&gt;Large data sets in a hierarchy&lt;/li&gt;
  &lt;li&gt;Self-referential hierarchies with access rights set at arbitrary levels (think pretty much any organisational hierarchy with scope-of-control security)&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;hierarchical-configuration&quot;&gt;Hierarchical Configuration&lt;/h1&gt;
&lt;p&gt;It is a common requirement to have Hierarchical Configuration where a certain setting is different for a certain part of the tree. For example, you may have some settings that only apply to a particular Reseller, Customer or even Branch etc.&lt;/p&gt;

&lt;p&gt;If you are already implementing Hierarchy IDs in the form of a delimited string, you could have a simple Configuration table that has the Key, the Value and the Hierarchy ID it applies to. 
When you want to find the value for a particular Hierarchy ID for a particular node, you can recursively search for a configuration setting that matches the node’s Hierarchy ID, or it’s parent or the grand parent etc, all the way up to the root. This is obviously not very efficient to query so you would need to cache it (or keep the whole thing in memory if possible), but it makes it very easy to model and extend.&lt;/p&gt;

&lt;p&gt;What you really want is something like (pseudo code)&lt;/p&gt;
&lt;div class=&quot;language-sql highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;SELECT&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Value&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Config&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;WHERE&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;HierarchyID&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;SUBSTRING&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;node_hierarchy_id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;LENGTH&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;HierarchyID&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;You may well be able to write a SQL statement something like that, though it will definitely use a table scan - so still cache it.&lt;/p&gt;

&lt;p&gt;In the case of document database with an array of Ancestor IDs, you’d need to ensure that the Ancestor ID array on the node is sorted by descent level so that you can look for configuration values in the right order (by walking up the tree).&lt;/p&gt;</content><author><name>Frans Lytzen</name></author><category term="Databases" /><category term="Performance" /><category term="SQL" /><category term="CosmosDB" /><summary type="html">Storing hierarchies in a database is easy - but applying hierarchical security and configuration can be very difficult and a significant performance problem. Hierarchy IDs can alleviate this, at the cost of a bit more complexity at write time.</summary></entry><entry><title type="html">GDPR for Software and how Azure can help</title><link href="https://www.lytzen.name/2018/06/25/GDPR-for-software.html" rel="alternate" type="text/html" title="GDPR for Software and how Azure can help" /><published>2018-06-25T00:00:00+01:00</published><updated>2018-06-25T00:00:00+01:00</updated><id>https://www.lytzen.name/2018/06/25/GDPR-for-software</id><content type="html" xml:base="https://www.lytzen.name/2018/06/25/GDPR-for-software.html">&lt;p&gt;When you develop software, whether for other people or for running your business, there are many things you have to consider which are quite different from the things you have to do to make your business GDPR compliant.&lt;br /&gt;
In the spring of 2018 I recorded a video with Microsoft that gives an overview of this as well looks at some of the ways in which Azure can help.&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/KlZhAG351Bs&quot; frameborder=&quot;0&quot; allow=&quot;autoplay; encrypted-media&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;For some more specific tips on how to lock down your software on Azure, including Managed Identity and KeyVault, see &lt;a href=&quot;/2018/04/29/securing-your-webapp-in-azure.html&quot;&gt;Securing your web app in Azure&lt;/a&gt;`&lt;/p&gt;</content><author><name>Frans Lytzen</name></author><category term="Azure" /><category term="GDPR" /><category term="Security" /><summary type="html">A video recording I made with Microsoft about how GDPR applies to Software development and how Azure can help.</summary></entry><entry><title type="html">CosmosDB token has wrong time</title><link href="https://www.lytzen.name/2018/05/08/Comos-db-token-failure.html" rel="alternate" type="text/html" title="CosmosDB token has wrong time" /><published>2018-05-08T12:00:00+01:00</published><updated>2018-05-08T12:00:00+01:00</updated><id>https://www.lytzen.name/2018/05/08/Comos-db-token-failure</id><content type="html" xml:base="https://www.lytzen.name/2018/05/08/Comos-db-token-failure.html">&lt;p&gt;Out of the blue, we started receiving the following error in our web app that uses CosmosDB:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;The authorization token is not valid at the current time. Please create another token and retry (token start time: Tue, 08 May 2018 09:17:33 GMT, token expiry time: Tue, 08 May 2018 09:32:33 GMT, current server time: Tue, 08 May 2018 09:11:00 GMT
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The important things to note here are &lt;code class=&quot;highlighter-rouge&quot;&gt;token start time: Tue, 08 May 2018 09:17:33 GMT&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;current server time: Tue, 08 May 2018 09:11:00 GMT&lt;/code&gt;.&lt;br /&gt;
The token has a start time &lt;em&gt;in the future&lt;/em&gt;. As a consequence, any request to CosmosDB made with this token fails.&lt;/p&gt;

&lt;p&gt;It took us a while to diagnose this because we just assumed that &lt;code class=&quot;highlighter-rouge&quot;&gt;current server time&lt;/code&gt; referred to the time on the Web server, but it actually refers to the time on the CosmosDB server.&lt;br /&gt;
We could see that the &lt;code class=&quot;highlighter-rouge&quot;&gt;current server time&lt;/code&gt; was correct, in that it matched the known correct time on other servers.&lt;/p&gt;

&lt;p&gt;What it boils down to is that the time on our webserver was wrong, specifically it was in the future, and the .Net client SDK was dutifully creating access tokens that started in the future.&lt;/p&gt;

&lt;p&gt;If you are running your app on your own server or a VM, you can stop reading now and go reset the time on your server and all will be well. What was really weird for us was that we are running this in a Web App in Azure App Services. They are supposed to be time synchronised and it simply shouldn’t be possible for their clock to drift. Except it did. We triangulated the problem by running the same code from another location and eventually from a web app deployed to a different App Service Plan.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;If you are experiencing this problem in Azure Web App, the TL;DR; is to scale your site &lt;em&gt;up&lt;/em&gt;, for example from an S1 to an S2 and then down again later. Details as to why are below.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The thing to remember here is that an App Service Plan is what encapsulates the underlying hardware that actually hosts the App Services. We had an App Service Plan with a single node in it and &lt;em&gt;all&lt;/em&gt; Web Apps deployed to this App Service Plan failed with the same error. Which is logical when you think about it, but easy to forget in the heat of battle.&lt;br /&gt;
We confirmed the issue by going to the “Advanced Tools” and, using the Powershell console, ran &lt;code class=&quot;highlighter-rouge&quot;&gt;Get-Date&lt;/code&gt;, which showed us that the web-server time was wrong.&lt;/p&gt;

&lt;p&gt;In our case we had just a single node in the App Service Plan. If you had multiple nodes and only one had time-drifted, you’d probably see the error intermittently and the &lt;code class=&quot;highlighter-rouge&quot;&gt;Get-Date&lt;/code&gt; would just return the time from whatever server the console happened to be running on. If you suspect this situation, it may be worthwhile scaling your app service plan down to a single node, testing it and scaling out again.&lt;/p&gt;

&lt;p&gt;What we needed was for Azure to kill our faulty node so we would automatically roll on to another node with the correct time. Unfortunately there are no tools to do this (and you wouldn’t expect to have to). However, there is a way you can do it and it was literally the process of writing this post that made me think about it, so I’ve just stopped writing to go fix the problem; Simply scale your site &lt;em&gt;up&lt;/em&gt; and then down again. For example, if you are on an S1, scale to an S2 as this will force Azure to deploy your site on a new underlying machine. You can scale back down again when you are happy.&lt;/p&gt;</content><author><name>Frans Lytzen</name></author><category term="Azure" /><category term="CosmosDB" /><summary type="html">CosmosDB error &quot;The authorization token is not valid at the current time&quot; and how to fix it.</summary></entry><entry><title type="html">Securing your web app in Azure</title><link href="https://www.lytzen.name/2018/04/29/securing-your-webapp-in-azure.html" rel="alternate" type="text/html" title="Securing your web app in Azure" /><published>2018-04-29T21:40:00+01:00</published><updated>2018-04-29T21:40:00+01:00</updated><id>https://www.lytzen.name/2018/04/29/securing-your-webapp-in-azure</id><content type="html" xml:base="https://www.lytzen.name/2018/04/29/securing-your-webapp-in-azure.html">&lt;p&gt;So you have deployed your web app to Azure. Now, how do you go about making it secure?
I gave a talk on this topic at  &lt;a href=&quot;https://www.meetup.com/dotnetoxford/&quot;&gt;DotNet Oxford&lt;/a&gt; on 24 April 2018 and recorded it. You can view the video below.&lt;/p&gt;

&lt;p&gt;The video runs through a scenario using an ASP.Net Web App hosted on Azure App Service and covers a number of features you can use to improve your security - as well as a number of features that are not available for App Services.&lt;/p&gt;

&lt;p&gt;The talk covers a lot of ground in an hour and everything is kept at a high level, but is nonetheless heavy on examples and code.&lt;br /&gt;
Watching the video myself, I realised I say “Okay” and “So” way, way too much. Sorry…&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;a href=&quot;https://twitter.com/jamesw0rld&quot;&gt;James World&lt;/a&gt; made this nice sketch note of the talk, reproduced with permission.&lt;br /&gt;
&lt;img src=&quot;https://www.lytzen.name/assets/2018-04-29-security-talk-sketch-note.jpg&quot; alt=&quot;Sketch note&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/2tR5sEk46v0&quot; frameborder=&quot;0&quot; allow=&quot;autoplay; encrypted-media&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;The source code is on &lt;a href=&quot;https://github.com/flytzen/SecurityTalk&quot;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;some-key-timings&quot;&gt;Some key timings&lt;/h2&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Use SSL&lt;/td&gt;
      &lt;td&gt;11:03&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Virus scanning&lt;/td&gt;
      &lt;td&gt;20:01&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;WAF&lt;/td&gt;
      &lt;td&gt;21:00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Vnet&lt;/td&gt;
      &lt;td&gt;23:20&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Azure Key Vault&lt;/td&gt;
      &lt;td&gt;26:10&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Managed Service Identity&lt;/td&gt;
      &lt;td&gt;27:50&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Use Key Vault and managed identify to store secrets&lt;/td&gt;
      &lt;td&gt;29:55&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;ASP.Net Core configuration with Key Vault&lt;/td&gt;
      &lt;td&gt;31:55&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Connect to Azure SQL with Managed Identity (or not)&lt;/td&gt;
      &lt;td&gt;36:27&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Encrypt data at rest&lt;/td&gt;
      &lt;td&gt;38:00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Require secure transport&lt;/td&gt;
      &lt;td&gt;40:30&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;SQL Always Encrypted&lt;/td&gt;
      &lt;td&gt;41:40&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Storage client-side encryption (not shown)&lt;/td&gt;
      &lt;td&gt;52:00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Use Azure AD to access Azure&lt;/td&gt;
      &lt;td&gt;53:25&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Use Azure AD to access Azure SQL&lt;/td&gt;
      &lt;td&gt;54:05&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Supporting Security tools in Azure&lt;/td&gt;
      &lt;td&gt;56:50&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Detection&lt;/td&gt;
      &lt;td&gt;57:45&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;</content><author><name>Frans Lytzen</name></author><category term="Azure" /><category term="Security" /><summary type="html">A video overview of some of the Azure technologies that you can use to better protect your web applications in Azure - all depending on your required security level, of course.</summary></entry><entry><title type="html">Azure Failover and Resilience</title><link href="https://www.lytzen.name/2017/06/29/azure-failover-and-resilience.html" rel="alternate" type="text/html" title="Azure Failover and Resilience" /><published>2017-06-29T17:54:00+01:00</published><updated>2017-06-29T17:54:00+01:00</updated><id>https://www.lytzen.name/2017/06/29/azure-failover-and-resilience</id><content type="html" xml:base="https://www.lytzen.name/2017/06/29/azure-failover-and-resilience.html">Azure provides a highly resilient hosting platform, with significant built-in redundancy within a data centre, as well as the presence of more than 30 data centres across the world.&lt;br /&gt;&lt;br /&gt;When first coming to Azure, it can be hard to understand what resilience you get automatically and what you might have to set up yourself.&lt;br /&gt;&lt;br /&gt;This post provides a high-level overview of the principles. It is intended as an introduction to help you ask the right questions.&lt;br /&gt;&lt;br /&gt;The usual starting point for a system is to host it in a single data centre. Azure is highly resilient even within a single data centre, but even so, all the data is continually backed up to a secondary data centre.&lt;br /&gt;&lt;br /&gt;In the case of a complete failure of a data centre, the data can be restored to another data centre. This is not the same as automatic failover to another data centre; In order to get the data restored in the other data centre and get the system back up and running, you will have to do it yourself; Azure will (for the most part) not do this for you. How much work depends on how much preparatory work has been done and is primarily a business decision based on risk and cost.&lt;br /&gt;&lt;br /&gt;Any conversation about failover is complicated by the fact that a system consists of different components, which can fail independently, which have different probability and impact and which require different failover strategies.&lt;br /&gt;&lt;br /&gt;Before going into the details, it is important to understand that even the most basic setup in Azure has a very high level of resilience with each individual component typically having a guaranteed uptime of 99.95% or more. At the same time, data is continually backed up to a secondary data centre. In other words, even the most basic Azure setup has a level of resilience that is difficult and expensive to achieve with on-premise hosting.&lt;br /&gt;&lt;br /&gt;In this post “failover” will refer to failing over between data centres.&lt;br /&gt;&lt;br /&gt;&lt;h4&gt;Resilience in a single data centre&lt;/h4&gt;&lt;div&gt;Azure Data Centres are built in a modular way, meaning that each data centre can be thought of as many smaller data centres built next to each other. This means that your data and system will be physically spread over different parts of the data centre, in turn meaning that even if an entire part of the data centre fails, you are unlikely to notice.&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;Physically, all Azure data centres have multiple redundant power grid connections, multiple redundant internet connections, redundant backup generators, batteries and so on and so forth.&lt;br /&gt;&lt;br /&gt;As a general rule, any data you save in Azure, in databases, to disk or to other types of storage, is written to three different locations inside that one data centre and a single copy is written to another remote data centre as a backup. For example the London data centre backs up to Cardiff and the Amsterdam data centre backs up to Dublin etc.&lt;br /&gt;&lt;br /&gt;Azure App Service has some built-in resilience so even with only a single compute node in your app hosting plan, you are pretty well protected from outages. With Cloud Services, you must ensure that you have at least two instances running at all times to ensure resilience. With Virtual Machines – you are much more on your own, though there are a number of things you can configure, such as Availability Sets etc. As a &lt;i&gt;very&lt;/i&gt; general rule, to make reliability easier, avoid using VMs, use one of the managed options instead, when you can.&lt;br /&gt;&lt;br /&gt;When you want to be able to fail over to another data centre, there are several options available to you. I have grouped them here under “Cold”, “Warm” and “Hot”. These are just convenience labels and may not correlate to other people’s definitions.&lt;br /&gt;&lt;br /&gt;&lt;h4&gt;Cold&lt;/h4&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://4.bp.blogspot.com/-wV3fbqWMgeU/WVUssFhNeqI/AAAAAAAALuA/n5RZdmPeCqM4Jr6snqblsOxVj7fUfEyvQCLcBGAs/s1600/Failover%2BOptions%2B-%2BCold.png&quot; imageanchor=&quot;1&quot; style=&quot;clear: right; float: right; margin-bottom: 1em; margin-left: 1em;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;560&quot; data-original-width=&quot;1180&quot; height=&quot;188&quot; src=&quot;https://4.bp.blogspot.com/-wV3fbqWMgeU/WVUssFhNeqI/AAAAAAAALuA/n5RZdmPeCqM4Jr6snqblsOxVj7fUfEyvQCLcBGAs/s400/Failover%2BOptions%2B-%2BCold.png&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;div&gt;A Cold failover is what you get by default.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;Your data is automatically backed up to another data centre. In the case of a failure of the primary data centre, you can go to the other data centre, set up your systems again, deploy everything and restore your database. Of course, the more automated your deployment is, the easier this will be.&lt;br /&gt;&lt;br /&gt;You should be aware that while you can manually trigger a restore of SQL and CosmosDB databases, you cannot yourself trigger a “restore” of anything you put into Azure Storage. Microsoft has to do that by changing DNS entries and their SLA on that is up to 48 hours, last time I checked. There are things you can do to improve this, such as using read-access geo-redundant storage, but you will need to develop specifically to take advantage of that. Often, though, the data in Azure Storage is secondary to the main system and you may be able to live without that data for a day or two.&lt;br /&gt;&lt;br /&gt;The exact frequency of database backups depends on the chosen database but is generally four hours or less.&lt;br /&gt;&lt;br /&gt;&lt;h4&gt;Warm&lt;/h4&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://4.bp.blogspot.com/-gk1abFDNI6A/WVUtOZGJaeI/AAAAAAAALuE/gEM_835uULUHb8zijsbskKBwbgclBf2zgCLcBGAs/s1600/Failover%2BOptions%2B-%2BWarm.png&quot; imageanchor=&quot;1&quot; style=&quot;clear: right; float: right; margin-bottom: 1em; margin-left: 1em;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;560&quot; data-original-width=&quot;1180&quot; height=&quot;188&quot; src=&quot;https://4.bp.blogspot.com/-gk1abFDNI6A/WVUtOZGJaeI/AAAAAAAALuE/gEM_835uULUHb8zijsbskKBwbgclBf2zgCLcBGAs/s400/Failover%2BOptions%2B-%2BWarm.png&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/div&gt;A Warm failover allows faster fail-over to a secondary data centre, but still requires manual intervention.&lt;br /&gt;&lt;br /&gt;In order to reduce the time it takes to move to a secondary data centre, it is possible to prepare the infrastructure and have detailed plans in place. You can do this by configuring the whole system in the secondary data centre but not deploy anything to it; For many services you can define it but just not deploy anything to it. Similarly, you can deploy VMs and then de-allocate them etc. An alternative is to create an ARM template, which will allow you to quickly create a whole environment in Azure.&lt;br /&gt;&lt;br /&gt;You should also write plans and scripts so you can quickly restore the databases and direct traffic to the other data centre etc. It may also require periodic testing of the plans.&lt;br /&gt;&lt;br /&gt;Finally, you should make sure your DNS is set up with a short enough TTL that you can quickly move traffic to the new websites.&lt;br /&gt;&lt;br /&gt;Document storage, such as files, may in the Warm scenario still take up to 48 hours to be made available in the other data centre.&lt;br /&gt;&lt;br /&gt;&lt;h4&gt;Hot&lt;/h4&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-jR6utAG2ch0/WVUtUWoKCoI/AAAAAAAALuI/uwkuqU-cVuIUfrzgbYqs1QsXDfgPzYemgCLcBGAs/s1600/Failover%2BOptions%2B-%2BHot.png&quot; imageanchor=&quot;1&quot; style=&quot;clear: right; float: right; margin-bottom: 1em; margin-left: 1em;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;980&quot; data-original-width=&quot;1180&quot; height=&quot;331&quot; src=&quot;https://1.bp.blogspot.com/-jR6utAG2ch0/WVUtUWoKCoI/AAAAAAAALuI/uwkuqU-cVuIUfrzgbYqs1QsXDfgPzYemgCLcBGAs/s400/Failover%2BOptions%2B-%2BHot.png&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/div&gt;A Hot failover will automatically fail over to a secondary data centre if the primary data centre fails, in whole or in part.&lt;br /&gt;&lt;br /&gt;In practice, there are many different components to a system and it usually makes sense to only have hot failover in place for some of the components. A bespoke cost/benefit exercise should be carried out where hot failover is desired.&lt;br /&gt;&lt;br /&gt;The primary things to consider;&lt;br /&gt;&lt;br /&gt;&lt;h3&gt;Web site failover&lt;/h3&gt;It is possible to deploy the front-end web servers &amp;nbsp;to more than one data centre and use Traffic Manager to automatically direct traffic between the two. This works with most kinds of web hosting you can do in Azure. This means that if the websites in one data centre fails, requests will automatically be served by the other data centre, usually within 1 minute. The main costs are in paying for the extra server(s) and the added complexity in every deployment.&lt;br /&gt;&lt;br /&gt;&lt;h3&gt;Database failover&lt;/h3&gt;Azure offers hot failover for both Azure SQL and CosmosDB, the two main databases. With this failover, Azure will dynamically fail over to a secondary data centre in case of a failure and/or serve requests from both data centres. The mechanisms used by SQL Azure and CosmosDB are fundamentally different and will require different approaches.&lt;br /&gt;&lt;br /&gt;&amp;nbsp;As a general rule, you have to pay for two copies of your database and you may have to use a more expensive service tier.&lt;br /&gt;&lt;br /&gt;In the case of CosmosDB, it may be required to consider consistency levels and the system may need to be adapted to deal with this.&lt;br /&gt;&lt;br /&gt;&lt;h3&gt;Storage failover&lt;/h3&gt;It is common to store files and certain other types of data in Azure Storage. By default, data is backed up to another data centre (though this can be disabled when not required). However, Azure is in control of enabling access to the backup in case of a failure and the SLA is up to 48 hours. In many cases, this is acceptable as the loss of access to historical files may be considered a service degradation rather than a failure.&lt;br /&gt;&lt;br /&gt;Where required, Azure do provide ways to have direct access to a read-only copy in the secondary data centre. This can be utilised to build a very high level of resilience, but it requires explicit programming in your software to take advantage of this.&lt;br /&gt;&lt;br /&gt;&lt;h3&gt;Queue failover&lt;/h3&gt;In an effort to increase resilience and scalability, it is common to use queues in systems; Rather than do something straight away, the system will put a message on a queue and a background job will then process this. This design has many benefits, including automatic retrying, resilience to external systems being down and significant scale benefits as sudden peaks in demand just causes queues to get longer for a little while.&lt;br /&gt;This does, however, mean that the queues can be a single point of failure; if the queue service fails, you can no longer enqueue messages.&lt;br /&gt;&lt;br /&gt;From NewOrbit’s many years of working with Azure, it is clear that Microsoft are very aware of the crucial importance queues play in many systems and they have worked very hard to make them extremely resilient; Despite very extensive usage, NewOrbit has never experienced a failure with “storage queues” and has only experienced an issue with “service bus queues” on a single occasion in 2013.&lt;br /&gt;&lt;br /&gt;It is possible to implement failover for queues and NewOrbit has done that before. There are different approaches that can be taken and Service Bus Queues have some native support for failover, though it does require programming to take full advantage of it.&lt;br /&gt;&lt;br /&gt;&lt;h3&gt;Other items&lt;/h3&gt;There are many other items that can be used in Azure, including virtual machines. For most of these items, a bespoke failover strategy is required to achieve hot failover.&lt;br /&gt;&lt;br /&gt;&lt;h4&gt;More SLA details&lt;/h4&gt;The individual SLAs for all Azure services can be found at https://azure.microsoft.com/en-gb/support/legal/sla/&lt;br /&gt;If you need to report on your overall SLA, it is important to understand how to combine them. If you have, say, an Azure App Service with 99.95% SLA and an Azure SQL database with a 99.99% SLA then the overall SLA for both to be up is (99.95% x 99.99%) = 99.94%. This obviously compounds with more components.&lt;br /&gt;&lt;br /&gt;On the other hand, adding a hot failover App Service in another data centre using Traffic Manager means you now have a better than 99.95% expected SLA for the App Service component. However, calculating the actual SLA is not practical due to the presence of “systemic risk”; There is one risk of a single data centre going down and a separate risk of a worldwide outage of Azure App Services.&lt;br /&gt;&lt;br /&gt;&lt;h4&gt;Help?&lt;/h4&gt;&lt;div&gt;If you have a quick question, ping me on &lt;a href=&quot;https://www.twitter.com/flytzen&quot; target=&quot;_blank&quot;&gt;twitter&lt;/a&gt;.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;If you want more extensive advice and guidance, my company &lt;a href=&quot;http://www.neworbit.co.uk/&quot; target=&quot;_blank&quot;&gt;NewOrbit&lt;/a&gt; offers help to other companies who are moving to Azure. We have been building systems on Azure since 2011 and are a Microsoft Cloud Gold Partner.&lt;/div&gt;&lt;br /&gt;</content><author><name>Frans Lytzen</name></author><category term="Azure" /><summary type="html">Azure provides a highly resilient hosting platform, with significant built-in redundancy, but it can be hard to understand what resilience you get automatically and what you might have to set up yourself.</summary></entry><entry><title type="html">Combine documents with other data in Azure Search</title><link href="https://www.lytzen.name/2017/01/30/combine-documents-with-other-data-in.html" rel="alternate" type="text/html" title="Combine documents with other data in Azure Search" /><published>2017-01-30T13:07:00+00:00</published><updated>2017-01-30T13:07:00+00:00</updated><id>https://www.lytzen.name/2017/01/30/combine-documents-with-other-data-in</id><content type="html" xml:base="https://www.lytzen.name/2017/01/30/combine-documents-with-other-data-in.html">Azure Search has built-in support for indexing documents in blob storage, which works great. But what if you want to &lt;i&gt;combine&lt;/i&gt;&amp;nbsp;the documents with other data, for example if you are building a recruitment system and want to search on, say, location and CV content at the same time?&lt;br /&gt;&lt;br /&gt;&lt;b&gt;TL;DR; &lt;/b&gt;Indexers in Azure Search uses a &quot;create or update&quot; approach; as long as your different indexers use the same &lt;i&gt;id, &lt;/i&gt;they can all write data to the same document in the index.&lt;br /&gt;&lt;br /&gt;&lt;h2&gt;Terminology in Azure Search&lt;/h2&gt;&lt;div&gt;&lt;i&gt;(deliberately simplified and made Azure Search specific)&lt;/i&gt;&lt;br /&gt;&lt;i&gt;&lt;br /&gt;&lt;/i&gt;&lt;/div&gt;&lt;div&gt;An &lt;b&gt;index&lt;/b&gt;&amp;nbsp;in Azure Search is more like a table in an RDBMS or a collection in a document-based database. It has a specific &quot;schema&quot; (the index definition) that specifies the structure of the &lt;b&gt;documents&lt;/b&gt;&lt;i&gt;&amp;nbsp;&lt;/i&gt;stored in the index.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;A &lt;b&gt;document&lt;/b&gt;&amp;nbsp;is a single entity in Azure Search, think &quot;record&quot; or (indeed) document in a document database. It has an &lt;i&gt;id&lt;/i&gt;&amp;nbsp;that is unique within that index.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;An &lt;b&gt;indexer&lt;/b&gt;&amp;nbsp;is something that takes data from a source, like Blob storage, SQL or something else, re-formats it and writes it to documents in an index in Azure Search.&lt;br /&gt;&lt;br /&gt;A &lt;b&gt;data source&lt;/b&gt;&amp;nbsp;is a definition in Azure Search of somewhere that an &lt;b&gt;indexer&lt;/b&gt;&amp;nbsp;can read data from. It's sort of like a connection string.&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;&lt;h2&gt;Scenario&lt;/h2&gt;&lt;div&gt;The specific scenario I have is the need to build a database of candidates for a recruitment system. I want to be able to search the contents of CVs, but I also want to be able to filter and use faceted search on various bits of data about the candidate themselves, such as location, qualification and so on.&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;Azure Search provides built-in functionality to read the contents from a range of files stored in Azure Blob Storage, so that sounded pretty ideal; just upload the CVs to blob storage, point Azure search to it and be done. Of course, that would not combine it with the other candidate data. Initially I thought about several options;&lt;br /&gt;&lt;ul&gt;&lt;li&gt;I could extract the content of the files myself and write code to combine it with candidate data in code. But extracting content out of files is not a trivial problem.&lt;/li&gt;&lt;li&gt;I could let Azure Search index the files, then write code to extract the content back out of Azure Search and write it back to another index. But that seemed like a very long-winded solution.&lt;/li&gt;&lt;li&gt;I asked Microsoft if I could somehow access the content-extraction feature directly. But you can't.&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;&lt;br /&gt;&lt;h2&gt;The solution&lt;/h2&gt;&lt;div&gt;&lt;a href=&quot;https://twitter.com/liamca&quot;&gt;Liam Cavanagh&lt;/a&gt;&amp;nbsp;gave me the outline solution with this statement;&lt;/div&gt;&lt;blockquote class=&quot;tr_bq&quot;&gt;Leverage Blob Storage with Data Store (such as Azure SQL or DocumentDB):&amp;nbsp; If you store your files in Azure Blob storage, and your structured data in something like Azure SQL or DocumentDB, you can use both the Azure Search Blob Indexer as well as the Azure Search SQL or DocumentDB Indexer together.&amp;nbsp; &lt;b&gt;The only trick is to make sure that the unique key that is defined for each document matches between Blob and the structured data store&lt;/b&gt;.&amp;nbsp; For example, if you choose to use the Base64 Encoded filename as the key, then you would need to make sure that the matching content in your structured datastore also contains this value.&amp;nbsp; That way the Indexer which does something called MergeOrUpdate, will first take a row from say the Blob Storage and insert it as a new row in Azure Search and then it will bring in the data from the structured data store and update the existing row with the new fields.&lt;/blockquote&gt;&lt;br /&gt;&lt;div&gt;With a bit more help from Liam I put together the sample solution outlined below. I should also mention that in this solution I am using the built-in tools in Azure Search to read data directly from SQL and Blob storage. If you wanted to use code to manually write to the index instead of letting Azure Search read from SQL then you can do that too, of course. Or read from some of the other supported data sources, or combine it all or...&lt;/div&gt;&lt;br /&gt;&lt;h2&gt;Implementation&lt;/h2&gt;&lt;div&gt;You'll need to create an Azure Search account and whatever data sources you want to use. You can do a number of the steps directly in the Azure Portal, though not all of them. For the ones you can't I will show the appropriate JSON payload you need to send. It is ridiculously easy with something like &lt;a href=&quot;https://www.getpostman.com/&quot;&gt;Postman&lt;/a&gt;. You just need to make sure you add two headers to your requests;&lt;/div&gt;&lt;div&gt;&lt;ul&gt;&lt;li&gt;Content-Type : application/json&lt;/li&gt;&lt;li&gt;api-key : [an admin key for your Azure Search instance]&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;&lt;div style=&quot;height: 480px; margin: 10px; position: relative; width: 640px;&quot;&gt;In summary, this is what we are going to build:&lt;br /&gt;&lt;iframe allowfullscreen=&quot;&quot; frameborder=&quot;0&quot; id=&quot;wGZtguhjzZug&quot; src=&quot;https://www.lucidchart.com/documents/embeddedchart/ce4e64ec-6570-48a9-a412-756e445e9d84&quot; style=&quot;height: 480px; width: 640px;&quot;&gt;&lt;/iframe&gt;&lt;br /&gt;&lt;/div&gt;&lt;br /&gt;&lt;br /&gt;&lt;h4&gt;Create the Index&lt;/h4&gt;&lt;div&gt;You can create a new index directly in the Azure Portal, which is what I did. It has a very nice editor that makes it quite easy. I ended up with something looking like this;&lt;br /&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://2.bp.blogspot.com/-yV3arlG8Z-c/WI41d2CtLnI/AAAAAAAAKSo/Rjj-6L0M0sQvH4EyVoTqMYFKIp0NBMNUACLcB/s1600/CandidatesIndex.PNG&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;268&quot; src=&quot;https://2.bp.blogspot.com/-yV3arlG8Z-c/WI41d2CtLnI/AAAAAAAAKSo/Rjj-6L0M0sQvH4EyVoTqMYFKIp0NBMNUACLcB/s640/CandidatesIndex.PNG&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;Please note the &quot;&lt;b&gt;content&lt;/b&gt;&quot; field; When Azure Search indexes files, it will place the content of those files in the &lt;b&gt;content&lt;/b&gt;&amp;nbsp;field. &lt;b&gt;Id&lt;/b&gt;&amp;nbsp;is there by default and we need that, so leave it alone, but the remaining fields can be whatever you like. I put a &lt;b&gt;name&lt;/b&gt; in there for the Candidate Name and the last two fields are some fields I added so I could experiment with faceted search.&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;&lt;h4&gt;Create the data sources&lt;/h4&gt;&lt;div&gt;Next we need to tell Azure Search where it can get the data - we need to create the Data Sources.&lt;/div&gt;&lt;div&gt;You can't create a Data Source on it's own in the Portal; there is an option to &quot;import data&quot; but that combines setting up the data source with the index and the indexer so we can't use it for our purposes.&lt;br /&gt;Time to start posting JSON (see above).&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;POST these to https://&lt;i&gt;yoursearchservice&lt;/i&gt;.search.windows.net/datasources?api-version=2016-09-01 and replace the XXX with valid connection strings.&lt;br /&gt;&lt;pre class=&quot;prettyprint&quot;&gt;{&lt;br /&gt;    &quot;name&quot; : &quot;blobcvs&quot;,&lt;br /&gt;    &quot;type&quot; : &quot;azureblob&quot;,&lt;br /&gt;    &quot;credentials&quot; : { &quot;connectionString&quot; : &quot;XXX&quot; },&lt;br /&gt;    &quot;container&quot; : { &quot;name&quot; : &quot;cvs&quot; }&lt;br /&gt;}&lt;br /&gt;&lt;br /&gt;{&lt;br /&gt;    &quot;name&quot; : &quot;candidates&quot;,&lt;br /&gt;    &quot;type&quot; : &quot;azuresql&quot;,&lt;br /&gt;    &quot;credentials&quot; : { &quot;connectionString&quot; : &quot;XXX&quot; },&lt;br /&gt;    &quot;container&quot; : { &quot;name&quot; : &quot;Candidates&quot; }&lt;br /&gt;} &lt;br /&gt;&lt;/pre&gt;This tells Azure Search how to access your data.&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;&lt;h4&gt;Create the indexers&lt;/h4&gt;&lt;div&gt;POST these to&amp;nbsp;https://&lt;i&gt;yoursearchservice&lt;/i&gt;.search.windows.net/indexers?api-version=2016-09-01&lt;br /&gt;&lt;pre class=&quot;prettyprint&quot;&gt;{&lt;br /&gt;    &quot;name&quot; : &quot;candidateindexer&quot;,&lt;br /&gt;    &quot;dataSourceName&quot; : &quot;candidates&quot;,&lt;br /&gt;    &quot;targetIndexName&quot; : &quot;candidates&quot;,&lt;br /&gt;    &quot;fieldMappings&quot; : [ { &quot;sourceFieldName&quot; : &quot;Thingiemajics&quot;, &lt;br /&gt;                          &quot;mappingFunction&quot; : { &quot;name&quot; : &quot;jsonArrayToStringCollection&quot; } &lt;br /&gt;                      } ]&lt;br /&gt;}&lt;br /&gt;&lt;/pre&gt;This tells Azure Search to take the data in the SQL database specified in the SQL data source and create a document in Azure Search for each row. Azure Search will &lt;i&gt;automatically &lt;/i&gt;match fields with the same names; I've got an Id field as well as Name, Type and Thingiemajics columns in SQL. The only one that is a bit special is Thingiemajics; I'm storing an array of tag values in that field in SQL in the format [&quot;red&quot;, &quot;white&quot;, &quot;blue&quot;] and putting that mapping function in there tells Azure Search to make them individual tags that can be filtered individually. See &lt;a href=&quot;https://docs.microsoft.com/en-us/azure/search/search-indexer-field-mappings#jsonArrayToStringCollectionFunction&quot;&gt;the docs&lt;/a&gt;&amp;nbsp;for more details.&lt;br /&gt;&lt;br /&gt;Before I create the indexer for the &lt;i&gt;files&lt;/i&gt;, let me just take a little detour. If you remember, the original statement was that for this to work, the two different indexers need to use the &lt;i&gt;same&lt;/i&gt;&amp;nbsp;id for data about the same candidate. The SQL indexer in my example uses the database ID of the candidate and we need to ensure that when Azure Search indexes the CV for a candidate it returns the same index. By &lt;i&gt;default&lt;/i&gt;&amp;nbsp;Azure Search will use the filename, which is obviously no good in this situation. The way I solved this was to add a custom meta data property to the blob when I uploaded it to Azure Blob Storage, something like this:&lt;br /&gt;&lt;br /&gt;&lt;pre class=&quot;prettyprint&quot;&gt;using (var fileStream = System.IO.File.OpenRead(file))&lt;br /&gt;{&lt;br /&gt;   await blob.UploadFromStreamAsync(fileStream);&lt;br /&gt;}&lt;br /&gt;blob.Metadata.Add(&quot;mykey&quot;, identifier);&lt;br /&gt;await blob.SetMetadataAsync();&lt;br /&gt;&lt;/pre&gt;Here I have called it &quot;mykey&quot;, but it could be called anything.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;On to the indexer, which is created with this:&lt;br /&gt;&lt;pre class=&quot;prettyprint&quot;&gt;{&lt;br /&gt;    &quot;name&quot; : &quot;cvindexer&quot;,&lt;br /&gt;    &quot;dataSourceName&quot; : &quot;blobcvs&quot;,&lt;br /&gt;    &quot;targetIndexName&quot; : &quot;candidates&quot;,&lt;br /&gt;    &quot;fieldMappings&quot; : [ { &quot;sourceFieldName&quot; : &quot;mykey&quot;, &quot;targetFieldName&quot; : &quot;id&quot; } ],&lt;br /&gt;    &quot;parameters&quot; : { &quot;configuration&quot; : { &quot;failOnUnsupportedContentType&quot; : false } }&lt;br /&gt;}&lt;br /&gt;&lt;/pre&gt;The most important thing here is that I tell Azure Search to take the metadata property &quot;mykey&quot; and map it to the &quot;id&quot; field in my index. That's all that is required to ensure the contents of the CV ends up in the same search document as the other Candidate information.&lt;/div&gt;&lt;br /&gt;&lt;h2&gt;Notes&lt;/h2&gt;&lt;div&gt;In my solution here I added an ID property to the blob meta data. I could, of course, have gone the other way instead and added the blob reference to the SQL data and then remapped that reference to &quot;id&quot; in the SQL indexer. Both approaches are equally valid, both have pros and cons, including the need to manually maintain uniqueness of the ID property and the need to base64 encode filenames.&lt;br /&gt;&lt;br /&gt;You should also note that this works because the different datasources do not have any of the same field names; if the two indexers both return the same field, one will win and delete the data from the other one.&lt;/div&gt;</content><author><name>Frans Lytzen</name></author><category term="Azure" /><summary type="html">Azure Search has built-in support for indexing documents in blob storage, which works great. But what if you want to combine&amp;nbsp;the documents with other data, for example if you are building a recruitment system and want to search on, say, location and CV content at the same time?TL;DR; Indexers in Azure Search uses a &quot;create or update&quot; approach; as long as your different indexers use the same id, they can all write data to the same document in the index.Terminology in Azure Search(deliberately simplified and made Azure Search specific)An index&amp;nbsp;in Azure Search is more like a table in an RDBMS or a collection in a document-based database. It has a specific &quot;schema&quot; (the index definition) that specifies the structure of the documents&amp;nbsp;stored in the index.A document&amp;nbsp;is a single entity in Azure Search, think &quot;record&quot; or (indeed) document in a document database. It has an id&amp;nbsp;that is unique within that index.An indexer&amp;nbsp;is something that takes data from a source, like Blob storage, SQL or something else, re-formats it and writes it to documents in an index in Azure Search.A data source&amp;nbsp;is a definition in Azure Search of somewhere that an indexer&amp;nbsp;can read data from. It's sort of like a connection string.ScenarioThe specific scenario I have is the need to build a database of candidates for a recruitment system. I want to be able to search the contents of CVs, but I also want to be able to filter and use faceted search on various bits of data about the candidate themselves, such as location, qualification and so on.Azure Search provides built-in functionality to read the contents from a range of files stored in Azure Blob Storage, so that sounded pretty ideal; just upload the CVs to blob storage, point Azure search to it and be done. Of course, that would not combine it with the other candidate data. Initially I thought about several options;I could extract the content of the files myself and write code to combine it with candidate data in code. But extracting content out of files is not a trivial problem.I could let Azure Search index the files, then write code to extract the content back out of Azure Search and write it back to another index. But that seemed like a very long-winded solution.I asked Microsoft if I could somehow access the content-extraction feature directly. But you can't.The solutionLiam Cavanagh&amp;nbsp;gave me the outline solution with this statement;Leverage Blob Storage with Data Store (such as Azure SQL or DocumentDB):&amp;nbsp; If you store your files in Azure Blob storage, and your structured data in something like Azure SQL or DocumentDB, you can use both the Azure Search Blob Indexer as well as the Azure Search SQL or DocumentDB Indexer together.&amp;nbsp; The only trick is to make sure that the unique key that is defined for each document matches between Blob and the structured data store.&amp;nbsp; For example, if you choose to use the Base64 Encoded filename as the key, then you would need to make sure that the matching content in your structured datastore also contains this value.&amp;nbsp; That way the Indexer which does something called MergeOrUpdate, will first take a row from say the Blob Storage and insert it as a new row in Azure Search and then it will bring in the data from the structured data store and update the existing row with the new fields.With a bit more help from Liam I put together the sample solution outlined below. I should also mention that in this solution I am using the built-in tools in Azure Search to read data directly from SQL and Blob storage. If you wanted to use code to manually write to the index instead of letting Azure Search read from SQL then you can do that too, of course. Or read from some of the other supported data sources, or combine it all or...ImplementationYou'll need to create an Azure Search account and whatever data sources you want to use. You can do a number of the steps directly in the Azure Portal, though not all of them. For the ones you can't I will show the appropriate JSON payload you need to send. It is ridiculously easy with something like Postman. You just need to make sure you add two headers to your requests;Content-Type : application/jsonapi-key : [an admin key for your Azure Search instance]In summary, this is what we are going to build:Create the IndexYou can create a new index directly in the Azure Portal, which is what I did. It has a very nice editor that makes it quite easy. I ended up with something looking like this;Please note the &quot;content&quot; field; When Azure Search indexes files, it will place the content of those files in the content&amp;nbsp;field. Id&amp;nbsp;is there by default and we need that, so leave it alone, but the remaining fields can be whatever you like. I put a name in there for the Candidate Name and the last two fields are some fields I added so I could experiment with faceted search.Create the data sourcesNext we need to tell Azure Search where it can get the data - we need to create the Data Sources.You can't create a Data Source on it's own in the Portal; there is an option to &quot;import data&quot; but that combines setting up the data source with the index and the indexer so we can't use it for our purposes.Time to start posting JSON (see above).POST these to https://yoursearchservice.search.windows.net/datasources?api-version=2016-09-01 and replace the XXX with valid connection strings.{ &quot;name&quot; : &quot;blobcvs&quot;, &quot;type&quot; : &quot;azureblob&quot;, &quot;credentials&quot; : { &quot;connectionString&quot; : &quot;XXX&quot; }, &quot;container&quot; : { &quot;name&quot; : &quot;cvs&quot; }}{ &quot;name&quot; : &quot;candidates&quot;, &quot;type&quot; : &quot;azuresql&quot;, &quot;credentials&quot; : { &quot;connectionString&quot; : &quot;XXX&quot; }, &quot;container&quot; : { &quot;name&quot; : &quot;Candidates&quot; }} This tells Azure Search how to access your data.Create the indexersPOST these to&amp;nbsp;https://yoursearchservice.search.windows.net/indexers?api-version=2016-09-01{ &quot;name&quot; : &quot;candidateindexer&quot;, &quot;dataSourceName&quot; : &quot;candidates&quot;, &quot;targetIndexName&quot; : &quot;candidates&quot;, &quot;fieldMappings&quot; : [ { &quot;sourceFieldName&quot; : &quot;Thingiemajics&quot;, &quot;mappingFunction&quot; : { &quot;name&quot; : &quot;jsonArrayToStringCollection&quot; } } ]}This tells Azure Search to take the data in the SQL database specified in the SQL data source and create a document in Azure Search for each row. Azure Search will automatically match fields with the same names; I've got an Id field as well as Name, Type and Thingiemajics columns in SQL. The only one that is a bit special is Thingiemajics; I'm storing an array of tag values in that field in SQL in the format [&quot;red&quot;, &quot;white&quot;, &quot;blue&quot;] and putting that mapping function in there tells Azure Search to make them individual tags that can be filtered individually. See the docs&amp;nbsp;for more details.Before I create the indexer for the files, let me just take a little detour. If you remember, the original statement was that for this to work, the two different indexers need to use the same&amp;nbsp;id for data about the same candidate. The SQL indexer in my example uses the database ID of the candidate and we need to ensure that when Azure Search indexes the CV for a candidate it returns the same index. By default&amp;nbsp;Azure Search will use the filename, which is obviously no good in this situation. The way I solved this was to add a custom meta data property to the blob when I uploaded it to Azure Blob Storage, something like this:using (var fileStream = System.IO.File.OpenRead(file)){ await blob.UploadFromStreamAsync(fileStream);}blob.Metadata.Add(&quot;mykey&quot;, identifier);await blob.SetMetadataAsync();Here I have called it &quot;mykey&quot;, but it could be called anything.On to the indexer, which is created with this:{ &quot;name&quot; : &quot;cvindexer&quot;, &quot;dataSourceName&quot; : &quot;blobcvs&quot;, &quot;targetIndexName&quot; : &quot;candidates&quot;, &quot;fieldMappings&quot; : [ { &quot;sourceFieldName&quot; : &quot;mykey&quot;, &quot;targetFieldName&quot; : &quot;id&quot; } ], &quot;parameters&quot; : { &quot;configuration&quot; : { &quot;failOnUnsupportedContentType&quot; : false } }}The most important thing here is that I tell Azure Search to take the metadata property &quot;mykey&quot; and map it to the &quot;id&quot; field in my index. That's all that is required to ensure the contents of the CV ends up in the same search document as the other Candidate information.NotesIn my solution here I added an ID property to the blob meta data. I could, of course, have gone the other way instead and added the blob reference to the SQL data and then remapped that reference to &quot;id&quot; in the SQL indexer. Both approaches are equally valid, both have pros and cons, including the need to manually maintain uniqueness of the ID property and the need to base64 encode filenames.You should also note that this works because the different datasources do not have any of the same field names; if the two indexers both return the same field, one will win and delete the data from the other one.</summary></entry></feed>