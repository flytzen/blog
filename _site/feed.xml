<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.7.3">Jekyll</generator><link href="https://www.lytzen.name/feed.xml" rel="self" type="application/atom+xml" /><link href="https://www.lytzen.name/" rel="alternate" type="text/html" /><updated>2018-04-21T21:10:37+01:00</updated><id>https://www.lytzen.name/</id><title type="html">Frans’ Randomness</title><subtitle>My very infrequent thoughts on the world of software development</subtitle><entry><title type="html">Azure Failover and Resilience</title><link href="https://www.lytzen.name/2017/06/29/azure-failover-and-resilience.html" rel="alternate" type="text/html" title="Azure Failover and Resilience" /><published>2017-06-29T17:54:00+01:00</published><updated>2017-06-29T17:54:00+01:00</updated><id>https://www.lytzen.name/2017/06/29/azure-failover-and-resilience</id><content type="html" xml:base="https://www.lytzen.name/2017/06/29/azure-failover-and-resilience.html">Azure provides a highly resilient hosting platform, with significant built-in redundancy within a data centre, as well as the presence of more than 30 data centres across the world.&lt;br /&gt;&lt;br /&gt;When first coming to Azure, it can be hard to understand what resilience you get automatically and what you might have to set up yourself.&lt;br /&gt;&lt;br /&gt;This post provides a high-level overview of the principles. It is intended as an introduction to help you ask the right questions.&lt;br /&gt;&lt;br /&gt;The usual starting point for a system is to host it in a single data centre. Azure is highly resilient even within a single data centre, but even so, all the data is continually backed up to a secondary data centre.&lt;br /&gt;&lt;br /&gt;In the case of a complete failure of a data centre, the data can be restored to another data centre. This is not the same as automatic failover to another data centre; In order to get the data restored in the other data centre and get the system back up and running, you will have to do it yourself; Azure will (for the most part) not do this for you. How much work depends on how much preparatory work has been done and is primarily a business decision based on risk and cost.&lt;br /&gt;&lt;br /&gt;Any conversation about failover is complicated by the fact that a system consists of different components, which can fail independently, which have different probability and impact and which require different failover strategies.&lt;br /&gt;&lt;br /&gt;Before going into the details, it is important to understand that even the most basic setup in Azure has a very high level of resilience with each individual component typically having a guaranteed uptime of 99.95% or more. At the same time, data is continually backed up to a secondary data centre. In other words, even the most basic Azure setup has a level of resilience that is difficult and expensive to achieve with on-premise hosting.&lt;br /&gt;&lt;br /&gt;In this post “failover” will refer to failing over between data centres.&lt;br /&gt;&lt;br /&gt;&lt;h4&gt;Resilience in a single data centre&lt;/h4&gt;&lt;div&gt;Azure Data Centres are built in a modular way, meaning that each data centre can be thought of as many smaller data centres built next to each other. This means that your data and system will be physically spread over different parts of the data centre, in turn meaning that even if an entire part of the data centre fails, you are unlikely to notice.&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;Physically, all Azure data centres have multiple redundant power grid connections, multiple redundant internet connections, redundant backup generators, batteries and so on and so forth.&lt;br /&gt;&lt;br /&gt;As a general rule, any data you save in Azure, in databases, to disk or to other types of storage, is written to three different locations inside that one data centre and a single copy is written to another remote data centre as a backup. For example the London data centre backs up to Cardiff and the Amsterdam data centre backs up to Dublin etc.&lt;br /&gt;&lt;br /&gt;Azure App Service has some built-in resilience so even with only a single compute node in your app hosting plan, you are pretty well protected from outages. With Cloud Services, you must ensure that you have at least two instances running at all times to ensure resilience. With Virtual Machines – you are much more on your own, though there are a number of things you can configure, such as Availability Sets etc. As a &lt;i&gt;very&lt;/i&gt; general rule, to make reliability easier, avoid using VMs, use one of the managed options instead, when you can.&lt;br /&gt;&lt;br /&gt;When you want to be able to fail over to another data centre, there are several options available to you. I have grouped them here under “Cold”, “Warm” and “Hot”. These are just convenience labels and may not correlate to other people’s definitions.&lt;br /&gt;&lt;br /&gt;&lt;h4&gt;Cold&lt;/h4&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://4.bp.blogspot.com/-wV3fbqWMgeU/WVUssFhNeqI/AAAAAAAALuA/n5RZdmPeCqM4Jr6snqblsOxVj7fUfEyvQCLcBGAs/s1600/Failover%2BOptions%2B-%2BCold.png&quot; imageanchor=&quot;1&quot; style=&quot;clear: right; float: right; margin-bottom: 1em; margin-left: 1em;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;560&quot; data-original-width=&quot;1180&quot; height=&quot;188&quot; src=&quot;https://4.bp.blogspot.com/-wV3fbqWMgeU/WVUssFhNeqI/AAAAAAAALuA/n5RZdmPeCqM4Jr6snqblsOxVj7fUfEyvQCLcBGAs/s400/Failover%2BOptions%2B-%2BCold.png&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;div&gt;A Cold failover is what you get by default.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;Your data is automatically backed up to another data centre. In the case of a failure of the primary data centre, you can go to the other data centre, set up your systems again, deploy everything and restore your database. Of course, the more automated your deployment is, the easier this will be.&lt;br /&gt;&lt;br /&gt;You should be aware that while you can manually trigger a restore of SQL and CosmosDB databases, you cannot yourself trigger a “restore” of anything you put into Azure Storage. Microsoft has to do that by changing DNS entries and their SLA on that is up to 48 hours, last time I checked. There are things you can do to improve this, such as using read-access geo-redundant storage, but you will need to develop specifically to take advantage of that. Often, though, the data in Azure Storage is secondary to the main system and you may be able to live without that data for a day or two.&lt;br /&gt;&lt;br /&gt;The exact frequency of database backups depends on the chosen database but is generally four hours or less.&lt;br /&gt;&lt;br /&gt;&lt;h4&gt;Warm&lt;/h4&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://4.bp.blogspot.com/-gk1abFDNI6A/WVUtOZGJaeI/AAAAAAAALuE/gEM_835uULUHb8zijsbskKBwbgclBf2zgCLcBGAs/s1600/Failover%2BOptions%2B-%2BWarm.png&quot; imageanchor=&quot;1&quot; style=&quot;clear: right; float: right; margin-bottom: 1em; margin-left: 1em;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;560&quot; data-original-width=&quot;1180&quot; height=&quot;188&quot; src=&quot;https://4.bp.blogspot.com/-gk1abFDNI6A/WVUtOZGJaeI/AAAAAAAALuE/gEM_835uULUHb8zijsbskKBwbgclBf2zgCLcBGAs/s400/Failover%2BOptions%2B-%2BWarm.png&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/div&gt;A Warm failover allows faster fail-over to a secondary data centre, but still requires manual intervention.&lt;br /&gt;&lt;br /&gt;In order to reduce the time it takes to move to a secondary data centre, it is possible to prepare the infrastructure and have detailed plans in place. You can do this by configuring the whole system in the secondary data centre but not deploy anything to it; For many services you can define it but just not deploy anything to it. Similarly, you can deploy VMs and then de-allocate them etc. An alternative is to create an ARM template, which will allow you to quickly create a whole environment in Azure.&lt;br /&gt;&lt;br /&gt;You should also write plans and scripts so you can quickly restore the databases and direct traffic to the other data centre etc. It may also require periodic testing of the plans.&lt;br /&gt;&lt;br /&gt;Finally, you should make sure your DNS is set up with a short enough TTL that you can quickly move traffic to the new websites.&lt;br /&gt;&lt;br /&gt;Document storage, such as files, may in the Warm scenario still take up to 48 hours to be made available in the other data centre.&lt;br /&gt;&lt;br /&gt;&lt;h4&gt;Hot&lt;/h4&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-jR6utAG2ch0/WVUtUWoKCoI/AAAAAAAALuI/uwkuqU-cVuIUfrzgbYqs1QsXDfgPzYemgCLcBGAs/s1600/Failover%2BOptions%2B-%2BHot.png&quot; imageanchor=&quot;1&quot; style=&quot;clear: right; float: right; margin-bottom: 1em; margin-left: 1em;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;980&quot; data-original-width=&quot;1180&quot; height=&quot;331&quot; src=&quot;https://1.bp.blogspot.com/-jR6utAG2ch0/WVUtUWoKCoI/AAAAAAAALuI/uwkuqU-cVuIUfrzgbYqs1QsXDfgPzYemgCLcBGAs/s400/Failover%2BOptions%2B-%2BHot.png&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/div&gt;A Hot failover will automatically fail over to a secondary data centre if the primary data centre fails, in whole or in part.&lt;br /&gt;&lt;br /&gt;In practice, there are many different components to a system and it usually makes sense to only have hot failover in place for some of the components. A bespoke cost/benefit exercise should be carried out where hot failover is desired.&lt;br /&gt;&lt;br /&gt;The primary things to consider;&lt;br /&gt;&lt;br /&gt;&lt;h3&gt;Web site failover&lt;/h3&gt;It is possible to deploy the front-end web servers &amp;nbsp;to more than one data centre and use Traffic Manager to automatically direct traffic between the two. This works with most kinds of web hosting you can do in Azure. This means that if the websites in one data centre fails, requests will automatically be served by the other data centre, usually within 1 minute. The main costs are in paying for the extra server(s) and the added complexity in every deployment.&lt;br /&gt;&lt;br /&gt;&lt;h3&gt;Database failover&lt;/h3&gt;Azure offers hot failover for both Azure SQL and CosmosDB, the two main databases. With this failover, Azure will dynamically fail over to a secondary data centre in case of a failure and/or serve requests from both data centres. The mechanisms used by SQL Azure and CosmosDB are fundamentally different and will require different approaches.&lt;br /&gt;&lt;br /&gt;&amp;nbsp;As a general rule, you have to pay for two copies of your database and you may have to use a more expensive service tier.&lt;br /&gt;&lt;br /&gt;In the case of CosmosDB, it may be required to consider consistency levels and the system may need to be adapted to deal with this.&lt;br /&gt;&lt;br /&gt;&lt;h3&gt;Storage failover&lt;/h3&gt;It is common to store files and certain other types of data in Azure Storage. By default, data is backed up to another data centre (though this can be disabled when not required). However, Azure is in control of enabling access to the backup in case of a failure and the SLA is up to 48 hours. In many cases, this is acceptable as the loss of access to historical files may be considered a service degradation rather than a failure.&lt;br /&gt;&lt;br /&gt;Where required, Azure do provide ways to have direct access to a read-only copy in the secondary data centre. This can be utilised to build a very high level of resilience, but it requires explicit programming in your software to take advantage of this.&lt;br /&gt;&lt;br /&gt;&lt;h3&gt;Queue failover&lt;/h3&gt;In an effort to increase resilience and scalability, it is common to use queues in systems; Rather than do something straight away, the system will put a message on a queue and a background job will then process this. This design has many benefits, including automatic retrying, resilience to external systems being down and significant scale benefits as sudden peaks in demand just causes queues to get longer for a little while.&lt;br /&gt;This does, however, mean that the queues can be a single point of failure; if the queue service fails, you can no longer enqueue messages.&lt;br /&gt;&lt;br /&gt;From NewOrbit’s many years of working with Azure, it is clear that Microsoft are very aware of the crucial importance queues play in many systems and they have worked very hard to make them extremely resilient; Despite very extensive usage, NewOrbit has never experienced a failure with “storage queues” and has only experienced an issue with “service bus queues” on a single occasion in 2013.&lt;br /&gt;&lt;br /&gt;It is possible to implement failover for queues and NewOrbit has done that before. There are different approaches that can be taken and Service Bus Queues have some native support for failover, though it does require programming to take full advantage of it.&lt;br /&gt;&lt;br /&gt;&lt;h3&gt;Other items&lt;/h3&gt;There are many other items that can be used in Azure, including virtual machines. For most of these items, a bespoke failover strategy is required to achieve hot failover.&lt;br /&gt;&lt;br /&gt;&lt;h4&gt;More SLA details&lt;/h4&gt;The individual SLAs for all Azure services can be found at https://azure.microsoft.com/en-gb/support/legal/sla/&lt;br /&gt;If you need to report on your overall SLA, it is important to understand how to combine them. If you have, say, an Azure App Service with 99.95% SLA and an Azure SQL database with a 99.99% SLA then the overall SLA for both to be up is (99.95% x 99.99%) = 99.94%. This obviously compounds with more components.&lt;br /&gt;&lt;br /&gt;On the other hand, adding a hot failover App Service in another data centre using Traffic Manager means you now have a better than 99.95% expected SLA for the App Service component. However, calculating the actual SLA is not practical due to the presence of “systemic risk”; There is one risk of a single data centre going down and a separate risk of a worldwide outage of Azure App Services.&lt;br /&gt;&lt;br /&gt;&lt;h4&gt;Help?&lt;/h4&gt;&lt;div&gt;If you have a quick question, ping me on &lt;a href=&quot;https://www.twitter.com/flytzen&quot; target=&quot;_blank&quot;&gt;twitter&lt;/a&gt;.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;If you want more extensive advice and guidance, my company &lt;a href=&quot;http://www.neworbit.co.uk/&quot; target=&quot;_blank&quot;&gt;NewOrbit&lt;/a&gt; offers help to other companies who are moving to Azure. We have been building systems on Azure since 2011 and are a Microsoft Cloud Gold Partner.&lt;/div&gt;&lt;br /&gt;</content><author><name>Frans Lytzen</name></author><category term="Azure" /><summary type="html">Azure provides a highly resilient hosting platform, with significant built-in redundancy, but it can be hard to understand what resilience you get automatically and what you might have to set up yourself.</summary></entry><entry><title type="html">Combine documents with other data in Azure Search</title><link href="https://www.lytzen.name/2017/01/30/combine-documents-with-other-data-in.html" rel="alternate" type="text/html" title="Combine documents with other data in Azure Search" /><published>2017-01-30T13:07:00+00:00</published><updated>2017-01-30T13:07:00+00:00</updated><id>https://www.lytzen.name/2017/01/30/combine-documents-with-other-data-in</id><content type="html" xml:base="https://www.lytzen.name/2017/01/30/combine-documents-with-other-data-in.html">Azure Search has built-in support for indexing documents in blob storage, which works great. But what if you want to &lt;i&gt;combine&lt;/i&gt;&amp;nbsp;the documents with other data, for example if you are building a recruitment system and want to search on, say, location and CV content at the same time?&lt;br /&gt;&lt;br /&gt;&lt;b&gt;TL;DR; &lt;/b&gt;Indexers in Azure Search uses a &quot;create or update&quot; approach; as long as your different indexers use the same &lt;i&gt;id, &lt;/i&gt;they can all write data to the same document in the index.&lt;br /&gt;&lt;br /&gt;&lt;h2&gt;Terminology in Azure Search&lt;/h2&gt;&lt;div&gt;&lt;i&gt;(deliberately simplified and made Azure Search specific)&lt;/i&gt;&lt;br /&gt;&lt;i&gt;&lt;br /&gt;&lt;/i&gt;&lt;/div&gt;&lt;div&gt;An &lt;b&gt;index&lt;/b&gt;&amp;nbsp;in Azure Search is more like a table in an RDBMS or a collection in a document-based database. It has a specific &quot;schema&quot; (the index definition) that specifies the structure of the &lt;b&gt;documents&lt;/b&gt;&lt;i&gt;&amp;nbsp;&lt;/i&gt;stored in the index.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;A &lt;b&gt;document&lt;/b&gt;&amp;nbsp;is a single entity in Azure Search, think &quot;record&quot; or (indeed) document in a document database. It has an &lt;i&gt;id&lt;/i&gt;&amp;nbsp;that is unique within that index.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;An &lt;b&gt;indexer&lt;/b&gt;&amp;nbsp;is something that takes data from a source, like Blob storage, SQL or something else, re-formats it and writes it to documents in an index in Azure Search.&lt;br /&gt;&lt;br /&gt;A &lt;b&gt;data source&lt;/b&gt;&amp;nbsp;is a definition in Azure Search of somewhere that an &lt;b&gt;indexer&lt;/b&gt;&amp;nbsp;can read data from. It's sort of like a connection string.&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;&lt;h2&gt;Scenario&lt;/h2&gt;&lt;div&gt;The specific scenario I have is the need to build a database of candidates for a recruitment system. I want to be able to search the contents of CVs, but I also want to be able to filter and use faceted search on various bits of data about the candidate themselves, such as location, qualification and so on.&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;Azure Search provides built-in functionality to read the contents from a range of files stored in Azure Blob Storage, so that sounded pretty ideal; just upload the CVs to blob storage, point Azure search to it and be done. Of course, that would not combine it with the other candidate data. Initially I thought about several options;&lt;br /&gt;&lt;ul&gt;&lt;li&gt;I could extract the content of the files myself and write code to combine it with candidate data in code. But extracting content out of files is not a trivial problem.&lt;/li&gt;&lt;li&gt;I could let Azure Search index the files, then write code to extract the content back out of Azure Search and write it back to another index. But that seemed like a very long-winded solution.&lt;/li&gt;&lt;li&gt;I asked Microsoft if I could somehow access the content-extraction feature directly. But you can't.&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;&lt;br /&gt;&lt;h2&gt;The solution&lt;/h2&gt;&lt;div&gt;&lt;a href=&quot;https://twitter.com/liamca&quot;&gt;Liam Cavanagh&lt;/a&gt;&amp;nbsp;gave me the outline solution with this statement;&lt;/div&gt;&lt;blockquote class=&quot;tr_bq&quot;&gt;Leverage Blob Storage with Data Store (such as Azure SQL or DocumentDB):&amp;nbsp; If you store your files in Azure Blob storage, and your structured data in something like Azure SQL or DocumentDB, you can use both the Azure Search Blob Indexer as well as the Azure Search SQL or DocumentDB Indexer together.&amp;nbsp; &lt;b&gt;The only trick is to make sure that the unique key that is defined for each document matches between Blob and the structured data store&lt;/b&gt;.&amp;nbsp; For example, if you choose to use the Base64 Encoded filename as the key, then you would need to make sure that the matching content in your structured datastore also contains this value.&amp;nbsp; That way the Indexer which does something called MergeOrUpdate, will first take a row from say the Blob Storage and insert it as a new row in Azure Search and then it will bring in the data from the structured data store and update the existing row with the new fields.&lt;/blockquote&gt;&lt;br /&gt;&lt;div&gt;With a bit more help from Liam I put together the sample solution outlined below. I should also mention that in this solution I am using the built-in tools in Azure Search to read data directly from SQL and Blob storage. If you wanted to use code to manually write to the index instead of letting Azure Search read from SQL then you can do that too, of course. Or read from some of the other supported data sources, or combine it all or...&lt;/div&gt;&lt;br /&gt;&lt;h2&gt;Implementation&lt;/h2&gt;&lt;div&gt;You'll need to create an Azure Search account and whatever data sources you want to use. You can do a number of the steps directly in the Azure Portal, though not all of them. For the ones you can't I will show the appropriate JSON payload you need to send. It is ridiculously easy with something like &lt;a href=&quot;https://www.getpostman.com/&quot;&gt;Postman&lt;/a&gt;. You just need to make sure you add two headers to your requests;&lt;/div&gt;&lt;div&gt;&lt;ul&gt;&lt;li&gt;Content-Type : application/json&lt;/li&gt;&lt;li&gt;api-key : [an admin key for your Azure Search instance]&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;&lt;div style=&quot;height: 480px; margin: 10px; position: relative; width: 640px;&quot;&gt;In summary, this is what we are going to build:&lt;br /&gt;&lt;iframe allowfullscreen=&quot;&quot; frameborder=&quot;0&quot; id=&quot;wGZtguhjzZug&quot; src=&quot;https://www.lucidchart.com/documents/embeddedchart/ce4e64ec-6570-48a9-a412-756e445e9d84&quot; style=&quot;height: 480px; width: 640px;&quot;&gt;&lt;/iframe&gt;&lt;br /&gt;&lt;/div&gt;&lt;br /&gt;&lt;br /&gt;&lt;h4&gt;Create the Index&lt;/h4&gt;&lt;div&gt;You can create a new index directly in the Azure Portal, which is what I did. It has a very nice editor that makes it quite easy. I ended up with something looking like this;&lt;br /&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://2.bp.blogspot.com/-yV3arlG8Z-c/WI41d2CtLnI/AAAAAAAAKSo/Rjj-6L0M0sQvH4EyVoTqMYFKIp0NBMNUACLcB/s1600/CandidatesIndex.PNG&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;268&quot; src=&quot;https://2.bp.blogspot.com/-yV3arlG8Z-c/WI41d2CtLnI/AAAAAAAAKSo/Rjj-6L0M0sQvH4EyVoTqMYFKIp0NBMNUACLcB/s640/CandidatesIndex.PNG&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;Please note the &quot;&lt;b&gt;content&lt;/b&gt;&quot; field; When Azure Search indexes files, it will place the content of those files in the &lt;b&gt;content&lt;/b&gt;&amp;nbsp;field. &lt;b&gt;Id&lt;/b&gt;&amp;nbsp;is there by default and we need that, so leave it alone, but the remaining fields can be whatever you like. I put a &lt;b&gt;name&lt;/b&gt; in there for the Candidate Name and the last two fields are some fields I added so I could experiment with faceted search.&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;&lt;h4&gt;Create the data sources&lt;/h4&gt;&lt;div&gt;Next we need to tell Azure Search where it can get the data - we need to create the Data Sources.&lt;/div&gt;&lt;div&gt;You can't create a Data Source on it's own in the Portal; there is an option to &quot;import data&quot; but that combines setting up the data source with the index and the indexer so we can't use it for our purposes.&lt;br /&gt;Time to start posting JSON (see above).&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;POST these to https://&lt;i&gt;yoursearchservice&lt;/i&gt;.search.windows.net/datasources?api-version=2016-09-01 and replace the XXX with valid connection strings.&lt;br /&gt;&lt;pre class=&quot;prettyprint&quot;&gt;{&lt;br /&gt;    &quot;name&quot; : &quot;blobcvs&quot;,&lt;br /&gt;    &quot;type&quot; : &quot;azureblob&quot;,&lt;br /&gt;    &quot;credentials&quot; : { &quot;connectionString&quot; : &quot;XXX&quot; },&lt;br /&gt;    &quot;container&quot; : { &quot;name&quot; : &quot;cvs&quot; }&lt;br /&gt;}&lt;br /&gt;&lt;br /&gt;{&lt;br /&gt;    &quot;name&quot; : &quot;candidates&quot;,&lt;br /&gt;    &quot;type&quot; : &quot;azuresql&quot;,&lt;br /&gt;    &quot;credentials&quot; : { &quot;connectionString&quot; : &quot;XXX&quot; },&lt;br /&gt;    &quot;container&quot; : { &quot;name&quot; : &quot;Candidates&quot; }&lt;br /&gt;} &lt;br /&gt;&lt;/pre&gt;This tells Azure Search how to access your data.&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;&lt;h4&gt;Create the indexers&lt;/h4&gt;&lt;div&gt;POST these to&amp;nbsp;https://&lt;i&gt;yoursearchservice&lt;/i&gt;.search.windows.net/indexers?api-version=2016-09-01&lt;br /&gt;&lt;pre class=&quot;prettyprint&quot;&gt;{&lt;br /&gt;    &quot;name&quot; : &quot;candidateindexer&quot;,&lt;br /&gt;    &quot;dataSourceName&quot; : &quot;candidates&quot;,&lt;br /&gt;    &quot;targetIndexName&quot; : &quot;candidates&quot;,&lt;br /&gt;    &quot;fieldMappings&quot; : [ { &quot;sourceFieldName&quot; : &quot;Thingiemajics&quot;, &lt;br /&gt;                          &quot;mappingFunction&quot; : { &quot;name&quot; : &quot;jsonArrayToStringCollection&quot; } &lt;br /&gt;                      } ]&lt;br /&gt;}&lt;br /&gt;&lt;/pre&gt;This tells Azure Search to take the data in the SQL database specified in the SQL data source and create a document in Azure Search for each row. Azure Search will &lt;i&gt;automatically &lt;/i&gt;match fields with the same names; I've got an Id field as well as Name, Type and Thingiemajics columns in SQL. The only one that is a bit special is Thingiemajics; I'm storing an array of tag values in that field in SQL in the format [&quot;red&quot;, &quot;white&quot;, &quot;blue&quot;] and putting that mapping function in there tells Azure Search to make them individual tags that can be filtered individually. See &lt;a href=&quot;https://docs.microsoft.com/en-us/azure/search/search-indexer-field-mappings#jsonArrayToStringCollectionFunction&quot;&gt;the docs&lt;/a&gt;&amp;nbsp;for more details.&lt;br /&gt;&lt;br /&gt;Before I create the indexer for the &lt;i&gt;files&lt;/i&gt;, let me just take a little detour. If you remember, the original statement was that for this to work, the two different indexers need to use the &lt;i&gt;same&lt;/i&gt;&amp;nbsp;id for data about the same candidate. The SQL indexer in my example uses the database ID of the candidate and we need to ensure that when Azure Search indexes the CV for a candidate it returns the same index. By &lt;i&gt;default&lt;/i&gt;&amp;nbsp;Azure Search will use the filename, which is obviously no good in this situation. The way I solved this was to add a custom meta data property to the blob when I uploaded it to Azure Blob Storage, something like this:&lt;br /&gt;&lt;br /&gt;&lt;pre class=&quot;prettyprint&quot;&gt;using (var fileStream = System.IO.File.OpenRead(file))&lt;br /&gt;{&lt;br /&gt;   await blob.UploadFromStreamAsync(fileStream);&lt;br /&gt;}&lt;br /&gt;blob.Metadata.Add(&quot;mykey&quot;, identifier);&lt;br /&gt;await blob.SetMetadataAsync();&lt;br /&gt;&lt;/pre&gt;Here I have called it &quot;mykey&quot;, but it could be called anything.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;On to the indexer, which is created with this:&lt;br /&gt;&lt;pre class=&quot;prettyprint&quot;&gt;{&lt;br /&gt;    &quot;name&quot; : &quot;cvindexer&quot;,&lt;br /&gt;    &quot;dataSourceName&quot; : &quot;blobcvs&quot;,&lt;br /&gt;    &quot;targetIndexName&quot; : &quot;candidates&quot;,&lt;br /&gt;    &quot;fieldMappings&quot; : [ { &quot;sourceFieldName&quot; : &quot;mykey&quot;, &quot;targetFieldName&quot; : &quot;id&quot; } ],&lt;br /&gt;    &quot;parameters&quot; : { &quot;configuration&quot; : { &quot;failOnUnsupportedContentType&quot; : false } }&lt;br /&gt;}&lt;br /&gt;&lt;/pre&gt;The most important thing here is that I tell Azure Search to take the metadata property &quot;mykey&quot; and map it to the &quot;id&quot; field in my index. That's all that is required to ensure the contents of the CV ends up in the same search document as the other Candidate information.&lt;/div&gt;&lt;br /&gt;&lt;h2&gt;Notes&lt;/h2&gt;&lt;div&gt;In my solution here I added an ID property to the blob meta data. I could, of course, have gone the other way instead and added the blob reference to the SQL data and then remapped that reference to &quot;id&quot; in the SQL indexer. Both approaches are equally valid, both have pros and cons, including the need to manually maintain uniqueness of the ID property and the need to base64 encode filenames.&lt;br /&gt;&lt;br /&gt;You should also note that this works because the different datasources do not have any of the same field names; if the two indexers both return the same field, one will win and delete the data from the other one.&lt;/div&gt;</content><author><name>Frans Lytzen</name></author><category term="Azure" /><summary type="html">Azure Search has built-in support for indexing documents in blob storage, which works great. But what if you want to combine&amp;nbsp;the documents with other data, for example if you are building a recruitment system and want to search on, say, location and CV content at the same time?TL;DR; Indexers in Azure Search uses a &quot;create or update&quot; approach; as long as your different indexers use the same id, they can all write data to the same document in the index.Terminology in Azure Search(deliberately simplified and made Azure Search specific)An index&amp;nbsp;in Azure Search is more like a table in an RDBMS or a collection in a document-based database. It has a specific &quot;schema&quot; (the index definition) that specifies the structure of the documents&amp;nbsp;stored in the index.A document&amp;nbsp;is a single entity in Azure Search, think &quot;record&quot; or (indeed) document in a document database. It has an id&amp;nbsp;that is unique within that index.An indexer&amp;nbsp;is something that takes data from a source, like Blob storage, SQL or something else, re-formats it and writes it to documents in an index in Azure Search.A data source&amp;nbsp;is a definition in Azure Search of somewhere that an indexer&amp;nbsp;can read data from. It's sort of like a connection string.ScenarioThe specific scenario I have is the need to build a database of candidates for a recruitment system. I want to be able to search the contents of CVs, but I also want to be able to filter and use faceted search on various bits of data about the candidate themselves, such as location, qualification and so on.Azure Search provides built-in functionality to read the contents from a range of files stored in Azure Blob Storage, so that sounded pretty ideal; just upload the CVs to blob storage, point Azure search to it and be done. Of course, that would not combine it with the other candidate data. Initially I thought about several options;I could extract the content of the files myself and write code to combine it with candidate data in code. But extracting content out of files is not a trivial problem.I could let Azure Search index the files, then write code to extract the content back out of Azure Search and write it back to another index. But that seemed like a very long-winded solution.I asked Microsoft if I could somehow access the content-extraction feature directly. But you can't.The solutionLiam Cavanagh&amp;nbsp;gave me the outline solution with this statement;Leverage Blob Storage with Data Store (such as Azure SQL or DocumentDB):&amp;nbsp; If you store your files in Azure Blob storage, and your structured data in something like Azure SQL or DocumentDB, you can use both the Azure Search Blob Indexer as well as the Azure Search SQL or DocumentDB Indexer together.&amp;nbsp; The only trick is to make sure that the unique key that is defined for each document matches between Blob and the structured data store.&amp;nbsp; For example, if you choose to use the Base64 Encoded filename as the key, then you would need to make sure that the matching content in your structured datastore also contains this value.&amp;nbsp; That way the Indexer which does something called MergeOrUpdate, will first take a row from say the Blob Storage and insert it as a new row in Azure Search and then it will bring in the data from the structured data store and update the existing row with the new fields.With a bit more help from Liam I put together the sample solution outlined below. I should also mention that in this solution I am using the built-in tools in Azure Search to read data directly from SQL and Blob storage. If you wanted to use code to manually write to the index instead of letting Azure Search read from SQL then you can do that too, of course. Or read from some of the other supported data sources, or combine it all or...ImplementationYou'll need to create an Azure Search account and whatever data sources you want to use. You can do a number of the steps directly in the Azure Portal, though not all of them. For the ones you can't I will show the appropriate JSON payload you need to send. It is ridiculously easy with something like Postman. You just need to make sure you add two headers to your requests;Content-Type : application/jsonapi-key : [an admin key for your Azure Search instance]In summary, this is what we are going to build:Create the IndexYou can create a new index directly in the Azure Portal, which is what I did. It has a very nice editor that makes it quite easy. I ended up with something looking like this;Please note the &quot;content&quot; field; When Azure Search indexes files, it will place the content of those files in the content&amp;nbsp;field. Id&amp;nbsp;is there by default and we need that, so leave it alone, but the remaining fields can be whatever you like. I put a name in there for the Candidate Name and the last two fields are some fields I added so I could experiment with faceted search.Create the data sourcesNext we need to tell Azure Search where it can get the data - we need to create the Data Sources.You can't create a Data Source on it's own in the Portal; there is an option to &quot;import data&quot; but that combines setting up the data source with the index and the indexer so we can't use it for our purposes.Time to start posting JSON (see above).POST these to https://yoursearchservice.search.windows.net/datasources?api-version=2016-09-01 and replace the XXX with valid connection strings.{ &quot;name&quot; : &quot;blobcvs&quot;, &quot;type&quot; : &quot;azureblob&quot;, &quot;credentials&quot; : { &quot;connectionString&quot; : &quot;XXX&quot; }, &quot;container&quot; : { &quot;name&quot; : &quot;cvs&quot; }}{ &quot;name&quot; : &quot;candidates&quot;, &quot;type&quot; : &quot;azuresql&quot;, &quot;credentials&quot; : { &quot;connectionString&quot; : &quot;XXX&quot; }, &quot;container&quot; : { &quot;name&quot; : &quot;Candidates&quot; }} This tells Azure Search how to access your data.Create the indexersPOST these to&amp;nbsp;https://yoursearchservice.search.windows.net/indexers?api-version=2016-09-01{ &quot;name&quot; : &quot;candidateindexer&quot;, &quot;dataSourceName&quot; : &quot;candidates&quot;, &quot;targetIndexName&quot; : &quot;candidates&quot;, &quot;fieldMappings&quot; : [ { &quot;sourceFieldName&quot; : &quot;Thingiemajics&quot;, &quot;mappingFunction&quot; : { &quot;name&quot; : &quot;jsonArrayToStringCollection&quot; } } ]}This tells Azure Search to take the data in the SQL database specified in the SQL data source and create a document in Azure Search for each row. Azure Search will automatically match fields with the same names; I've got an Id field as well as Name, Type and Thingiemajics columns in SQL. The only one that is a bit special is Thingiemajics; I'm storing an array of tag values in that field in SQL in the format [&quot;red&quot;, &quot;white&quot;, &quot;blue&quot;] and putting that mapping function in there tells Azure Search to make them individual tags that can be filtered individually. See the docs&amp;nbsp;for more details.Before I create the indexer for the files, let me just take a little detour. If you remember, the original statement was that for this to work, the two different indexers need to use the same&amp;nbsp;id for data about the same candidate. The SQL indexer in my example uses the database ID of the candidate and we need to ensure that when Azure Search indexes the CV for a candidate it returns the same index. By default&amp;nbsp;Azure Search will use the filename, which is obviously no good in this situation. The way I solved this was to add a custom meta data property to the blob when I uploaded it to Azure Blob Storage, something like this:using (var fileStream = System.IO.File.OpenRead(file)){ await blob.UploadFromStreamAsync(fileStream);}blob.Metadata.Add(&quot;mykey&quot;, identifier);await blob.SetMetadataAsync();Here I have called it &quot;mykey&quot;, but it could be called anything.On to the indexer, which is created with this:{ &quot;name&quot; : &quot;cvindexer&quot;, &quot;dataSourceName&quot; : &quot;blobcvs&quot;, &quot;targetIndexName&quot; : &quot;candidates&quot;, &quot;fieldMappings&quot; : [ { &quot;sourceFieldName&quot; : &quot;mykey&quot;, &quot;targetFieldName&quot; : &quot;id&quot; } ], &quot;parameters&quot; : { &quot;configuration&quot; : { &quot;failOnUnsupportedContentType&quot; : false } }}The most important thing here is that I tell Azure Search to take the metadata property &quot;mykey&quot; and map it to the &quot;id&quot; field in my index. That's all that is required to ensure the contents of the CV ends up in the same search document as the other Candidate information.NotesIn my solution here I added an ID property to the blob meta data. I could, of course, have gone the other way instead and added the blob reference to the SQL data and then remapped that reference to &quot;id&quot; in the SQL indexer. Both approaches are equally valid, both have pros and cons, including the need to manually maintain uniqueness of the ID property and the need to base64 encode filenames.You should also note that this works because the different datasources do not have any of the same field names; if the two indexers both return the same field, one will win and delete the data from the other one.</summary></entry><entry><title type="html">How many ways can I host a web app in Azure?</title><link href="https://www.lytzen.name/2017/01/06/how-many-ways-can-i-host-web-app-in.html" rel="alternate" type="text/html" title="How many ways can I host a web app in Azure?" /><published>2017-01-06T20:17:00+00:00</published><updated>2017-01-06T20:17:00+00:00</updated><id>https://www.lytzen.name/2017/01/06/how-many-ways-can-i-host-web-app-in</id><content type="html" xml:base="https://www.lytzen.name/2017/01/06/how-many-ways-can-i-host-web-app-in.html">I got talking to a colleague about how many ways there are to host web apps in Azure and even managed to surprise myself by just how many ways I could think of. It inspired me to compile this list, which is just off the top of my head. I'm sure there are more - if you can think of other ways, please leave a comment.&lt;br /&gt;&lt;br /&gt;For the purposes of this, I am defining a web app as something that has a user-facing UI &lt;i&gt;and &lt;/i&gt;some server-side functionality. I have listed a few additional options at the bottom if you only need one of those things.&lt;br /&gt;&lt;br /&gt;I put this list together real quick like and the description of each service is just a very quick summary, mainly just from memory so please do not view this as a definitive or highly accurate document; It's mainly just a fun exercise. That said, do please point out any factual errors in the comments so I can correct them.&lt;br /&gt;&lt;br /&gt;&lt;h3&gt;&lt;a href=&quot;https://azure.microsoft.com/en-gb/services/app-service/web/&quot;&gt;Web Apps&lt;/a&gt;&lt;/h3&gt;&lt;div&gt;This is the default option and probably what you should choose if in doubt. It's getting a lot of love from Microsoft at the moment and is constantly getting new features. You get fail-over and auto-scaling by default, plans from free to expensive and it is available in both Windows and Linux flavours (in preview). It's very easy to get started with and supports pretty much anything from static pages to complex deployment processes, staging slots and even a built-in basic CD pipeline.&lt;br /&gt;The main thing to be aware of is that you don't get admin access to the server, so if you need to, say, customise which SSL protocols are available or you need to install fonts, you are out of luck. From experience, it is very rare that you need this, though.&amp;nbsp;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;h3&gt;&lt;a href=&quot;https://azure.microsoft.com/en-gb/services/cloud-services/&quot;&gt;Cloud Services&lt;/a&gt;&lt;/h3&gt;&lt;div&gt;This is the original Platform-as-a-Service option in Azure. It doesn't get much love these days, but I am still fond of it for those few situations where I need more than Web Apps can give me. Essentially, you provide Azure with an application package and Azure will take care of deploying that to one or more servers for you. You do get full admin access to the servers so you can do what you like - as long as you script it as part of your package. Patching, fail-over, load-balancing, auto-scaling, health monitoring etc is all taken care of for you.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;h3&gt;&lt;a href=&quot;https://azure.microsoft.com/en-gb/services/functions/&quot;&gt;Functions&lt;/a&gt;&lt;/h3&gt;&lt;div&gt;This isn't really meant for doing a proper web app, but you &lt;i&gt;can &lt;/i&gt;write a collection of functions that acts as an API or return some HTML etc, so you could certainly do it if you really wanted. Understand me right, though, Functions are brilliant for what they are meant to do, even if building whole web apps isn't it. That said, if you just need a couple of simple APIs to support a front-end app then it is certainly something you should consider.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;h3&gt;&lt;a href=&quot;https://azure.microsoft.com/en-gb/services/service-fabric/&quot;&gt;Service Fabric&lt;/a&gt;&lt;/h3&gt;&lt;div&gt;Not very well known, but Azure provides a pretty advanced micro-services framework that you can use for building sophisticated, large-scale applications. It supports both a service model and a basic Actor model out of the box. It's got a pretty high base cost relative to other Azure services due to the minimum number of nodes you have to use, but if you have a need for lots of scale then you should definitely look at this. Azure uses it to power a lot of their own architecture, including both SQL Azure and DocumentDb.&amp;nbsp;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;h3&gt;&lt;a href=&quot;https://azure.microsoft.com/en-gb/services/virtual-machines/&quot;&gt;Virtual Machine&lt;/a&gt;&lt;/h3&gt;&lt;div&gt;You can, of course, deploy good old-fashined virtual machines and run your web app on them. You are then responsible for patching and some level of maintenance yourself. You can define images so Azure can do auto scaling for you as well. I personally try to avoid using VMs as far as I can as I don't like to be responsible for patching and maintenance etc - yet, I still have about 40 of them :).&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;h3&gt;&lt;a href=&quot;https://azure.microsoft.com/en-gb/services/container-service/&quot;&gt;Container Services / Docker&lt;/a&gt;&lt;/h3&gt;&lt;div&gt;If you are one of those cool kids who like Docker, you are in luck. Azure has native support for Docker and support DC/OS, Docker Swarm and Kubernetes out of the box.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;So, how many was that? Six, I think, though I probably shouldn't count the Functions one :)&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;h4&gt;Related&lt;/h4&gt;&lt;div&gt;As if all the above wasn't enough, there are a couple of other technologies that can also be used to deal with web sites or APIs.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;h3&gt;&lt;a href=&quot;https://azure.microsoft.com/en-gb/services/storage/blobs/&quot;&gt;Blob Storage&lt;/a&gt; / &lt;a href=&quot;https://azure.microsoft.com/en-gb/services/cdn/&quot;&gt;CDN&lt;/a&gt;&lt;/h3&gt;&lt;div&gt;Blob storage is Azure's file storage system (okay, it's more than that, but let that suffice for now). You can share a container publicly and put html, js, css and whatever other files you like in it. I quite often use it for static assets, though in theory you could host a whole website in there. Azure CDN &lt;i&gt;can&lt;/i&gt; sit on top of blob storage and gives you geo-replication of the files. You can map custom domains to blob storage as well. Of course, this is all static so I couldn't quite include it in my list above.&amp;nbsp;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;h3&gt;&lt;a href=&quot;https://azure.microsoft.com/en-gb/services/traffic-manager/&quot;&gt;Traffic Manager&lt;/a&gt;&lt;/h3&gt;&lt;div&gt;Traffic Manager doesn't host anything, but it is worth understanding where it fits in. Pretty much all of the above options include a load balancer (automatic, you don't really have to worry about it) and fail-over within a data centre. If you need fail-over between data centres or want to route traffic to servers close to your users, you can use Traffic Manager. It works at the DNS level and is used to direct user requests for a given domain name to the nearest available data centre.&amp;nbsp;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;h3&gt;&lt;a href=&quot;https://azure.microsoft.com/en-gb/services/api-management/&quot;&gt;API Management&lt;/a&gt;&lt;/h3&gt;&lt;div&gt;Not strongly related to web app hosting, but I just thought I'd mention it; Basically, if you develop an API and you want to give external users access to it you probably want to do things like controlling who can use it (maybe so you can bill them), rate limiting, authentication and so on. Azure API management deals with all that as a service that just sits in front of your naked API so you don't have to write it all yourself.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;h4&gt;Takeaway&lt;/h4&gt;&lt;div&gt;When I started this list, I really didn't expect it would be this long. I think it's great there is so much choice and I know that each option has it's own set of strengths and weaknesses. We use most of the technologies listed here on different projects and for different reasons, and I'm very happy that I can choose. At the same time, I think it is probably quite tough for someone new to Azure to even get started on figuring out which of the many options are right for their particular scenario. And I have only dealt with web app hosting here, not the multitude of other things you can do.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;I just meant for this post to be a fun little exercise but, having written it, I should mention that at &lt;a href=&quot;http://www.neworbit.co.uk/&quot;&gt;NewOrbit&lt;/a&gt; we have recently started helping other companies move to the cloud and sharing our years of Azure experience with them.&amp;nbsp;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;If you have any questions or want to talk more, &lt;a href=&quot;https://twitter.com/flytzen&quot;&gt;ping me on twitter&lt;/a&gt;&amp;nbsp;or add a comment below.&lt;/div&gt;</content><author><name>Frans Lytzen</name></author><category term="Azure" /><summary type="html">I got talking to a colleague about how many ways there are to host web apps in Azure and even managed to surprise myself by just how many ways I could think of. It inspired me to compile this list, which is just off the top of my head. I'm sure there are more - if you can think of other ways, please leave a comment.For the purposes of this, I am defining a web app as something that has a user-facing UI and some server-side functionality. I have listed a few additional options at the bottom if you only need one of those things.I put this list together real quick like and the description of each service is just a very quick summary, mainly just from memory so please do not view this as a definitive or highly accurate document; It's mainly just a fun exercise. That said, do please point out any factual errors in the comments so I can correct them.Web AppsThis is the default option and probably what you should choose if in doubt. It's getting a lot of love from Microsoft at the moment and is constantly getting new features. You get fail-over and auto-scaling by default, plans from free to expensive and it is available in both Windows and Linux flavours (in preview). It's very easy to get started with and supports pretty much anything from static pages to complex deployment processes, staging slots and even a built-in basic CD pipeline.The main thing to be aware of is that you don't get admin access to the server, so if you need to, say, customise which SSL protocols are available or you need to install fonts, you are out of luck. From experience, it is very rare that you need this, though.&amp;nbsp;Cloud ServicesThis is the original Platform-as-a-Service option in Azure. It doesn't get much love these days, but I am still fond of it for those few situations where I need more than Web Apps can give me. Essentially, you provide Azure with an application package and Azure will take care of deploying that to one or more servers for you. You do get full admin access to the servers so you can do what you like - as long as you script it as part of your package. Patching, fail-over, load-balancing, auto-scaling, health monitoring etc is all taken care of for you.FunctionsThis isn't really meant for doing a proper web app, but you can write a collection of functions that acts as an API or return some HTML etc, so you could certainly do it if you really wanted. Understand me right, though, Functions are brilliant for what they are meant to do, even if building whole web apps isn't it. That said, if you just need a couple of simple APIs to support a front-end app then it is certainly something you should consider.Service FabricNot very well known, but Azure provides a pretty advanced micro-services framework that you can use for building sophisticated, large-scale applications. It supports both a service model and a basic Actor model out of the box. It's got a pretty high base cost relative to other Azure services due to the minimum number of nodes you have to use, but if you have a need for lots of scale then you should definitely look at this. Azure uses it to power a lot of their own architecture, including both SQL Azure and DocumentDb.&amp;nbsp;Virtual MachineYou can, of course, deploy good old-fashined virtual machines and run your web app on them. You are then responsible for patching and some level of maintenance yourself. You can define images so Azure can do auto scaling for you as well. I personally try to avoid using VMs as far as I can as I don't like to be responsible for patching and maintenance etc - yet, I still have about 40 of them :).Container Services / DockerIf you are one of those cool kids who like Docker, you are in luck. Azure has native support for Docker and support DC/OS, Docker Swarm and Kubernetes out of the box.So, how many was that? Six, I think, though I probably shouldn't count the Functions one :)RelatedAs if all the above wasn't enough, there are a couple of other technologies that can also be used to deal with web sites or APIs.Blob Storage / CDNBlob storage is Azure's file storage system (okay, it's more than that, but let that suffice for now). You can share a container publicly and put html, js, css and whatever other files you like in it. I quite often use it for static assets, though in theory you could host a whole website in there. Azure CDN can sit on top of blob storage and gives you geo-replication of the files. You can map custom domains to blob storage as well. Of course, this is all static so I couldn't quite include it in my list above.&amp;nbsp;Traffic ManagerTraffic Manager doesn't host anything, but it is worth understanding where it fits in. Pretty much all of the above options include a load balancer (automatic, you don't really have to worry about it) and fail-over within a data centre. If you need fail-over between data centres or want to route traffic to servers close to your users, you can use Traffic Manager. It works at the DNS level and is used to direct user requests for a given domain name to the nearest available data centre.&amp;nbsp;API ManagementNot strongly related to web app hosting, but I just thought I'd mention it; Basically, if you develop an API and you want to give external users access to it you probably want to do things like controlling who can use it (maybe so you can bill them), rate limiting, authentication and so on. Azure API management deals with all that as a service that just sits in front of your naked API so you don't have to write it all yourself.TakeawayWhen I started this list, I really didn't expect it would be this long. I think it's great there is so much choice and I know that each option has it's own set of strengths and weaknesses. We use most of the technologies listed here on different projects and for different reasons, and I'm very happy that I can choose. At the same time, I think it is probably quite tough for someone new to Azure to even get started on figuring out which of the many options are right for their particular scenario. And I have only dealt with web app hosting here, not the multitude of other things you can do.I just meant for this post to be a fun little exercise but, having written it, I should mention that at NewOrbit we have recently started helping other companies move to the cloud and sharing our years of Azure experience with them.&amp;nbsp;If you have any questions or want to talk more, ping me on twitter&amp;nbsp;or add a comment below.</summary></entry><entry><title type="html">Find docs with no PartitionKey in Azure DocumentDb</title><link href="https://www.lytzen.name/2016/12/06/find-docs-with-no-partitionkey-in-azure.html" rel="alternate" type="text/html" title="Find docs with no PartitionKey in Azure DocumentDb" /><published>2016-12-06T21:31:00+00:00</published><updated>2016-12-06T21:31:00+00:00</updated><id>https://www.lytzen.name/2016/12/06/find-docs-with-no-partitionkey-in-azure</id><content type="html" xml:base="https://www.lytzen.name/2016/12/06/find-docs-with-no-partitionkey-in-azure.html">When you are using Partitioned Collections in Azure DocumentDb you need to specify a &lt;i&gt;Partition Key&lt;/i&gt; on each Document. At least, I thought you did. But, it turns out that you actually &lt;i&gt;can&lt;/i&gt;&amp;nbsp;save documents without a partitionkey. But if you do, you'll have a hard time retrieving or deleting them - until you meet Undefined.Value.&lt;br /&gt;&lt;i&gt;Note: This post is written for C#, I am not sure about the equivalent for other languages.&lt;/i&gt;&lt;br /&gt;&lt;i&gt;&lt;br /&gt;&lt;/i&gt;&lt;br /&gt;&lt;h4&gt;Details&lt;/h4&gt;&lt;div&gt;If you create a Partitioned Collection in Azure DocumentDb you probably think that every document you save must have a partitionkey property and probably also that it must have a value. In this post I am dealing with the situation where you don't have a partition key property on your document at all, not the situation where you have one but you set it to null or an empty string.&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;For example, if you have created your collection with code similar to this;&lt;br /&gt;&lt;pre class=&quot;prettyprint&quot;&gt;var docCollection = new DocumentCollection()&lt;br /&gt;{&lt;br /&gt;   Id = this.collectionName&lt;br /&gt;};&lt;br /&gt;docCollection.PartitionKey.Paths.Add(&quot;/partitionKey&quot;);&lt;br /&gt;await docClient.CreateDocumentCollectionAsync(&lt;br /&gt;                    UriFactory.CreateDatabaseUri(this.dbName), &lt;br /&gt;                    docCollection);&lt;/pre&gt;and you then try to save an instance of a class that looks like this:&lt;br /&gt;&lt;pre class=&quot;prettyprint&quot;&gt;public class MyItem&lt;br /&gt;{&lt;br /&gt;    [JsonProperty(&quot;id&quot;)]&lt;br /&gt;    public string Id { get; set; }&lt;br /&gt;&lt;br /&gt;    public string SomeValue { get; set; }&lt;br /&gt;}&lt;br /&gt;&lt;/pre&gt;then you may expect to get an error. But, in fact, it will save just fine as I found out to my detriment after a major refactoring.&lt;/div&gt;&lt;br /&gt;&lt;div&gt;Now that you have that item in the database you will find it hard to retrieve it and even harder to delete it - until you meet your new friend &lt;b&gt;Undefined.Value&lt;/b&gt;.&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;h3&gt;How to read the document:&lt;/h3&gt;&lt;pre class=&quot;prettyprint&quot;&gt;MyItem item = (dynamic)client.ReadDocumentAsync(&lt;br /&gt;                           UriFactory.CreateDocumentUri(DbName, CollectionName, id),&lt;br /&gt;                           new RequestOptions() {&lt;br /&gt;                             PartitionKey = new PartitionKey(Undefined.Value)&lt;br /&gt;                           })&lt;br /&gt;                         .Result.Resource;&lt;br /&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div&gt;&lt;h3&gt;How to delete the document:&lt;/h3&gt;&lt;pre class=&quot;prettyprint&quot;&gt;client.DeleteDocumentAsync(&lt;br /&gt;          UriFactory.CreateDocumentUri(DbName, CollectionName, id), &lt;br /&gt;          new RequestOptions() { PartitionKey = new PartitionKey(Undefined.Value) });&lt;br /&gt;&lt;/pre&gt;&lt;/div&gt;Many thanks to &lt;a href=&quot;https://twitter.com/arkramac&quot;&gt;Aravind Ramachandran&lt;/a&gt; for telling me about Undefined.Value.&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;</content><author><name>Frans Lytzen</name></author><category term="CosmosDB" /><category term="Azure" /><summary type="html">When you are using Partitioned Collections in Azure DocumentDb you need to specify a Partition Key on each Document. At least, I thought you did. But, it turns out that you actually can&amp;nbsp;save documents without a partitionkey. But if you do, you'll have a hard time retrieving or deleting them - until you meet Undefined.Value.Note: This post is written for C#, I am not sure about the equivalent for other languages.DetailsIf you create a Partitioned Collection in Azure DocumentDb you probably think that every document you save must have a partitionkey property and probably also that it must have a value. In this post I am dealing with the situation where you don't have a partition key property on your document at all, not the situation where you have one but you set it to null or an empty string.For example, if you have created your collection with code similar to this;var docCollection = new DocumentCollection(){ Id = this.collectionName};docCollection.PartitionKey.Paths.Add(&quot;/partitionKey&quot;);await docClient.CreateDocumentCollectionAsync( UriFactory.CreateDatabaseUri(this.dbName), docCollection);and you then try to save an instance of a class that looks like this:public class MyItem{ [JsonProperty(&quot;id&quot;)] public string Id { get; set; } public string SomeValue { get; set; }}then you may expect to get an error. But, in fact, it will save just fine as I found out to my detriment after a major refactoring.Now that you have that item in the database you will find it hard to retrieve it and even harder to delete it - until you meet your new friend Undefined.Value.How to read the document:MyItem item = (dynamic)client.ReadDocumentAsync( UriFactory.CreateDocumentUri(DbName, CollectionName, id), new RequestOptions() { PartitionKey = new PartitionKey(Undefined.Value) }) .Result.Resource;How to delete the document:client.DeleteDocumentAsync( UriFactory.CreateDocumentUri(DbName, CollectionName, id), new RequestOptions() { PartitionKey = new PartitionKey(Undefined.Value) });Many thanks to Aravind Ramachandran for telling me about Undefined.Value.</summary></entry><entry><title type="html">Find Documents with missing properties in Azure DocumentDb with the .Net SDK</title><link href="https://www.lytzen.name/2016/09/29/find-documents-with-missing-properties.html" rel="alternate" type="text/html" title="Find Documents with missing properties in Azure DocumentDb with the .Net SDK" /><published>2016-09-29T21:06:00+01:00</published><updated>2016-09-29T21:06:00+01:00</updated><id>https://www.lytzen.name/2016/09/29/find-documents-with-missing-properties</id><content type="html" xml:base="https://www.lytzen.name/2016/09/29/find-documents-with-missing-properties.html">Azure DocumentDb stores documents as JSON. One of the effects of this is that sometimes you may end up with documents in the database that have missing properties and it can be quite tricky to search for them with the .Net SDK. This blog post has an approach to doing it - and quite simply too.&lt;br /&gt;&lt;br /&gt;&lt;h3&gt;Background&lt;/h3&gt;&lt;div&gt;Most commonly, you would encounter the issue of the missing property when you add a new property to an existing class in your .Net code. There is no automatic method of adding this new property to all the existing entries in the database, short of re-saving them all.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;Alternatively, you can explicitly configure Json.Net to not store properties that have null values like this:&lt;/div&gt;&lt;pre class=&quot;prettyprint&quot;&gt;JsonConvert.DefaultSettings = () =&amp;gt; &lt;br /&gt;  new JsonSerializerSettings&lt;br /&gt;       {&lt;br /&gt;          NullValueHandling = NullValueHandling.Ignore&lt;br /&gt;       };&lt;br /&gt;&lt;/pre&gt;&lt;br /&gt;&lt;div&gt;You can use this configuration option to test the behaviour I am describing here or to save space in the database.&lt;br /&gt;&lt;br /&gt;For example, imagine you have a class called MyItem looking like this:&lt;/div&gt;&lt;pre class=&quot;prettyprint&quot;&gt;public class MyItem&lt;br /&gt;{&lt;br /&gt;    [JsonProperty(&quot;id&quot;)]&lt;br /&gt;    public string Id { get; set; }&lt;br /&gt;    public string SomeValue { get; set; }&lt;br /&gt;}&lt;br /&gt;&lt;/pre&gt;&lt;br /&gt;&lt;div&gt;If you have an item where SomeValue is null, by default that will be serialised and stored in DocumentDb like this:&lt;/div&gt;&lt;pre class=&quot;prettyprint&quot;&gt;{&lt;br /&gt;  &quot;id&quot; : &quot;1&quot;,&lt;br /&gt;  &quot;SomeValue&quot; : null&lt;br /&gt;}&lt;br /&gt;&lt;/pre&gt;&lt;br /&gt;&lt;div&gt;However, if you configure Json.Net to not store null values (or the SomeValue field was added to your .Net code after you stored this item in DocumentDb) it will look like this in the database:&lt;/div&gt;&lt;pre class=&quot;prettyprint&quot;&gt;{&lt;br /&gt;  &quot;id&quot; : &quot;1&quot;,&lt;br /&gt;}&lt;br /&gt;&lt;/pre&gt;&lt;br /&gt;&lt;br /&gt;&lt;h3&gt;Selecting missing properties with SQL&lt;/h3&gt;&lt;div&gt;According to&amp;nbsp;&lt;a href=&quot;https://azure.microsoft.com/en-gb/documentation/articles/documentdb-sql-query/&quot;&gt;the documentation&lt;/a&gt;&amp;nbsp;you can use SQL to select missing properties like this:&lt;/div&gt;&lt;pre class=&quot;prettyprint&quot;&gt;SELECT f.lastName ?? f.surname AS familyName&lt;br /&gt;FROM Families f&lt;br /&gt;&lt;/pre&gt;&lt;div&gt;You can then extrapolate from that example to, for example, select items etc.&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;h3&gt;Finding items with null &lt;i&gt;or&lt;/i&gt;&amp;nbsp;missing with the .Net SDK&lt;/h3&gt;&lt;/div&gt;&lt;div&gt;Imagine you have added the SomeValue property to the MyItem class after you had already saved some items. Further, sometimes you store a null in the SomeValue property. Or you have configured Json.Net to ignore null values. And now you want to find all the items where SomeValue is either missing or null.&lt;/div&gt;&lt;div&gt;You might try this:&amp;nbsp;&lt;/div&gt;&lt;pre class=&quot;prettyprint&quot;&gt;var query1 = client.CreateDocumentQuery&amp;lt;MyItem&amp;gt;(collectionUrl)&lt;br /&gt;              .Where(i =&amp;gt; i.SomeValue == null);&lt;br /&gt;&lt;/pre&gt;&lt;div&gt;But you will find that this will not actually return any results - at least, it won't return any documents where SomeValue is not present at all. However, this odd-looking statement will work:&lt;/div&gt;&lt;pre class=&quot;prettyprint&quot;&gt;var query2 = client.CreateDocumentQuery&amp;lt;MyItem&amp;gt;(collectionUrl)&lt;br /&gt;              .Where(i =&amp;gt; (i.SomeValue ?? null) == null);&lt;br /&gt;&lt;/pre&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;It is using the null coalescor to make DocumentDb return a null value for the property for the property if it does not exist, which we can then compare to null.&lt;br /&gt;&lt;br /&gt;I have tested this with version 1.6, 1.8 and 1.10 of the SDK, but I would advise you to put an integration test in your code if you are going to rely on it, just in case the behaviour changes in the future. You'll probably also want to put a comment wherever you use this syntax as R# is quite keen to tell you that you should get rid of the &quot;?? null&quot; part.&lt;br /&gt;Finally, I have not done any performance testing on this, but I suspect DocumentDb won't be able to use any indexes to execute this query; it will probably have to evaluate each document in a scan so use with caution.&lt;/div&gt;&lt;br /&gt;&lt;h3&gt;A full sample&lt;/h3&gt;&lt;div&gt;If you want to try this out, there is a full example here:&amp;nbsp;&lt;a href=&quot;https://gist.github.com/flytzen/b8dbcb2079a66b1c8a67005ef5198052&quot;&gt;https://gist.github.com/flytzen/b8dbcb2079a66b1c8a67005ef5198052&lt;/a&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;</content><author><name>Frans Lytzen</name></author><category term="CosmosDB" /><category term="Azure" /><summary type="html">Azure DocumentDb stores documents as JSON. One of the effects of this is that sometimes you may end up with documents in the database that have missing properties and it can be quite tricky to search for them with the .Net SDK. This blog post has an approach to doing it - and quite simply too.BackgroundMost commonly, you would encounter the issue of the missing property when you add a new property to an existing class in your .Net code. There is no automatic method of adding this new property to all the existing entries in the database, short of re-saving them all.Alternatively, you can explicitly configure Json.Net to not store properties that have null values like this:JsonConvert.DefaultSettings = () =&amp;gt; new JsonSerializerSettings { NullValueHandling = NullValueHandling.Ignore };You can use this configuration option to test the behaviour I am describing here or to save space in the database.For example, imagine you have a class called MyItem looking like this:public class MyItem{ [JsonProperty(&quot;id&quot;)] public string Id { get; set; } public string SomeValue { get; set; }}If you have an item where SomeValue is null, by default that will be serialised and stored in DocumentDb like this:{ &quot;id&quot; : &quot;1&quot;, &quot;SomeValue&quot; : null}However, if you configure Json.Net to not store null values (or the SomeValue field was added to your .Net code after you stored this item in DocumentDb) it will look like this in the database:{ &quot;id&quot; : &quot;1&quot;,}Selecting missing properties with SQLAccording to&amp;nbsp;the documentation&amp;nbsp;you can use SQL to select missing properties like this:SELECT f.lastName ?? f.surname AS familyNameFROM Families fYou can then extrapolate from that example to, for example, select items etc.Finding items with null or&amp;nbsp;missing with the .Net SDKImagine you have added the SomeValue property to the MyItem class after you had already saved some items. Further, sometimes you store a null in the SomeValue property. Or you have configured Json.Net to ignore null values. And now you want to find all the items where SomeValue is either missing or null.You might try this:&amp;nbsp;var query1 = client.CreateDocumentQuery&amp;lt;MyItem&amp;gt;(collectionUrl) .Where(i =&amp;gt; i.SomeValue == null);But you will find that this will not actually return any results - at least, it won't return any documents where SomeValue is not present at all. However, this odd-looking statement will work:var query2 = client.CreateDocumentQuery&amp;lt;MyItem&amp;gt;(collectionUrl) .Where(i =&amp;gt; (i.SomeValue ?? null) == null);It is using the null coalescor to make DocumentDb return a null value for the property for the property if it does not exist, which we can then compare to null.I have tested this with version 1.6, 1.8 and 1.10 of the SDK, but I would advise you to put an integration test in your code if you are going to rely on it, just in case the behaviour changes in the future. You'll probably also want to put a comment wherever you use this syntax as R# is quite keen to tell you that you should get rid of the &quot;?? null&quot; part.Finally, I have not done any performance testing on this, but I suspect DocumentDb won't be able to use any indexes to execute this query; it will probably have to evaluate each document in a scan so use with caution.A full sampleIf you want to try this out, there is a full example here:&amp;nbsp;https://gist.github.com/flytzen/b8dbcb2079a66b1c8a67005ef5198052</summary></entry><entry><title type="html">Auto publish Azure Web Jobs with ASP.Net Core RTM</title><link href="https://www.lytzen.name/2016/08/26/auto-publish-azure-web-jobs-with-aspnet.html" rel="alternate" type="text/html" title="Auto publish Azure Web Jobs with ASP.Net Core RTM" /><published>2016-08-26T10:36:00+01:00</published><updated>2016-08-26T10:36:00+01:00</updated><id>https://www.lytzen.name/2016/08/26/auto-publish-azure-web-jobs-with-aspnet</id><content type="html" xml:base="https://www.lytzen.name/2016/08/26/auto-publish-azure-web-jobs-with-aspnet.html">&lt;i&gt;This is an update of my &lt;a href=&quot;http://blog.lytzen.name/2016/03/auto-deploy-azure-web-job-with-aspnet-5.html&quot;&gt;original post&lt;/a&gt; on how to do this with ASP.Net Core RC1&lt;/i&gt;&lt;br /&gt;At the time of this writing, there is no tooling to auto publish Azure Web Jobs with an ASP.Net Core website. You can do it with old-style websites, but not yet the new ones. The tooling is highly likely to appear eventually, but as of right now you have to take a few steps to set it up yourself, which is what I describe in this post. For clarity, what I am describing works only with source control deploy (git etc),&amp;nbsp;&lt;i&gt;not&lt;/i&gt;&amp;nbsp;with publishing from Visual Studio.&lt;br /&gt;&lt;br /&gt;&lt;b&gt;This information will eventually become obsolete, probably around the RTM of the ASP.Net Core Visual Studio tooling. If that has happened and I have not updated the post, please post a comment and I'll get on it.&lt;/b&gt;&lt;br /&gt;Note that for very simple, self-contained web jobs, there may be a simpler approach (see&amp;nbsp;&lt;a href=&quot;http://stackoverflow.com/a/33291097/11534&quot;&gt;http://stackoverflow.com/a/33291097/11534&lt;/a&gt;&amp;nbsp;for some thoughts). In this post I am catering for the scenario where your webjob references another project in your solution - though it'll work just as well if it doesn't.&lt;br /&gt;&lt;h3&gt;In summary&lt;/h3&gt;&lt;div&gt;You have to use a custom deployment script and insert an action to&amp;nbsp;&lt;i&gt;publish&lt;/i&gt;&amp;nbsp;your webjob to the path that Azure looks for webjobs in; App_Data/Jobs/Continuous for continuous jobs. Azure will automatically detect content in that folder and assume it's a webjob, so all we really have to do is make sure our webjob is copied there. And yes, it will happily overwrite a running webjob, the Kudu functionality handles that somehow.&lt;br /&gt;&lt;br /&gt;The reason we are&amp;nbsp;&lt;i&gt;publishing,&amp;nbsp;&lt;/i&gt;using &lt;i&gt;dotnet publish&lt;/i&gt;, is to ensure we get all the dependencies. It won't re-compile so it's just a copy operation.&lt;br /&gt;If we had a very simple webjob that was self contained, you could just copy&amp;nbsp;the files, as long as you added a&amp;nbsp;&lt;i&gt;run.cmd&lt;/i&gt;&amp;nbsp;(see link above).&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;&lt;h3&gt;In detail&lt;/h3&gt;&lt;div&gt;Prepare a site&lt;br /&gt;&lt;ol&gt;&lt;li&gt;Set up a solution with an ASP.Net 5 Web site and a webjob written as a ASP.Net 5 / Core console app.&lt;/li&gt;&lt;li&gt;Set up source control deploy to an Azure website. It shouldn't matter which type.&lt;/li&gt;&lt;li&gt;Wait for the initial deploy of the site.&amp;nbsp;&lt;/li&gt;&lt;/ol&gt;&lt;h4&gt;Set up a custom deployment script&lt;/h4&gt;&lt;/div&gt;&lt;div&gt;In this example, we will download the deployment script that Azure has created. There are ways to also do this with the Azure CLI, but the generated script is not quite the same at this point in time - which may or may not matter.&lt;/div&gt;&lt;div&gt;&lt;ol&gt;&lt;li&gt;Get the auto generated script&lt;/li&gt;&lt;ol&gt;&lt;li&gt;Log in to the web app console at https://[yoursite].scm.azurewebsites.net&lt;/li&gt;&lt;li&gt;Go to Tools - Download deployment script&lt;/li&gt;&lt;li&gt;Unzip the downloaded zip file to the root of your solution folder&lt;/li&gt;&lt;/ol&gt;&lt;li&gt;Modify your deploy.cmd file in the following ways&lt;br /&gt;&lt;/li&gt;&lt;ol&gt;&lt;li&gt;Add a line like this near the top - just to make it easier to check that your custom script is being used (whatever you put after echo will be output in the log file)&lt;br /&gt;&lt;pre class=&quot;prettyprint&quot;&gt;echo CUSTOM SCRIPT Start&lt;br /&gt;&lt;/pre&gt;&lt;/li&gt;&lt;li&gt;Near the middle of the file you will find a series of steps that are numbered. One of the steps will look like this;&lt;br /&gt;&lt;strong&gt;NOTE: The indented lines will be on a single line, the line breaks are only added here for readability&lt;/strong&gt;&lt;br /&gt;&lt;pre class=&quot;prettyprint&quot;&gt;:: 2. Build and publish&lt;br /&gt;call :ExecuteCmd &quot;%MSBUILD_PATH%&quot; &quot;%DEPLOYMENT_SOURCE%\MySolution.sln&quot; &lt;br /&gt;       /nologo &lt;br /&gt;       /verbosity:m &lt;br /&gt;       /p:deployOnBuild=True;&lt;br /&gt;          AutoParameterizationWebConfigConnectionStrings=false;&lt;br /&gt;          Configuration=Release;&lt;br /&gt;          UseSharedCompilation=false;&lt;br /&gt;          publishUrl=&quot;%DEPLOYMENT_TEMP%&quot; &lt;br /&gt;       %SCM_BUILD_ARGS%&lt;br /&gt;&lt;br /&gt;&lt;/pre&gt;&lt;/li&gt;&lt;li&gt;Below that insert these lines (updating the path with the actual path to your webjob in the solution)&lt;br /&gt;&lt;strong&gt;NOTE: The indented lines need to put on a single line, the line breaks are only here for readability&lt;/strong&gt;&lt;br /&gt;&lt;pre class=&quot;prettyprint&quot;&gt;:: 2.1 Publish webjobs&lt;br /&gt;echo STARTING TO PUBLISH WEBJOBS&lt;br /&gt;echo DEPLOYMENT_TEMP is %DEPLOYMENT_TEMP%&lt;br /&gt;call :ExecuteCmd dotnet publish &lt;br /&gt;       &quot;%DEPLOYMENT_SOURCE%\src\MyWebJob\project.json&quot; &lt;br /&gt;         -o &quot;%DEPLOYMENT_TEMP%\App_Data\Jobs\Continuous\MyWebJob&quot; &lt;br /&gt;       -c Release&lt;br /&gt;&lt;/pre&gt;If you have more than one web job, just add individual publish lines for each one&lt;/li&gt;&lt;/ol&gt;&lt;/ol&gt;&lt;/div&gt;&lt;br /&gt;&lt;h4&gt;Check that it worked&lt;/h4&gt;&lt;div&gt;Push your changes and wait for Azure to deploy, then look at the webjobs in the Azure portal. You should see your job there. In case you don't, have a look at the publish log file to see if there are any errors in there.&lt;/div&gt;</content><author><name>Frans Lytzen</name></author><category term="Azure" /><summary type="html">This is an update of my original post on how to do this with ASP.Net Core RC1At the time of this writing, there is no tooling to auto publish Azure Web Jobs with an ASP.Net Core website. You can do it with old-style websites, but not yet the new ones. The tooling is highly likely to appear eventually, but as of right now you have to take a few steps to set it up yourself, which is what I describe in this post. For clarity, what I am describing works only with source control deploy (git etc),&amp;nbsp;not&amp;nbsp;with publishing from Visual Studio.This information will eventually become obsolete, probably around the RTM of the ASP.Net Core Visual Studio tooling. If that has happened and I have not updated the post, please post a comment and I'll get on it.Note that for very simple, self-contained web jobs, there may be a simpler approach (see&amp;nbsp;http://stackoverflow.com/a/33291097/11534&amp;nbsp;for some thoughts). In this post I am catering for the scenario where your webjob references another project in your solution - though it'll work just as well if it doesn't.In summaryYou have to use a custom deployment script and insert an action to&amp;nbsp;publish&amp;nbsp;your webjob to the path that Azure looks for webjobs in; App_Data/Jobs/Continuous for continuous jobs. Azure will automatically detect content in that folder and assume it's a webjob, so all we really have to do is make sure our webjob is copied there. And yes, it will happily overwrite a running webjob, the Kudu functionality handles that somehow.The reason we are&amp;nbsp;publishing,&amp;nbsp;using dotnet publish, is to ensure we get all the dependencies. It won't re-compile so it's just a copy operation.If we had a very simple webjob that was self contained, you could just copy&amp;nbsp;the files, as long as you added a&amp;nbsp;run.cmd&amp;nbsp;(see link above).In detailPrepare a siteSet up a solution with an ASP.Net 5 Web site and a webjob written as a ASP.Net 5 / Core console app.Set up source control deploy to an Azure website. It shouldn't matter which type.Wait for the initial deploy of the site.&amp;nbsp;Set up a custom deployment scriptIn this example, we will download the deployment script that Azure has created. There are ways to also do this with the Azure CLI, but the generated script is not quite the same at this point in time - which may or may not matter.Get the auto generated scriptLog in to the web app console at https://[yoursite].scm.azurewebsites.netGo to Tools - Download deployment scriptUnzip the downloaded zip file to the root of your solution folderModify your deploy.cmd file in the following waysAdd a line like this near the top - just to make it easier to check that your custom script is being used (whatever you put after echo will be output in the log file)echo CUSTOM SCRIPT StartNear the middle of the file you will find a series of steps that are numbered. One of the steps will look like this;NOTE: The indented lines will be on a single line, the line breaks are only added here for readability:: 2. Build and publishcall :ExecuteCmd &quot;%MSBUILD_PATH%&quot; &quot;%DEPLOYMENT_SOURCE%\MySolution.sln&quot; /nologo /verbosity:m /p:deployOnBuild=True; AutoParameterizationWebConfigConnectionStrings=false; Configuration=Release; UseSharedCompilation=false; publishUrl=&quot;%DEPLOYMENT_TEMP%&quot; %SCM_BUILD_ARGS%Below that insert these lines (updating the path with the actual path to your webjob in the solution)NOTE: The indented lines need to put on a single line, the line breaks are only here for readability:: 2.1 Publish webjobsecho STARTING TO PUBLISH WEBJOBSecho DEPLOYMENT_TEMP is %DEPLOYMENT_TEMP%call :ExecuteCmd dotnet publish &quot;%DEPLOYMENT_SOURCE%\src\MyWebJob\project.json&quot; -o &quot;%DEPLOYMENT_TEMP%\App_Data\Jobs\Continuous\MyWebJob&quot; -c ReleaseIf you have more than one web job, just add individual publish lines for each oneCheck that it workedPush your changes and wait for Azure to deploy, then look at the webjobs in the Azure portal. You should see your job there. In case you don't, have a look at the publish log file to see if there are any errors in there.</summary></entry><entry><title type="html">InfoSec with SQL Azure</title><link href="https://www.lytzen.name/2016/08/25/infosec-with-sql-azure.html" rel="alternate" type="text/html" title="InfoSec with SQL Azure" /><published>2016-08-25T15:17:00+01:00</published><updated>2016-08-25T15:17:00+01:00</updated><id>https://www.lytzen.name/2016/08/25/infosec-with-sql-azure</id><content type="html" xml:base="https://www.lytzen.name/2016/08/25/infosec-with-sql-azure.html">I've been using SQL Azure since 2011 and it's been a journey. One of the big problems I used to have was passing security audits from some of our clients; We deal with a &amp;nbsp;lot of data that is highly sensitive so are under a lot of scrutiny to make sure it is protected.&lt;br /&gt;&lt;br /&gt;A lot of what you have to comply with when you start going into the areas of ISO27001 and PCI compliance is not just about technical security in the way we normally think about it, it’s just as much about people and processes. In my experience from filling in dozens of security questionnaires, I believe that SQL Azure can tick all the boxes, as long as you switch on the right features. Just to be clear, I'm not required to comply with PCI so can't vouch for that, but I do have to comply with a number of ISO27001 and Data Protection requirements.&lt;br /&gt;&lt;br /&gt;&lt;h2&gt;Box ticking&lt;/h2&gt;Most security audits will ask you if your data is &quot;encrypted at rest&quot;. If you understand how Azure really works and where &amp;nbsp;that requirement originally comes from, you'll know that this is a meaningless requirement when you're in Azure. But, just try to convince a security auditor of that. So go ahead and switch on &lt;a href=&quot;https://msdn.microsoft.com/en-gb/library/dn948096.aspx&quot;&gt;Transparent Data Encryption&lt;/a&gt;&amp;nbsp; and you can tick that box on the questionnaire.&lt;br /&gt;Incidentally, Azure also recently enabled you to do something similar for &lt;a href=&quot;https://azure.microsoft.com/en-gb/documentation/articles/storage-service-encryption/&quot;&gt;Blob storage&lt;/a&gt; - again mainly so you can tick the box.&lt;br /&gt;&lt;br /&gt;&lt;h2&gt;Intrusion detection&lt;/h2&gt;SQL Azure has a neat Threat Detection facility to monitor the use of your database and alert you to anomalous behaviour. I've seen it detect potential SQL injection attacks and allegedly it will detect &quot;unusual&quot; behaviour, though I have yet to actually see that (thankfully :)).&lt;br /&gt;This is &amp;nbsp;a useful feature to potentially detect both external and internal attacks (see below).&lt;br /&gt;You have nothing to lose by &lt;a href=&quot;https://azure.microsoft.com/en-gb/documentation/articles/sql-database-threat-detection-get-started/&quot;&gt;switching it on&lt;/a&gt;.&lt;br /&gt;&lt;br /&gt;&lt;h2&gt;Internal attacks&lt;/h2&gt;As a developer it's easy to think that once you pass penetration testing, your job is done. But, for the organisation, that's only half the battle. A proper security audit will look just as much at how you are preventing and detecting what they sometimes call &quot;Internal Data Leakage&quot;. Basically, they are worried that people inside your organisation may access the data and leak it. This may be deliberately, it may be a result of social engineering or it may be that an external agency decided to hack a person who works for you. It's the modern equivalent of sneaking in through the kitchen entrance instead of taking a battering ram to the front gates. It's hardly surprising there is a lot of focus on this, given that most of the big attacks recently seems to have come from some kind of inside job.&lt;br /&gt;You may well trust everyone in your organisation to not want to deliberately betray you and to be sensible about not having their laptops hacked - but an auditor won't, so you may as well buckle up and do the right thing. SQL Azure makes it surprisingly easy, though there are number of manual steps that could be made a lot easier with some better tooling, especially if you have many databases.&lt;br /&gt;&lt;br /&gt;&lt;h3&gt;Step 1 - Give users individual logins&lt;/h3&gt;Your application probably uses a master username and password to log in to SQL. It's tempting to just give that to the people who need to go and look stuff up in the database, including developers, devops, support etc. But this means you have no way of knowing which actual person did what. Worse, when someone leaves the organisation you may not find it that easy to change the application's SQL Password.&lt;br /&gt;&lt;br /&gt;You can create individual users in SQL Azure to give to the people who need access and I urge you to do so. You may want to give users read-only access while you are it. The tooling is lacking, in that you have to create the users using T-SQL from something like SQL Server Management Studio or similar; you can’t manage the users in the Azure Portal, which is quite an oversight IMO.&lt;br /&gt;&lt;br /&gt;If you use Azure AD for authentication you can &lt;a href=&quot;https://azure.microsoft.com/en-gb/documentation/articles/sql-database-aad-authentication/&quot;&gt;add Azure AD&lt;/a&gt; users to the SQL Databases. This has the benefit that user access will be automatically revoked from all databases when the user's AD Account is disabled - probably when they leave the organisation. It also means that, as of August 2016, you can use 2 Factor Authentication with the SQL logins - just download the latest version of SQL Server Management Studio 2016 and choose the Universal authentication method.&lt;br /&gt;At &lt;a href=&quot;https://www.neworbit.co.uk/&quot;&gt;NewOrbit&lt;/a&gt;, all user access to Azure and SQL &amp;nbsp;Azure is controlled through Azure AD, through our Office 365 subscription and all users are required to have 2FA enabled.&lt;br /&gt;&lt;br /&gt;&lt;h3&gt;Step 2 - hide the sign-in details for the application&lt;/h3&gt;Once you have given users individual logins, you'll need to change the password the application uses to login to SQL. And you need to ensure it's not available to developers or others. You can do this by specifying things in the portal, if you use web apps. However, there are two other methods you may want to consider;&lt;br /&gt;&lt;br /&gt;&lt;ul&gt;&lt;li&gt;You can let your Application itself authenticate to SQL with Azure AD using a certificate - see the same link as above about using Azure AD with SQL Azure. This way there literally is no password and as long as you delete the certificate after uploading it, there is no way for anyone to log in to SQL as the application.&lt;br /&gt;&lt;/li&gt;&lt;li&gt;You could store the SQL password in Azure Key Vault.&lt;/li&gt;&lt;/ul&gt;&lt;br /&gt;&lt;br /&gt;&lt;h3&gt;Step 3 - audit all the things&lt;/h3&gt;SQL Azure has an &lt;a href=&quot;https://azure.microsoft.com/en-gb/documentation/articles/sql-database-auditing-get-started/&quot;&gt;Auditing facility&lt;/a&gt; whereby you can audit all queries made against the database. Given that you have assigned an individual user name to each of your people, you can (and should) audit exactly which queries each of your people have run - and Azure will even flag up queries that return an unusually large number of records. This is gold dust for complying with InfoSec requirements about preventing internal data leakage.&lt;br /&gt;&lt;br /&gt;&lt;h3&gt;Step 4 - use data masking&lt;/h3&gt;SQL Azure has a &lt;a href=&quot;https://azure.microsoft.com/en-gb/documentation/articles/sql-database-dynamic-data-masking-get-started/&quot;&gt;facility to &quot;mask&quot;&lt;/a&gt; certain columns, such as email addresses, phone numbers and so on. I think the original design of this feature was as an aide to applications. But actually it's really useful in complying with InfoSec requirements about limiting access to sensitive data; You can set it up so your Application has normal access to all the data, but the logins your users have can automatically mask sensitive data. This means your users can write queries as normal and even see if data exists, but sensitive details that they don't need for troubleshooting will be masked in the output.&lt;br /&gt;Do understand that this feature is not completely tamper proof; you can still search on the underlying data, so with patience you can triangulate your way to the real data - but then that will show up in the audit logs.&lt;br /&gt;&lt;br /&gt;&lt;h2&gt;Conclusion&lt;/h2&gt;When properly configured, I believe SQL Azure offers a very compelling InfoSec story, one that should be able to satisfy most security audits you are likely to encounter.&lt;br /&gt;&lt;br /&gt;</content><author><name>Frans Lytzen</name></author><category term="Security" /><category term="Azure" /><category term="SQL" /><summary type="html">I've been using SQL Azure since 2011 and it's been a journey. One of the big problems I used to have was passing security audits from some of our clients; We deal with a &amp;nbsp;lot of data that is highly sensitive so are under a lot of scrutiny to make sure it is protected.A lot of what you have to comply with when you start going into the areas of ISO27001 and PCI compliance is not just about technical security in the way we normally think about it, it’s just as much about people and processes. In my experience from filling in dozens of security questionnaires, I believe that SQL Azure can tick all the boxes, as long as you switch on the right features. Just to be clear, I'm not required to comply with PCI so can't vouch for that, but I do have to comply with a number of ISO27001 and Data Protection requirements.Box tickingMost security audits will ask you if your data is &quot;encrypted at rest&quot;. If you understand how Azure really works and where &amp;nbsp;that requirement originally comes from, you'll know that this is a meaningless requirement when you're in Azure. But, just try to convince a security auditor of that. So go ahead and switch on Transparent Data Encryption&amp;nbsp; and you can tick that box on the questionnaire.Incidentally, Azure also recently enabled you to do something similar for Blob storage - again mainly so you can tick the box.Intrusion detectionSQL Azure has a neat Threat Detection facility to monitor the use of your database and alert you to anomalous behaviour. I've seen it detect potential SQL injection attacks and allegedly it will detect &quot;unusual&quot; behaviour, though I have yet to actually see that (thankfully :)).This is &amp;nbsp;a useful feature to potentially detect both external and internal attacks (see below).You have nothing to lose by switching it on.Internal attacksAs a developer it's easy to think that once you pass penetration testing, your job is done. But, for the organisation, that's only half the battle. A proper security audit will look just as much at how you are preventing and detecting what they sometimes call &quot;Internal Data Leakage&quot;. Basically, they are worried that people inside your organisation may access the data and leak it. This may be deliberately, it may be a result of social engineering or it may be that an external agency decided to hack a person who works for you. It's the modern equivalent of sneaking in through the kitchen entrance instead of taking a battering ram to the front gates. It's hardly surprising there is a lot of focus on this, given that most of the big attacks recently seems to have come from some kind of inside job.You may well trust everyone in your organisation to not want to deliberately betray you and to be sensible about not having their laptops hacked - but an auditor won't, so you may as well buckle up and do the right thing. SQL Azure makes it surprisingly easy, though there are number of manual steps that could be made a lot easier with some better tooling, especially if you have many databases.Step 1 - Give users individual loginsYour application probably uses a master username and password to log in to SQL. It's tempting to just give that to the people who need to go and look stuff up in the database, including developers, devops, support etc. But this means you have no way of knowing which actual person did what. Worse, when someone leaves the organisation you may not find it that easy to change the application's SQL Password.You can create individual users in SQL Azure to give to the people who need access and I urge you to do so. You may want to give users read-only access while you are it. The tooling is lacking, in that you have to create the users using T-SQL from something like SQL Server Management Studio or similar; you can’t manage the users in the Azure Portal, which is quite an oversight IMO.If you use Azure AD for authentication you can add Azure AD users to the SQL Databases. This has the benefit that user access will be automatically revoked from all databases when the user's AD Account is disabled - probably when they leave the organisation. It also means that, as of August 2016, you can use 2 Factor Authentication with the SQL logins - just download the latest version of SQL Server Management Studio 2016 and choose the Universal authentication method.At NewOrbit, all user access to Azure and SQL &amp;nbsp;Azure is controlled through Azure AD, through our Office 365 subscription and all users are required to have 2FA enabled.Step 2 - hide the sign-in details for the applicationOnce you have given users individual logins, you'll need to change the password the application uses to login to SQL. And you need to ensure it's not available to developers or others. You can do this by specifying things in the portal, if you use web apps. However, there are two other methods you may want to consider;You can let your Application itself authenticate to SQL with Azure AD using a certificate - see the same link as above about using Azure AD with SQL Azure. This way there literally is no password and as long as you delete the certificate after uploading it, there is no way for anyone to log in to SQL as the application.You could store the SQL password in Azure Key Vault.Step 3 - audit all the thingsSQL Azure has an Auditing facility whereby you can audit all queries made against the database. Given that you have assigned an individual user name to each of your people, you can (and should) audit exactly which queries each of your people have run - and Azure will even flag up queries that return an unusually large number of records. This is gold dust for complying with InfoSec requirements about preventing internal data leakage.Step 4 - use data maskingSQL Azure has a facility to &quot;mask&quot; certain columns, such as email addresses, phone numbers and so on. I think the original design of this feature was as an aide to applications. But actually it's really useful in complying with InfoSec requirements about limiting access to sensitive data; You can set it up so your Application has normal access to all the data, but the logins your users have can automatically mask sensitive data. This means your users can write queries as normal and even see if data exists, but sensitive details that they don't need for troubleshooting will be masked in the output.Do understand that this feature is not completely tamper proof; you can still search on the underlying data, so with patience you can triangulate your way to the real data - but then that will show up in the audit logs.ConclusionWhen properly configured, I believe SQL Azure offers a very compelling InfoSec story, one that should be able to satisfy most security audits you are likely to encounter.</summary></entry><entry><title type="html">How React.js suddenly clicked</title><link href="https://www.lytzen.name/2016/07/15/reactjs-clicking-for-me.html" rel="alternate" type="text/html" title="How React.js suddenly clicked" /><published>2016-07-15T14:01:00+01:00</published><updated>2016-07-15T14:01:00+01:00</updated><id>https://www.lytzen.name/2016/07/15/reactjs-clicking-for-me</id><content type="html" xml:base="https://www.lytzen.name/2016/07/15/reactjs-clicking-for-me.html">I've been working with Angular 1.0 for a few years and have been looking at other frameworks to move to in the last six months or so, including Angular 2.0, Aurelia, Ember and React. This post is not about which is best, it's about the mental block I had on React specifically and how I had to change my perspective to &quot;get&quot; it. Just wanted to share in case someone else has the same perspective problem I had.&lt;br /&gt;&lt;br /&gt;I viewed one of the Pluralsight courses on React, listened to people, read posts and documentation but I was constantly left with the feeling that React was just a disparate set of ideas and libraries and to actually build something useful I'd have to spend a lot of time deciding which bits to use and writing a lot of low-level code before I could start to do anything useful. That probably isn't actually true - but my real problem was that I was thinking about it all wrong.&lt;br /&gt;&lt;br /&gt;Primarily, in working with Angular 1.0 - and indeed ASP.Net MVC before that and a host of other frameworks and approaches going back a couple of decades, I had a fixed view in my mind that went something like &quot;user navigates to X, then we run some code that deals with that part of the application and renders a UI&quot;. If you think of ASP.Net MVC or Angular 1.0, for example, you have a route (usually encoded as a URL) and when the user goes there in the browser, we run some code that is essentially isolated from everything else in order to render a relevant UI.&lt;br /&gt;&lt;br /&gt;When looking at React I couldn't see how I could easily get there - how could I easily &quot;navigate to different parts of the application&quot;. The React library itself had no way to give me that kind of thing so it felt like &quot;sure, it can do some simple little todo list, but what happens when I have multiple screens etc - I'll have to use these other libraries to add the navigation etc etc&quot;. That thinking is fundamentally wrong with React.&lt;br /&gt;&lt;br /&gt;In more traditional frameworks and approaches you usually have separate application parts that work largely in isolation to each other and you &quot;navigate&quot; between those by some means, causing each &quot;part&quot; to spin up, set it's state and render a UI. In React, there is a single &quot;application&quot; or &quot;part&quot;; it starts with a root component, you change the state of the single application/part and the UI (re-)renders from the root down.&lt;br /&gt;&lt;br /&gt;With React, you don't really &quot;navigate between discrete parts of the application&quot;, you &quot;change the state of the application, which in turn causes the UI to render differently&quot;.&lt;br /&gt;So, to use an oversimplified example, the application may be in an &quot;edit users&quot; &lt;i&gt;state&lt;/i&gt;&amp;nbsp;which then renders the whole UI &lt;i&gt;from the root down&lt;/i&gt; to show you what you need in order to edit users - along with everything that sits above that, such as menus etc.&lt;br /&gt;This is in contrast to the more traditional approach where you'd &quot;navigate&quot; to &quot;edit users&quot; through some means, which would (in Angular or MVC terms) fire up an &quot;edit users&quot; controller, which would render the appropriate UI. That UI would usually sit within a kind of &quot;master page&quot; or similar, which would give menus etc - but, crucially, this is not really an application hierarchy, it's more about UI containers.&lt;br /&gt;&lt;br /&gt;Another way of thinking about it, if a bit overstretched, is that in React the UI is a side-effect of the &lt;i&gt;state &lt;/i&gt;of the application as a whole, whereas in the more traditional approach, you have a bunch of separate UI parts that each control bits of state.&lt;br /&gt;&lt;br /&gt;Yet another way to think about it is that a React application consists of a tree of components and any change to the state of any one component changes the state of the application as a whole, causing the whole UI to re-render.&lt;br /&gt;&lt;br /&gt;Of course, React has some very clever algorithms to make that constant re-render of the entire UI extremely fast and efficient which is why this works.&lt;br /&gt;&lt;br /&gt;What does this mean? Well, the whole notion of &quot;routes&quot; or &quot;parts of the application&quot; that I was so hung up on is now unnecessary - at least from the perspective of &amp;nbsp;building an application that the user can interact with and move around in. You probably &lt;i&gt;do&lt;/i&gt;&amp;nbsp;still want routes, so the URL can reflect the state of the application, allow users to &quot;go back&quot;, bookmark specific states and so on. But this is not a fundamental requirement for building the app, it's now something you can simply layer on top as a convenience feature for the user, if it's appropriate for your scenario.&lt;br /&gt;&lt;br /&gt;There is, obviously, a lot more to actually building real-world React applications. But getting over that initial fundamental hurdle to understand how I even start was a breakthrough.&lt;br /&gt;I should also mention that one might read what I have said above in a way that implies that a React application is a big pile of mud where everything is tightly coupled to everything else. That is categorically not the case, in fact the opposite is true; The React approach is excellent at separating concerns and managing data flows in a smart way - it's just not something that was particularly relevant to explain in the context of how React suddenly clicked for me.</content><author><name>Frans Lytzen</name></author><category term="JavaScript" /><category term="React" /><summary type="html">I've been working with Angular 1.0 for a few years and have been looking at other frameworks to move to in the last six months or so, including Angular 2.0, Aurelia, Ember and React. This post is not about which is best, it's about the mental block I had on React specifically and how I had to change my perspective to &quot;get&quot; it. Just wanted to share in case someone else has the same perspective problem I had.I viewed one of the Pluralsight courses on React, listened to people, read posts and documentation but I was constantly left with the feeling that React was just a disparate set of ideas and libraries and to actually build something useful I'd have to spend a lot of time deciding which bits to use and writing a lot of low-level code before I could start to do anything useful. That probably isn't actually true - but my real problem was that I was thinking about it all wrong.Primarily, in working with Angular 1.0 - and indeed ASP.Net MVC before that and a host of other frameworks and approaches going back a couple of decades, I had a fixed view in my mind that went something like &quot;user navigates to X, then we run some code that deals with that part of the application and renders a UI&quot;. If you think of ASP.Net MVC or Angular 1.0, for example, you have a route (usually encoded as a URL) and when the user goes there in the browser, we run some code that is essentially isolated from everything else in order to render a relevant UI.When looking at React I couldn't see how I could easily get there - how could I easily &quot;navigate to different parts of the application&quot;. The React library itself had no way to give me that kind of thing so it felt like &quot;sure, it can do some simple little todo list, but what happens when I have multiple screens etc - I'll have to use these other libraries to add the navigation etc etc&quot;. That thinking is fundamentally wrong with React.In more traditional frameworks and approaches you usually have separate application parts that work largely in isolation to each other and you &quot;navigate&quot; between those by some means, causing each &quot;part&quot; to spin up, set it's state and render a UI. In React, there is a single &quot;application&quot; or &quot;part&quot;; it starts with a root component, you change the state of the single application/part and the UI (re-)renders from the root down.With React, you don't really &quot;navigate between discrete parts of the application&quot;, you &quot;change the state of the application, which in turn causes the UI to render differently&quot;.So, to use an oversimplified example, the application may be in an &quot;edit users&quot; state&amp;nbsp;which then renders the whole UI from the root down to show you what you need in order to edit users - along with everything that sits above that, such as menus etc.This is in contrast to the more traditional approach where you'd &quot;navigate&quot; to &quot;edit users&quot; through some means, which would (in Angular or MVC terms) fire up an &quot;edit users&quot; controller, which would render the appropriate UI. That UI would usually sit within a kind of &quot;master page&quot; or similar, which would give menus etc - but, crucially, this is not really an application hierarchy, it's more about UI containers.Another way of thinking about it, if a bit overstretched, is that in React the UI is a side-effect of the state of the application as a whole, whereas in the more traditional approach, you have a bunch of separate UI parts that each control bits of state.Yet another way to think about it is that a React application consists of a tree of components and any change to the state of any one component changes the state of the application as a whole, causing the whole UI to re-render.Of course, React has some very clever algorithms to make that constant re-render of the entire UI extremely fast and efficient which is why this works.What does this mean? Well, the whole notion of &quot;routes&quot; or &quot;parts of the application&quot; that I was so hung up on is now unnecessary - at least from the perspective of &amp;nbsp;building an application that the user can interact with and move around in. You probably do&amp;nbsp;still want routes, so the URL can reflect the state of the application, allow users to &quot;go back&quot;, bookmark specific states and so on. But this is not a fundamental requirement for building the app, it's now something you can simply layer on top as a convenience feature for the user, if it's appropriate for your scenario.There is, obviously, a lot more to actually building real-world React applications. But getting over that initial fundamental hurdle to understand how I even start was a breakthrough.I should also mention that one might read what I have said above in a way that implies that a React application is a big pile of mud where everything is tightly coupled to everything else. That is categorically not the case, in fact the opposite is true; The React approach is excellent at separating concerns and managing data flows in a smart way - it's just not something that was particularly relevant to explain in the context of how React suddenly clicked for me.</summary></entry><entry><title type="html">Azure DocumentDb Go-Faster button</title><link href="https://www.lytzen.name/2016/05/12/azure-documentdb-go-faster-button.html" rel="alternate" type="text/html" title="Azure DocumentDb Go-Faster button" /><published>2016-05-12T23:04:00+01:00</published><updated>2016-05-12T23:04:00+01:00</updated><id>https://www.lytzen.name/2016/05/12/azure-documentdb-go-faster-button</id><content type="html" xml:base="https://www.lytzen.name/2016/05/12/azure-documentdb-go-faster-button.html">&lt;blockquote&gt;&quot;Premature optimisation is the root of all evil.&quot;&lt;/blockquote&gt;That said, if you are using DocumentDb with the .Net client please do two things in your code right now. They are safe and they will speed up your database access - potentially by several orders of magnitude. I delayed putting it into my own application because I assumed that, like most of these things, it would only be a minor improvement in certain edge cases. In reality, I increased my sustained write speed by about 15x.&lt;br /&gt;&lt;br /&gt;&lt;h4&gt;Increase the connection limit&lt;/h4&gt;If you do nothing else, at least stick this somewhere in your application startup. The most important bit is the DefaultConnectionLimit. The actual number you set for connection limit depends on your particular scenario. However, AFAIK, the main reason for keeping it low has to do with the risk of trading outbound connections for inbound connections in a web application - but if you have to worry about that, you almost certainly already know about it.&lt;br /&gt;&lt;pre class=&quot;prettyprint&quot;&gt;    ServicePointManager.UseNagleAlgorithm = false;&lt;br /&gt;    ServicePointManager.Expect100Continue = false;&lt;br /&gt;    ServicePointManager.DefaultConnectionLimit = 10000;&lt;/pre&gt;&lt;h4&gt;Use direct connection&lt;/h4&gt;&lt;i&gt;Update: In SDK 1.9 Direct is now the default so you don't have to do this anymore&lt;/i&gt;&lt;br /&gt;When you construct your document client, do it something like this:&lt;br /&gt;&lt;pre class=&quot;prettyprint&quot;&gt;    return new DocumentClient(&lt;br /&gt;            endpoint, &lt;br /&gt;            secureKey, &lt;br /&gt;            new ConnectionPolicy()&lt;br /&gt;                {&lt;br /&gt;                     ConnectionMode = ConnectionMode.Direct,&lt;br /&gt;                     ConnectionProtocol = Protocol.Tcp&lt;br /&gt;                });&lt;br /&gt;&lt;/pre&gt;&lt;br /&gt;&lt;h1&gt;Background&lt;/h1&gt;You need to only have a single DocumentDb Client within you app domain - otherwise you may get intermittent socket errors. However, by default, the underlying .Net comms layer will only allow you to have two (2) concurrent connections. So, no matter how much you do in parallel or how many separate web requests you are serving, only two concurrent calls will be made to the database - which will completely kill your performance. Setting the DefaultConnectionLimit to a higher number allows you to have more parallel database connections and is genuinely DocumentDb's go-faster button.&lt;br /&gt;The other bits on ServicePointManager are much more marginal but won't hurt and may help with other Azure services, including Table and Blob Storage.&lt;br /&gt;&lt;br /&gt;&lt;i&gt;Update: In SDK 1.9 Direct is now the default&lt;/i&gt;&lt;br /&gt;&lt;strike&gt;Using the Direct Connection option when creating your DocumentClient is also pretty much a no-brainer. The default setting to use http and a gateway is just to make sure it &quot;always works&quot;, even behind obscure corporate firewalls. Changing to Direct and Tcp is safe - the .Net SDK will handle the work for you so there is no extra code you have to write. I haven't measured how much extra raw performance it gives you and it may be somewhat marginal, but just get it in there while you are at it.&lt;/strike&gt;&lt;br /&gt;&lt;br /&gt;See also an example from Microsoft here: &lt;a href=&quot;https://github.com/arramac/documentdb-partitioned-collections-demo/blob/master/ElasticCollectionsDemo/Program.cs&quot;&gt;https://github.com/arramac/documentdb-partitioned-collections-demo/blob/master/ElasticCollectionsDemo/Program.cs&lt;/a&gt;. They also increase the minimum number of active threads in the background thread pool. You may want to look at that too, I haven't tested it.</content><author><name>Frans Lytzen</name></author><category term="CosmosDB" /><category term="Azure" /><summary type="html">&quot;Premature optimisation is the root of all evil.&quot;That said, if you are using DocumentDb with the .Net client please do two things in your code right now. They are safe and they will speed up your database access - potentially by several orders of magnitude. I delayed putting it into my own application because I assumed that, like most of these things, it would only be a minor improvement in certain edge cases. In reality, I increased my sustained write speed by about 15x.Increase the connection limitIf you do nothing else, at least stick this somewhere in your application startup. The most important bit is the DefaultConnectionLimit. The actual number you set for connection limit depends on your particular scenario. However, AFAIK, the main reason for keeping it low has to do with the risk of trading outbound connections for inbound connections in a web application - but if you have to worry about that, you almost certainly already know about it. ServicePointManager.UseNagleAlgorithm = false; ServicePointManager.Expect100Continue = false; ServicePointManager.DefaultConnectionLimit = 10000;Use direct connectionUpdate: In SDK 1.9 Direct is now the default so you don't have to do this anymoreWhen you construct your document client, do it something like this: return new DocumentClient( endpoint, secureKey, new ConnectionPolicy() { ConnectionMode = ConnectionMode.Direct, ConnectionProtocol = Protocol.Tcp });BackgroundYou need to only have a single DocumentDb Client within you app domain - otherwise you may get intermittent socket errors. However, by default, the underlying .Net comms layer will only allow you to have two (2) concurrent connections. So, no matter how much you do in parallel or how many separate web requests you are serving, only two concurrent calls will be made to the database - which will completely kill your performance. Setting the DefaultConnectionLimit to a higher number allows you to have more parallel database connections and is genuinely DocumentDb's go-faster button.The other bits on ServicePointManager are much more marginal but won't hurt and may help with other Azure services, including Table and Blob Storage.Update: In SDK 1.9 Direct is now the defaultUsing the Direct Connection option when creating your DocumentClient is also pretty much a no-brainer. The default setting to use http and a gateway is just to make sure it &quot;always works&quot;, even behind obscure corporate firewalls. Changing to Direct and Tcp is safe - the .Net SDK will handle the work for you so there is no extra code you have to write. I haven't measured how much extra raw performance it gives you and it may be somewhat marginal, but just get it in there while you are at it.See also an example from Microsoft here: https://github.com/arramac/documentdb-partitioned-collections-demo/blob/master/ElasticCollectionsDemo/Program.cs. They also increase the minimum number of active threads in the background thread pool. You may want to look at that too, I haven't tested it.</summary></entry><entry><title type="html">Auto deploy Azure Web Job with ASP.Net 5 and source control deploy</title><link href="https://www.lytzen.name/2016/03/29/auto-deploy-azure-web-job-with-aspnet-5.html" rel="alternate" type="text/html" title="Auto deploy Azure Web Job with ASP.Net 5 and source control deploy" /><published>2016-03-29T21:37:00+01:00</published><updated>2016-03-29T21:37:00+01:00</updated><id>https://www.lytzen.name/2016/03/29/auto-deploy-azure-web-job-with-aspnet-5</id><content type="html" xml:base="https://www.lytzen.name/2016/03/29/auto-deploy-azure-web-job-with-aspnet-5.html">&lt;h2&gt;Update&lt;/h2&gt;An updated version of this post that targets ASP.Net Core RTM &lt;a href=&quot;http://blog.lytzen.name/2016/08/auto-publish-azure-web-jobs-with-aspnet.html&quot;&gt;is now available&lt;/a&gt;.&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;h2&gt;The content below here refers to RC1 which is not supported on Azure&lt;/h2&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;At the time of this writing, there is no tooling to auto publish Azure Web Jobs with an ASP.Net 5 (DNX) website. You can do it with old-style websites, but not yet the new ones. The tooling is highly likely to appear as we get closer to release, but as of right now you have to take a few steps to set it up yourself, which is what I describe in this post. For clarity, what I am describing works only with source control deploy (git etc), &lt;i&gt;not&lt;/i&gt;&amp;nbsp;with publishing from Visual Studio.&lt;br /&gt;&lt;br /&gt;&lt;b&gt;This information will become obsolete nearer to the release of ASP.Net 5. If that has happened and I have not updated the post, please post a comment and I'll get on it.&lt;/b&gt;&lt;br /&gt;Note that for very simple, self-contained web jobs, there may be a simpler approach (see&amp;nbsp;&lt;a href=&quot;http://stackoverflow.com/a/33291097/11534&quot;&gt;http://stackoverflow.com/a/33291097/11534&lt;/a&gt; for some thoughts). In this post I am catering for the scenario where your webjob references another project in your solution - though it'll work just as well if it doesn't.&lt;br /&gt;&lt;br /&gt;&lt;h3&gt;In summary&lt;/h3&gt;&lt;div&gt;You have to use a custom deployment script and insert an action to &lt;i&gt;publish&lt;/i&gt;&amp;nbsp;your webjob to the path that Azure looks for webjobs in; App_Data/Jobs/Continuous for continuous jobs. Azure will automatically pick up on content in that folder. And yes, it will happily overwrite a running webjob.&lt;br /&gt;&lt;br /&gt;The reason we are &lt;i&gt;publishing, &lt;/i&gt;using DNU Publish, is to effectively build the web job and get all it's dependencies. Of course, once RC2 is out we'll probably have to change the syntax.&lt;br /&gt;If we had a very simple webjob that was self contained, you could just copy&amp;nbsp;the files, as long as you added a &lt;i&gt;run.cmd&lt;/i&gt;&amp;nbsp;(see link above).&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;&lt;h3&gt;In detail&lt;/h3&gt;&lt;div&gt;&lt;a href=&quot;https://github.com/flytzen/AutoDeplyAzureWebJob&quot;&gt;Full example on Github&lt;/a&gt;.&lt;br /&gt;&lt;br /&gt;&lt;h4&gt;Prepare a site&lt;/h4&gt;&lt;br /&gt;&lt;ol&gt;&lt;li&gt;Set up a solution with an ASP.Net 5 Web site and a webjob written as a ASP.Net 5 / Core console app.&lt;/li&gt;&lt;li&gt;Set up source control deploy to an Azure website. It shouldn't matter which type.&lt;/li&gt;&lt;li&gt;Wait for the initial deploy of the site.&amp;nbsp;&lt;/li&gt;&lt;/ol&gt;&lt;br /&gt;&lt;h4&gt;Set up a custom deployment script&lt;/h4&gt;&lt;/div&gt;&lt;div&gt;In this example, we will download the deployment script that Azure has created. There are ways to also do this with the Azure CLI, but the generated script is not quite the same at this point in time - which may or may not matter.&lt;/div&gt;&lt;div&gt;&lt;ol&gt;&lt;li&gt;Get the auto generated script&lt;/li&gt;&lt;ol&gt;&lt;li&gt;Log in to the web app console at https://[yoursite].scm.azurewebsites.net&lt;/li&gt;&lt;li&gt;Select Debug Console -&amp;gt; Powershell&lt;/li&gt;&lt;li&gt;Navigate to site/deployments/tools&lt;/li&gt;&lt;li&gt;Download the file deploy.cmd to the root of your solution folder&lt;/li&gt;&lt;/ol&gt;&lt;li&gt;Create a file in the root of your solution folder called .deployment with the following content&lt;br /&gt;&lt;pre class=&quot;prettyprint&quot;&gt;[config]&lt;br /&gt;command = deploy.cmd&lt;br /&gt;&lt;/pre&gt;&lt;/li&gt;&lt;li&gt;Modify your deploy.cmd file in the following ways&lt;/li&gt;&lt;ol&gt;&lt;li&gt;Add a line like this near the top - just to make it easier to check that your custom script is being used (whatever you put after echo will be output in the log file)&lt;br /&gt;&lt;pre class=&quot;prettyprint&quot;&gt;echo CUSTOM SCRIPT Start&lt;br /&gt;&lt;/pre&gt;&lt;/li&gt;&lt;li&gt;Near the middle of the file you will find a series of steps that are numbered. One of the steps will look like this; &lt;br /&gt;&lt;strong&gt;NOTE: The indented lines will be on a single line, the line breaks are only added here for readability&lt;/strong&gt;&lt;br /&gt;&lt;pre class=&quot;prettyprint&quot;&gt;:: 4. Run DNU Bundle&lt;br /&gt;call %DNX_RUNTIME%\bin\dnu publish&lt;br /&gt;  &quot;D:\home\site\repository\src\Web\project.json&quot; &lt;br /&gt;  --runtime %DNX_RUNTIME% &lt;br /&gt;  --out &quot;%DEPLOYMENT_TEMP%&quot; &lt;br /&gt;  %SCM_DNU_PUBLISH_OPTIONS%&lt;br /&gt;IF !ERRORLEVEL! NEQ 0 goto error&lt;br /&gt;&lt;/pre&gt;&lt;/li&gt;&lt;li&gt;Below that insert these lines (updating the path with the actual path to your webjob in the solution)&lt;br /&gt;&lt;strong&gt;NOTE: The indented lines need to put on a single line, the line breaks are only here for readability&lt;/strong&gt;&lt;br /&gt;&lt;pre class=&quot;prettyprint&quot;&gt;:: 4.1 build web job&lt;br /&gt;echo Starting to build webjob&lt;br /&gt;call %DNX_RUNTIME%\bin\dnu publish &lt;br /&gt;  &quot;D:\home\site\repository\src\MyWebJob\project.json&quot; &lt;br /&gt;  --runtime %DNX_RUNTIME% &lt;br /&gt;  --out &quot;%DEPLOYMENT_TEMP%\wwwroot\App_Data\Jobs\Continuous\MyWebJob&quot; &lt;br /&gt;  %SCM_DNU_PUBLISH_OPTIONS%&lt;br /&gt;IF !ERRORLEVEL! NEQ 0 goto error&lt;/pre&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/ol&gt;&lt;/div&gt;&lt;br /&gt;&lt;h4&gt;Check that it worked&lt;/h4&gt;&lt;div&gt;Push your changes and wait for Azure to deploy, then look at the webjobs in the Azure portal. You should see your job there. In case you don't, have a look at the publish log file to see if there are any errors in there.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;</content><author><name>Frans Lytzen</name></author><category term="Azure" /><summary type="html">UpdateAn updated version of this post that targets ASP.Net Core RTM is now available.The content below here refers to RC1 which is not supported on AzureAt the time of this writing, there is no tooling to auto publish Azure Web Jobs with an ASP.Net 5 (DNX) website. You can do it with old-style websites, but not yet the new ones. The tooling is highly likely to appear as we get closer to release, but as of right now you have to take a few steps to set it up yourself, which is what I describe in this post. For clarity, what I am describing works only with source control deploy (git etc), not&amp;nbsp;with publishing from Visual Studio.This information will become obsolete nearer to the release of ASP.Net 5. If that has happened and I have not updated the post, please post a comment and I'll get on it.Note that for very simple, self-contained web jobs, there may be a simpler approach (see&amp;nbsp;http://stackoverflow.com/a/33291097/11534 for some thoughts). In this post I am catering for the scenario where your webjob references another project in your solution - though it'll work just as well if it doesn't.In summaryYou have to use a custom deployment script and insert an action to publish&amp;nbsp;your webjob to the path that Azure looks for webjobs in; App_Data/Jobs/Continuous for continuous jobs. Azure will automatically pick up on content in that folder. And yes, it will happily overwrite a running webjob.The reason we are publishing, using DNU Publish, is to effectively build the web job and get all it's dependencies. Of course, once RC2 is out we'll probably have to change the syntax.If we had a very simple webjob that was self contained, you could just copy&amp;nbsp;the files, as long as you added a run.cmd&amp;nbsp;(see link above).In detailFull example on Github.Prepare a siteSet up a solution with an ASP.Net 5 Web site and a webjob written as a ASP.Net 5 / Core console app.Set up source control deploy to an Azure website. It shouldn't matter which type.Wait for the initial deploy of the site.&amp;nbsp;Set up a custom deployment scriptIn this example, we will download the deployment script that Azure has created. There are ways to also do this with the Azure CLI, but the generated script is not quite the same at this point in time - which may or may not matter.Get the auto generated scriptLog in to the web app console at https://[yoursite].scm.azurewebsites.netSelect Debug Console -&amp;gt; PowershellNavigate to site/deployments/toolsDownload the file deploy.cmd to the root of your solution folderCreate a file in the root of your solution folder called .deployment with the following content[config]command = deploy.cmdModify your deploy.cmd file in the following waysAdd a line like this near the top - just to make it easier to check that your custom script is being used (whatever you put after echo will be output in the log file)echo CUSTOM SCRIPT StartNear the middle of the file you will find a series of steps that are numbered. One of the steps will look like this; NOTE: The indented lines will be on a single line, the line breaks are only added here for readability:: 4. Run DNU Bundlecall %DNX_RUNTIME%\bin\dnu publish &quot;D:\home\site\repository\src\Web\project.json&quot; --runtime %DNX_RUNTIME% --out &quot;%DEPLOYMENT_TEMP%&quot; %SCM_DNU_PUBLISH_OPTIONS%IF !ERRORLEVEL! NEQ 0 goto errorBelow that insert these lines (updating the path with the actual path to your webjob in the solution)NOTE: The indented lines need to put on a single line, the line breaks are only here for readability:: 4.1 build web jobecho Starting to build webjobcall %DNX_RUNTIME%\bin\dnu publish &quot;D:\home\site\repository\src\MyWebJob\project.json&quot; --runtime %DNX_RUNTIME% --out &quot;%DEPLOYMENT_TEMP%\wwwroot\App_Data\Jobs\Continuous\MyWebJob&quot; %SCM_DNU_PUBLISH_OPTIONS%IF !ERRORLEVEL! NEQ 0 goto errorCheck that it workedPush your changes and wait for Azure to deploy, then look at the webjobs in the Azure portal. You should see your job there. In case you don't, have a look at the publish log file to see if there are any errors in there.</summary></entry></feed>