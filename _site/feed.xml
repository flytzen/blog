<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.7.3">Jekyll</generator><link href="https://www.lytzen.name/feed.xml" rel="self" type="application/atom+xml" /><link href="https://www.lytzen.name/" rel="alternate" type="text/html" /><updated>2018-10-03T22:18:02+01:00</updated><id>https://www.lytzen.name/</id><title type="html">Frans’ Randomness</title><subtitle>My very infrequent thoughts on the world of software development</subtitle><entry><title type="html">Publsh Nuget packages with Azure Dev Ops</title><link href="https://www.lytzen.name/2018/10/03/publish-nuget-packages-with-azure-devops.html" rel="alternate" type="text/html" title="Publsh Nuget packages with Azure Dev Ops" /><published>2018-10-03T00:00:00+01:00</published><updated>2018-10-03T00:00:00+01:00</updated><id>https://www.lytzen.name/2018/10/03/publish-nuget-packages-with-azure-devops</id><content type="html" xml:base="https://www.lytzen.name/2018/10/03/publish-nuget-packages-with-azure-devops.html">&lt;p&gt;Whenever I decide to create a Nuget package, whether for OSS or to publish on our internal MyGet feed I end up spending an inordinate amount of time trying to figure out a flow that works for testing and publishing. I guess it’s one of those things that, once you have figured it out, becomes easy but it has eluded me until recently.&lt;/p&gt;

&lt;p&gt;My requirements are quite specific and may not be to everyone’s liking;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;I want to use &lt;a href=&quot;https://datasift.github.io/gitflow/IntroducingGitFlow.html&quot;&gt;GitFlow&lt;/a&gt; to control my branches, including using Pull Requests etc.&lt;/li&gt;
  &lt;li&gt;Whenever a commit is made to &lt;code class=&quot;highlighter-rouge&quot;&gt;develop&lt;/code&gt; (or a PR is merged in), I want to publish that package with a “&lt;code class=&quot;highlighter-rouge&quot;&gt;-pre.123&lt;/code&gt;” suffix as per [SemVer].(https://semver.org/)&lt;/li&gt;
  &lt;li&gt;Whenever the same happens to &lt;code class=&quot;highlighter-rouge&quot;&gt;master&lt;/code&gt; I want to publish a “full” release.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I haven’t explicitly covered it here, but it would also be nice to have packages sat in a &lt;code class=&quot;highlighter-rouge&quot;&gt;release/*&lt;/code&gt; branch be published with a &lt;code class=&quot;highlighter-rouge&quot;&gt;-beta.1&lt;/code&gt; suffix - but you can easily extend it to cover that scenario as well.&lt;/p&gt;

&lt;p&gt;In this post I will show how to set up GitHub with Azure DevOps to do this for us.&lt;/p&gt;

&lt;p&gt;I am basing this on “modern” (i.e. 2017) csproj files, the ones where the package references are in the .csproj files. This came in with .Net Core but works fine with Full Framework projects.&lt;/p&gt;

&lt;h2 id=&quot;version-numbers&quot;&gt;Version numbers&lt;/h2&gt;
&lt;p&gt;When you create and publish Nuget packages you can specify the version number you want to use on the command line and there is ample of documentation about how to do that with Azure DevOps and other builder services, including MyGet build services, which I used previously.&lt;br /&gt;
However, I really like more control so I like to control the version number in my .csproj file - but I want the build service to automatically append &lt;code class=&quot;highlighter-rouge&quot;&gt;-pre.nnn&lt;/code&gt; when it published from the &lt;code class=&quot;highlighter-rouge&quot;&gt;develop&lt;/code&gt; branch.&lt;/p&gt;

&lt;p&gt;The first thing to understand is that there are two ways you can specify the version number in your &lt;code class=&quot;highlighter-rouge&quot;&gt;.csproj&lt;/code&gt; file:&lt;/p&gt;
&lt;div class=&quot;language-xml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nt&quot;&gt;&amp;lt;version&amp;gt;&lt;/span&gt;1.2.3-pre.987&lt;span class=&quot;nt&quot;&gt;&amp;lt;/version&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;or&lt;/p&gt;
&lt;div class=&quot;language-xml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nt&quot;&gt;&amp;lt;VersionPrefix&amp;gt;&lt;/span&gt;1.2.3&lt;span class=&quot;nt&quot;&gt;&amp;lt;/VersionPrefix&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;VersionSuffix&amp;gt;&lt;/span&gt;pre.987&lt;span class=&quot;nt&quot;&gt;&amp;lt;/VersionSuffix&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Both the above will create packages with version &lt;code class=&quot;highlighter-rouge&quot;&gt;1.2.3-pre.987&lt;/code&gt;. The naming of the Prefix and Suffix threw me for the longest time - I thought they were meant to interact with the &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;Version&amp;gt;&lt;/code&gt; attribute somehow, but Prefix and Suffix is more like “main part” and “extra bit” and you should &lt;em&gt;either&lt;/em&gt; those &lt;em&gt;or&lt;/em&gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;Version&amp;gt;&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The second thing to understand is that you can use conditionals and environment variables in the attributes. 
For my purposes, this is what I ended up with:&lt;/p&gt;

&lt;div class=&quot;language-xml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nt&quot;&gt;&amp;lt;VersionPrefix&amp;gt;&lt;/span&gt;1.2.3&lt;span class=&quot;nt&quot;&gt;&amp;lt;/VersionPrefix&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;VersionSuffix&amp;gt;&amp;lt;/VersionSuffix&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;VersionSuffix&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;Condition=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot; '$(Configuration)' == 'Debug' &quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;debug&lt;span class=&quot;nt&quot;&gt;&amp;lt;/VersionSuffix&amp;gt;&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;&amp;lt;!-- For local/debug builds --&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;VersionSuffix&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;Condition=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot; '$(Build_SourceBranch)' == 'refs/heads/develop' &quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;pre.$(Build_BuildID)&lt;span class=&quot;nt&quot;&gt;&amp;lt;/VersionSuffix&amp;gt;&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;&amp;lt;!-- This is using variables that are specific to Azure Dev Ops Pipelines --&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;My &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;VersionPrefix&amp;gt;&lt;/code&gt; here is really the proper version I want my package to have.&lt;br /&gt;
I have an empty &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;VersionSuffix&amp;gt;&lt;/code&gt; as default. I probably don’t actually need that tag, but it helps make it clearer in my mind.&lt;/p&gt;

&lt;p&gt;The next &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;VersionSuffix&amp;gt;&lt;/code&gt; uses the &lt;code class=&quot;highlighter-rouge&quot;&gt;Configuration&lt;/code&gt; variable that is provided by the dotnet build process; if I build in Debug mode, the package version will become &lt;code class=&quot;highlighter-rouge&quot;&gt;1.2.3-debug&lt;/code&gt;. This is mainly useful for local scenarios as I will always build in Release mode for publishing.&lt;/p&gt;

&lt;p&gt;The &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;VersionSuffix&amp;gt;&lt;/code&gt; after that looks at an environment variable provided by Azure DevOps when you are running in the pipeline. This means that if I am building from the &lt;code class=&quot;highlighter-rouge&quot;&gt;develop&lt;/code&gt; branch in an Azure Pipeline then it will set the Suffix. &lt;code class=&quot;highlighter-rouge&quot;&gt;Build_BuildID&lt;/code&gt; is another environment variable provided by Azure Dev Ops to the pipeline, which will always increment. So, in the example here I may end up with a version number of &lt;code class=&quot;highlighter-rouge&quot;&gt;1.2.3-pre.6239&lt;/code&gt;. As long as that last nunber reliably increments (which it does) you are fine for package control.&lt;br /&gt;
There is another variable called &lt;code class=&quot;highlighter-rouge&quot;&gt;Build_BuildNumber&lt;/code&gt; which you may be tempted to use instead. However, I found some scenarios where that variable would have the name of the pipeline instead of a number, which causes the build to fail.&lt;/p&gt;

&lt;p&gt;For more advanced scenarios you can invent your own attributes, which become variables in their own right, which you can then re-combine in other ways.&lt;/p&gt;

&lt;h2 id=&quot;publish-symbols&quot;&gt;Publish Symbols&lt;/h2&gt;
&lt;p&gt;Traditionally, when you create a Nuget package, it &lt;em&gt;won’t&lt;/em&gt; include the &lt;code class=&quot;highlighter-rouge&quot;&gt;pdb&lt;/code&gt; files (the debug symbols). In the past, the answer was to &lt;code class=&quot;highlighter-rouge&quot;&gt;--include-symbols&lt;/code&gt; when building your Nuget pacakge. This will create &lt;em&gt;two&lt;/em&gt; Nuget packages, one with the &lt;code class=&quot;highlighter-rouge&quot;&gt;pdb&lt;/code&gt; files and one without. Up until a few years ago, you could publish both of these together to Nuget, but then that changed and now you have to publish the symbols package to a different server with a different API key and a different command. It becomes a real headache, especially because of the inconsistent and out of date documentation. Hence why I have included it in this guide; I either need to tell you how to publish symbols from within the pipeline or tell you how to avoid it.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/dotnet/sourcelink&quot;&gt;SourceLink&lt;/a&gt; to the rescue. SourceLink provides a way to link your package to a specific commit on, say, GitHub or elsewhere. I do recommend using SourceLink as it does so much more than just give you the PDB file - but even if you can’t or won’t, there is a gem hidden in the documentation, namely this line to add to your &lt;code class=&quot;highlighter-rouge&quot;&gt;.csproj&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&quot;language-xml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nt&quot;&gt;&amp;lt;AllowedOutputExtensionsInPackageBuildOutputFolder&amp;gt;&lt;/span&gt;
  $(AllowedOutputExtensionsInPackageBuildOutputFolder);.pdb
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/AllowedOutputExtensionsInPackageBuildOutputFolder&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;What this will do is include the &lt;code class=&quot;highlighter-rouge&quot;&gt;pdb&lt;/code&gt; file in your main Nuget package, meaning you don’t need a separate symbols package at all. Of course, using the full SourceLink is much better. Incidentally, this also works for private repos without sharing the source publicly.&lt;/p&gt;

&lt;h2 id=&quot;variables&quot;&gt;Variables&lt;/h2&gt;
&lt;p&gt;When you are looking at the documentation for Azure DevOps there are lists of variables scattered in different places. You will probably also find that the same variable in some context is referred to as Build.BuildId and in another as BUILD_BUILDID etc. Sometimes you have to reference it as %BUILD_BUILDID%, other times as $(Build.BuildId) and yet other times as $(Build_BuildID). It does sort of make sense, but as a good starting point, when designing your YAML file, I recommend adding this task somewhere:&lt;/p&gt;
&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;script&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;set&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;displayName&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;show variables&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;It just dumps all the environment variables to the log, so you can have a look through to see what is actually available for you to reference.&lt;/p&gt;

&lt;h2 id=&quot;setting-up-a-pipeline&quot;&gt;Setting up a pipeline.&lt;/h2&gt;
&lt;p&gt;The easiest way to set up a build pipeline on Azure DevOps from GitHub is to add the &lt;a href=&quot;https://github.com/marketplace/azure-pipelines&quot;&gt;Azure Pipelines&lt;/a&gt; GitHub App to your Github account. When you connect it to a repository, it will walk you through setting up a default pipeline; just choose the “empty” option. This pipeline will save a YAML file into your repository and will set up two triggers. One is a simple trigger to run the pipeline for any commit on any branch, the other is a specific integration into Pull Requests; essentially any pull request will be run through the pipeline and if it fails it will block the PR from being merged.&lt;/p&gt;

&lt;p&gt;This is the YAML file I ended up:&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# This only runs for master and develop. Plus a seperate trigger is run for PR validation. This means commits to branches not in a PR won't get tested. Choices, choices...&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;NewOrbit.NewOrbit.AddOne - build and test&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;trigger&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;master&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;develop&lt;/span&gt;

&lt;span class=&quot;na&quot;&gt;variables&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;buildConfiguration&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Release&lt;/span&gt;

&lt;span class=&quot;na&quot;&gt;pool&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;vmImage&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;vs2017-win2016'&lt;/span&gt;

&lt;span class=&quot;na&quot;&gt;steps&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;

&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;script&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;set&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;displayName&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;show variables&lt;/span&gt;

&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;script&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;dotnet restore&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;displayName&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;dotnet restore&lt;/span&gt;

&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;script&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;dotnet build --configuration $(buildConfiguration) --no-restore&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;displayName&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;build&lt;/span&gt;

&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;task&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;DotNetCoreCLI@2&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;displayName&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;test&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;inputs&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;command&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;test&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;projects&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;**/*tests/*.csproj'&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;arguments&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;--configuration&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;$(buildConfiguration)'&lt;/span&gt;

&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;script&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;dotnet pack --configuration $(buildConfiguration) --no-build --output %Build_ArtifactStagingDirectory%&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;condition&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;and(succeeded(), or(eq(variables['Build.SourceBranchName'], 'master'),eq(variables['Build.SourceBranchName'], 'develop')))&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;displayName&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;pack&lt;/span&gt;

&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;task&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;NuGetCommand@2&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;displayName&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;publish&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;condition&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;and(succeeded(), or(eq(variables['Build.SourceBranchName'], 'master'),eq(variables['Build.SourceBranchName'], 'develop')))&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;inputs&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;command&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;push&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;nuGetFeedType&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;external&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;publishFeedCredentials&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;NewOrbit&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;MyGet&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Nuget'&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;packagesToPush&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;$(Build.ArtifactStagingDirectory)/**/*.nupkg'&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The &lt;code class=&quot;highlighter-rouge&quot;&gt;trigger&lt;/code&gt; part limits this to only run on checkins to &lt;code class=&quot;highlighter-rouge&quot;&gt;develop&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;master&lt;/code&gt; (note, some of the documentation has a more verbose syntax that seems to not work). The Pull Request trigger still works so all Pull Request and all commits into an open Pull Request will be run through this pipeline. But, for me, I don’t need CI to run on every commit on every feature branch. That’s just me - if you want the pipeline to run for every commit, just delete the &lt;code class=&quot;highlighter-rouge&quot;&gt;trigger&lt;/code&gt; section altogether.&lt;/p&gt;

&lt;p&gt;The steps through restore and build should be obvious. The step after that uses a special Azure DevOps task to run the unit tests, which ensures that the results are reported in a nice way in the pipeline.&lt;/p&gt;

&lt;p&gt;The &lt;code class=&quot;highlighter-rouge&quot;&gt;script: dotnet pack&lt;/code&gt; packs the Nuget package and outputs the package to a particular holding area. To be honest, I could probably forgo the &lt;code class=&quot;highlighter-rouge&quot;&gt;output&lt;/code&gt; parameter but it helps to understand what is going on.&lt;br /&gt;
The key thing here is the &lt;code class=&quot;highlighter-rouge&quot;&gt;condition&lt;/code&gt; line. This will ensure that a Nuget package is &lt;em&gt;only&lt;/em&gt; created if the build is of either the develop or the master branch. If you wanted to publish “beta” versions from &lt;code class=&quot;highlighter-rouge&quot;&gt;release/*&lt;/code&gt; branches, it should be straight forward to extend the condition accordingly.&lt;br /&gt;
Incidentally, there is an Azure DevOps task for creating the Nuget package but I couldn’t get it to work so used &lt;code class=&quot;highlighter-rouge&quot;&gt;dotnet pack&lt;/code&gt; instead.&lt;/p&gt;

&lt;p&gt;The final task publishes the created nuget package to Nuget. In this case I am publishing it to Myget; In order to do this, you first need to go to your &lt;em&gt;project&lt;/em&gt; in Azure DevOps, go to Project Settings and then select Service Connections (it’s well hidden). Then add a connection to Nuget or MyGet or whatever Nuget feed you want to publish to. You put the &lt;em&gt;name&lt;/em&gt; of that service connection in the &lt;code class=&quot;highlighter-rouge&quot;&gt;publishFeedCredentials&lt;/code&gt; property in the YAML file.&lt;/p&gt;

&lt;p&gt;if you wanted to publish packages from your develop branch to MyGet and the ones from Master to NuGet you can hopefully see how you can just duplicate the last task and change the &lt;code class=&quot;highlighter-rouge&quot;&gt;condition&lt;/code&gt; statements to suit your needs.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt; There is a bug in Azure DevOps that may result in an error saying something like that your pipeline doesn’t have the right permissiom to use the service connection. It’s easy to fix by following the guidance &lt;a href=&quot;https://docs.microsoft.com/en-us/azure/devops/pipelines/process/resources?view=vsts&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;approvals&quot;&gt;Approvals&lt;/h2&gt;
&lt;p&gt;The approach described above will publish packages immediately. If you wanted, you can easily set it up so you have to manually approve the publish. In short, you need to replace the final publish task in the YAML above with a Publish Artifacts task.&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;task&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;PublishBuildArtifacts@1&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;inputs&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;artifactName&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;package'&lt;/span&gt; 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This will copy whatever is in the &lt;code class=&quot;highlighter-rouge&quot;&gt;%Build_ArtifactStagingDirectory%&lt;/code&gt; directory (where we put the Nuget package before) and make it available as an artefact of the build. Once you run the pipeline, look at the build and you will see an Artefact. If you click on that, Azure DevOps will take you through a wizard to set up a release pipeline, which you can then use to add manual approval before you publish the package to Nuget.&lt;/p&gt;</content><author><name>Frans Lytzen</name></author><category term="DevOps" /><summary type="html">Use GitFlow and Azure Devops to automatically publish Nuget packages with sensible version numbers</summary></entry><entry><title type="html">Hierarchy IDs for Fun and Performance</title><link href="https://www.lytzen.name/2018/06/27/hierarchyid-for-fun-and-performance.html" rel="alternate" type="text/html" title="Hierarchy IDs for Fun and Performance" /><published>2018-06-27T00:00:00+01:00</published><updated>2018-06-27T00:00:00+01:00</updated><id>https://www.lytzen.name/2018/06/27/hierarchyid-for-fun-and-performance</id><content type="html" xml:base="https://www.lytzen.name/2018/06/27/hierarchyid-for-fun-and-performance.html">&lt;p&gt;Many systems have a &lt;em&gt;hierarchy&lt;/em&gt; in the data. This may be an organisation hierarchy, or maybe a hierarchy caused by a multi-tenant system or a combination thereof.&lt;br /&gt;
It’s relatively easy to model a hierarchy in a relational or document database - it is much harder to effectively filter which part of the tree a given user can see or act on. You may find yourself traversing up or down the tree or making multi-table joins.&lt;br /&gt;
A simpler solution is to use a Hierarchy ID. In a relational database, you would implement this as a string field with a delimiter-separated list of its parents, for example &lt;code class=&quot;highlighter-rouge&quot;&gt;120/23/47/19&lt;/code&gt;. If a user is allowed to see everything from &lt;code class=&quot;highlighter-rouge&quot;&gt;120/23&lt;/code&gt; downwards, you can easily search for all the records where the Hierarchy ID starts with &lt;code class=&quot;highlighter-rouge&quot;&gt;120/23&lt;/code&gt; (indexes will work well with that). In document databases it ma be more optimal to have an array of all the Ancestor IDs instead of a string value.&lt;/p&gt;

&lt;p&gt;This post explores the use of Hierarchy IDs to make filtering easy and performant. In addition, it has some pointers about how to set up hierarchical configuration using the same Hierarchy ID you are implementing anyway. Of course, you could just use a graph database and then it’s a different story altogether.&lt;/p&gt;

&lt;h1 id=&quot;the-problem&quot;&gt;The problem&lt;/h1&gt;

&lt;p&gt;A multi-tenant hierarchy may look like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.lytzen.name/assets/fixedhierarchy.png&quot; alt=&quot;Fixed Hierarchy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We’ll call this is a &lt;em&gt;fixed&lt;/em&gt; hierarchy as it has a fixed depth and different types at different levels.&lt;/p&gt;

&lt;p&gt;An organisational hierarchy may look like this;
&lt;img src=&quot;https://www.lytzen.name/assets/selfreferrentialhierarchy.png&quot; alt=&quot;Self-referential Hierarchy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We’ll call this a &lt;em&gt;self-referential&lt;/em&gt; hierarchy as each layer in the hierarchy refers to parent/children of the same type.&lt;/p&gt;

&lt;p&gt;At &lt;a href=&quot;https://neworbit.co.uk&quot;&gt;NewOrbit&lt;/a&gt; we sometimes build multi-tenanted systems which has resellers, who have system customers, who in turn have organisational hierarchies so it can quickly become a very deep hierarchy.&lt;/p&gt;

&lt;p&gt;When you are creating your data structure, whether in a relational database such as Azure SQL or a document database such as Mongo or CosmosDb, you will typically have a Parent ID on records in the hierarchy as illustrated above. This is easy at write time and makes it relatively easy to traverse the hierarchy both up and down. 
However, when you need to get access to a only a part of the tree or indeed multiple subsets of the tree then it becomes very hard to query efficiently and performantly.&lt;/p&gt;

&lt;p&gt;As an example with the self-referential hierarchy, imagine in the illustration above that a given user is allowed to see the details of anybody who is in Bob’s hierarchy; you’d need to write a recursive function to keep drilling down the layers until there are no more subordinates. That means running many different SQL queries or using UDFs (which in turn will run a recursive function).&lt;/p&gt;

&lt;p&gt;Alternatively in the fixed hierarchy, imagine if a given user has access to see all the projects for three separate branches and they want to see a list of all tasks across all projects they have access to. Or imagine that a Reseller user has access to see all the details for all their Customers, but not for any other Customer - and they want to see a list of all Projects (okay, maybe not the best example in the world, but you get the idea). In that scenario you’d probably write some code that JOINs all the way up the hierarchy and out to various permissions tables, such as “are they in the list that can see the project or in the list that can see the branch or in the list that can see the customer or in the list that can see the reseller”. It may not be that hard to write the code, but it’s easy to end up with a 20-table &lt;code class=&quot;highlighter-rouge&quot;&gt;JOIN&lt;/code&gt;, which is expensive on a relational database - and impossible if you use CosmosSB which does not support &lt;code class=&quot;highlighter-rouge&quot;&gt;JOIN&lt;/code&gt;s.&lt;/p&gt;

&lt;p&gt;In this post I am primarily focusing on how to &lt;em&gt;filter&lt;/em&gt; the data so a user can only see the data they are allowed in an efficient manner. There are other hierarchy scenarios, in particular around set based operations and there are other patterns that are better suited to those than what I am showing here. I highly recommend &lt;a href=&quot;https://www.amazon.co.uk/Hierarchies-Smarties-Kaufmann-Management-Systems/dp/0123877334&quot;&gt;Joe Celko’s Trees and Hierarchies in SQL for Smarties&lt;/a&gt; for understanding more about this. In fact, the lessons that I am expounding on in this post are based on what I learnt from that book many moons ago. Even if you use a document database, it’s still a good read to understand the patterns.&lt;/p&gt;

&lt;h1 id=&quot;how-to-store-hierarchy-ids&quot;&gt;How to store Hierarchy IDs&lt;/h1&gt;
&lt;p&gt;As discussed above, you will typically have some kind of Parent ID on each node in the hierarchy (though it’s usually called something more meaningful, such as CustomerID or ManagerID). 
The trick to efficient querying is to maintain a &lt;em&gt;Hierarchy ID&lt;/em&gt; on each record that has all the parent IDs all the way up the tree.&lt;br /&gt;
In a relational database, you would store it as a text string like this (for the Task in the fixed hierarchy example above);
&lt;code class=&quot;highlighter-rouge&quot;&gt;10/20/57/2/1047&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;In a Document Database you can do the same or use a slightly more efficient approach, which is to store an &lt;code class=&quot;highlighter-rouge&quot;&gt;array&lt;/code&gt; of Ancestor IDs. If your documents use &lt;code class=&quot;highlighter-rouge&quot;&gt;GUID&lt;/code&gt;s for IDs, then you can just put all the Ancestor IDs in the array. Otherwise (and only in the fixed hierarchy) you could prefix each ID with its record type - but that starts making it too blurry in my opinion.&lt;/p&gt;

&lt;p&gt;SQL Server has a native Hierarchy ID data type that gives you a few convenience functions, but it is not mapped in Entity Framework so if you use EF you may prefer to just use a normal string.&lt;/p&gt;

&lt;h1 id=&quot;querying&quot;&gt;Querying&lt;/h1&gt;
&lt;p&gt;In order to query the database you need to be able to search on Hierarchy IDs so ensure there is an index on the field for performance. Next you need to know which Hierarchy IDs a user is allowed to see.&lt;/p&gt;

&lt;p&gt;In the case of a &lt;strong&gt;relational database&lt;/strong&gt; you have a couple of options;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;If a user will only have access to a few Hierarchy IDs, then you can simply have those on listed on whatever user object (such as a ClaimsPrincipal) you are passing around in your code and you can then do some SQL along the lines of &lt;code class=&quot;highlighter-rouge&quot;&gt;SELECT FROM xx WHERE HierarchyID LIKE &quot;123/23%&quot; OR HierarchyID LIKE &quot;516/67/43/109%&quot; &lt;/code&gt;. 
If you have many Hierarchy IDs per user then this will end up with a lot of &lt;code class=&quot;highlighter-rouge&quot;&gt;OR&lt;/code&gt; statements which can really hurt performance, so be careful.&lt;/li&gt;
  &lt;li&gt;If a user has access to many Hierarchy IDs it may be better to maintain a Hierarchy ID table in the database, with each row having a user ID and a Hierarchy ID and then &lt;code class=&quot;highlighter-rouge&quot;&gt;INNER JOIN&lt;/code&gt; or &lt;code class=&quot;highlighter-rouge&quot;&gt;APPLY&lt;/code&gt; to that table in your query.&lt;br /&gt;
Just be mindful that if a user is allowed two Hierarchy IDs where one is a subset of the other, you will get duplicate records so you need to either use &lt;code class=&quot;highlighter-rouge&quot;&gt;DISTINCT&lt;/code&gt; or remove such “duplicates” from the User-HierarchyID table before &lt;code class=&quot;highlighter-rouge&quot;&gt;JOIN&lt;/code&gt;ing.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In the case of a &lt;strong&gt;document database&lt;/strong&gt; that doesn’t support &lt;code class=&quot;highlighter-rouge&quot;&gt;JOIN&lt;/code&gt;s your only option is to keep that list of allowed Hierarchy IDs or Ancestor IDs and then use OR statements.&lt;/p&gt;

&lt;h1 id=&quot;mutability-of-the-hierarchy&quot;&gt;Mutability of the hierarchy&lt;/h1&gt;
&lt;p&gt;Some hierarchies are essentially immutable, others can change at times. For example, in the example above with customers and projects, it is exceedingly unlikely that a customer will move between resellers or a project will move between customers. In that scenario you can probably treat the Hierarchy ID as write-only and just set it once when you create the records. In the rare circumstance where you may have to change it, you can deal with that as a one-off and handle it manually.&lt;/p&gt;

&lt;p&gt;Organisation hierarchies, in particular, have an annoying habit of changing over time. If someone’s manager changes, you have to update the Hierarchy IDs all the way down the tree. Depending on the potential size of the tree and your security requirements, this may be done in different ways. Bear in mind that when the CEO of a 100,000 person company changes, that’s a lot of records to update.
In most scenarios you can just have a simple function that recurses through all the affected records and updates each one in turn. You may implement this at the database level or in application code, depending on your requirements. In a relational database you may be tempted to wrap this entire thing in a transaction, but be mindful that this may escalate to a table lock, which may effectively lock your whole system up. Alternatively, updating each record in turn may mean that for a few minutes (for a very large change) some users will see a mixture of the records they used to be able to see and the records they are going to be able to see. The user should never see records they weren’t meant to see; it may just take a few minutes to remove all the records they used to be able to see and add all the new records. 
Users may also need to re-login if you are caching their list of allowed Hierarchy IDs in a session object of some kind. Mostly, these changes are infrequent - at least changes that affect many records - so it’s usually not something to worry too much about, as long as you understand it for your system.&lt;/p&gt;

&lt;h1 id=&quot;ease-of-writing-vs-ease-of-reading&quot;&gt;Ease of Writing vs ease of Reading&lt;/h1&gt;
&lt;p&gt;When you implement a Hierarchy ID, you are making it more complex to &lt;em&gt;write&lt;/em&gt; data; Whenever you add a node in the hierarchy you now have to set it’s Hierarchy ID. If your tree is mutable, you also need to handle updating the Hierarchy IDs of all children whenever nodes are moved - something that can take considerable time and capacity if a high-level node is moved. Similarly, you may need to write code to maintain a list of “allowed hierarchy IDs” per user.  In other words, Hierarchy IDs adds extra complexity to your system.&lt;br /&gt;
On the other hand, once Hierarchy IDs are in place, your data filtering/security code becomes much easier to write and your database queries will be much more performant.&lt;/p&gt;

&lt;p&gt;Whether Hierarchy IDs are right for a given solution will, as always, depend on the needs of that system. Some of the key indicators that you may need it are;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Deep hierarchies with access rights determined at multiple levels&lt;/li&gt;
  &lt;li&gt;Large data sets in a hierarchy&lt;/li&gt;
  &lt;li&gt;Self-referential hierarchies with access rights set at arbitrary levels (think pretty much any organisational hierarchy with scope-of-control security)&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;hierarchical-configuration&quot;&gt;Hierarchical Configuration&lt;/h1&gt;
&lt;p&gt;It is a common requirement to have Hierarchical Configuration where a certain setting is different for a certain part of the tree. For example, you may have some settings that only apply to a particular Reseller, Customer or even Branch etc.&lt;/p&gt;

&lt;p&gt;If you are already implementing Hierarchy IDs in the form of a delimited string, you could have a simple Configuration table that has the Key, the Value and the Hierarchy ID it applies to. 
When you want to find the value for a particular Hierarchy ID for a particular node, you can recursively search for a configuration setting that matches the node’s Hierarchy ID, or it’s parent or the grand parent etc, all the way up to the root. This is obviously not very efficient to query so you would need to cache it (or keep the whole thing in memory if possible), but it makes it very easy to model and extend.&lt;/p&gt;

&lt;p&gt;What you really want is something like (pseudo code)&lt;/p&gt;
&lt;div class=&quot;language-sql highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;SELECT&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Value&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Config&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;WHERE&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;HierarchyID&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;SUBSTRING&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;node_hierarchy_id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;LENGTH&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;HierarchyID&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;You may well be able to write a SQL statement something like that, though it will definitely use a table scan - so still cache it.&lt;/p&gt;

&lt;p&gt;In the case of document database with an array of Ancestor IDs, you’d need to ensure that the Ancestor ID array on the node is sorted by descent level so that you can look for configuration values in the right order (by walking up the tree).&lt;/p&gt;</content><author><name>Frans Lytzen</name></author><category term="Databases" /><category term="Performance" /><category term="SQL" /><category term="CosmosDB" /><summary type="html">Storing hierarchies in a database is easy - but applying hierarchical security and configuration can be very difficult and a significant performance problem. Hierarchy IDs can alleviate this, at the cost of a bit more complexity at write time.</summary></entry><entry><title type="html">GDPR for Software and how Azure can help</title><link href="https://www.lytzen.name/2018/06/25/GDPR-for-software.html" rel="alternate" type="text/html" title="GDPR for Software and how Azure can help" /><published>2018-06-25T00:00:00+01:00</published><updated>2018-06-25T00:00:00+01:00</updated><id>https://www.lytzen.name/2018/06/25/GDPR-for-software</id><content type="html" xml:base="https://www.lytzen.name/2018/06/25/GDPR-for-software.html">&lt;p&gt;When you develop software, whether for other people or for running your business, there are many things you have to consider which are quite different from the things you have to do to make your business GDPR compliant.&lt;br /&gt;
In the spring of 2018 I recorded a video with Microsoft that gives an overview of this as well looks at some of the ways in which Azure can help.&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/KlZhAG351Bs&quot; frameborder=&quot;0&quot; allow=&quot;autoplay; encrypted-media&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;For some more specific tips on how to lock down your software on Azure, including Managed Identity and KeyVault, see &lt;a href=&quot;/2018/04/29/securing-your-webapp-in-azure.html&quot;&gt;Securing your web app in Azure&lt;/a&gt;`&lt;/p&gt;</content><author><name>Frans Lytzen</name></author><category term="Azure" /><category term="GDPR" /><category term="Security" /><summary type="html">A video recording I made with Microsoft about how GDPR applies to Software development and how Azure can help.</summary></entry><entry><title type="html">CosmosDB token has wrong time</title><link href="https://www.lytzen.name/2018/05/08/Comos-db-token-failure.html" rel="alternate" type="text/html" title="CosmosDB token has wrong time" /><published>2018-05-08T12:00:00+01:00</published><updated>2018-05-08T12:00:00+01:00</updated><id>https://www.lytzen.name/2018/05/08/Comos-db-token-failure</id><content type="html" xml:base="https://www.lytzen.name/2018/05/08/Comos-db-token-failure.html">&lt;p&gt;Out of the blue, we started receiving the following error in our web app that uses CosmosDB:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;The authorization token is not valid at the current time. Please create another token and retry (token start time: Tue, 08 May 2018 09:17:33 GMT, token expiry time: Tue, 08 May 2018 09:32:33 GMT, current server time: Tue, 08 May 2018 09:11:00 GMT
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The important things to note here are &lt;code class=&quot;highlighter-rouge&quot;&gt;token start time: Tue, 08 May 2018 09:17:33 GMT&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;current server time: Tue, 08 May 2018 09:11:00 GMT&lt;/code&gt;.&lt;br /&gt;
The token has a start time &lt;em&gt;in the future&lt;/em&gt;. As a consequence, any request to CosmosDB made with this token fails.&lt;/p&gt;

&lt;p&gt;It took us a while to diagnose this because we just assumed that &lt;code class=&quot;highlighter-rouge&quot;&gt;current server time&lt;/code&gt; referred to the time on the Web server, but it actually refers to the time on the CosmosDB server.&lt;br /&gt;
We could see that the &lt;code class=&quot;highlighter-rouge&quot;&gt;current server time&lt;/code&gt; was correct, in that it matched the known correct time on other servers.&lt;/p&gt;

&lt;p&gt;What it boils down to is that the time on our webserver was wrong, specifically it was in the future, and the .Net client SDK was dutifully creating access tokens that started in the future.&lt;/p&gt;

&lt;p&gt;If you are running your app on your own server or a VM, you can stop reading now and go reset the time on your server and all will be well. What was really weird for us was that we are running this in a Web App in Azure App Services. They are supposed to be time synchronised and it simply shouldn’t be possible for their clock to drift. Except it did. We triangulated the problem by running the same code from another location and eventually from a web app deployed to a different App Service Plan.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;If you are experiencing this problem in Azure Web App, the TL;DR; is to scale your site &lt;em&gt;up&lt;/em&gt;, for example from an S1 to an S2 and then down again later. Details as to why are below.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The thing to remember here is that an App Service Plan is what encapsulates the underlying hardware that actually hosts the App Services. We had an App Service Plan with a single node in it and &lt;em&gt;all&lt;/em&gt; Web Apps deployed to this App Service Plan failed with the same error. Which is logical when you think about it, but easy to forget in the heat of battle.&lt;br /&gt;
We confirmed the issue by going to the “Advanced Tools” and, using the Powershell console, ran &lt;code class=&quot;highlighter-rouge&quot;&gt;Get-Date&lt;/code&gt;, which showed us that the web-server time was wrong.&lt;/p&gt;

&lt;p&gt;In our case we had just a single node in the App Service Plan. If you had multiple nodes and only one had time-drifted, you’d probably see the error intermittently and the &lt;code class=&quot;highlighter-rouge&quot;&gt;Get-Date&lt;/code&gt; would just return the time from whatever server the console happened to be running on. If you suspect this situation, it may be worthwhile scaling your app service plan down to a single node, testing it and scaling out again.&lt;/p&gt;

&lt;p&gt;What we needed was for Azure to kill our faulty node so we would automatically roll on to another node with the correct time. Unfortunately there are no tools to do this (and you wouldn’t expect to have to). However, there is a way you can do it and it was literally the process of writing this post that made me think about it, so I’ve just stopped writing to go fix the problem; Simply scale your site &lt;em&gt;up&lt;/em&gt; and then down again. For example, if you are on an S1, scale to an S2 as this will force Azure to deploy your site on a new underlying machine. You can scale back down again when you are happy.&lt;/p&gt;</content><author><name>Frans Lytzen</name></author><category term="Azure," /><category term="CosmosDB" /><summary type="html">CosmosDB error &quot;The authorization token is not valid at the current time&quot; and how to fix it.</summary></entry><entry><title type="html">Securing your web app in Azure</title><link href="https://www.lytzen.name/2018/04/29/securing-your-webapp-in-azure.html" rel="alternate" type="text/html" title="Securing your web app in Azure" /><published>2018-04-29T21:40:00+01:00</published><updated>2018-04-29T21:40:00+01:00</updated><id>https://www.lytzen.name/2018/04/29/securing-your-webapp-in-azure</id><content type="html" xml:base="https://www.lytzen.name/2018/04/29/securing-your-webapp-in-azure.html">&lt;p&gt;So you have deployed your web app to Azure. Now, how do you go about making it secure?
I gave a talk on this topic at  &lt;a href=&quot;https://www.meetup.com/dotnetoxford/&quot;&gt;DotNet Oxford&lt;/a&gt; on 24 April 2018 and recorded it. You can view the video below.&lt;/p&gt;

&lt;p&gt;The video runs through a scenario using an ASP.Net Web App hosted on Azure App Service and covers a number of features you can use to improve your security - as well as a number of features that are not available for App Services.&lt;/p&gt;

&lt;p&gt;The talk covers a lot of ground in an hour and everything is kept at a high level, but is nonetheless heavy on examples and code.&lt;br /&gt;
Watching the video myself, I realised I say “Okay” and “So” way, way too much. Sorry…&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;a href=&quot;https://twitter.com/jamesw0rld&quot;&gt;James World&lt;/a&gt; made this nice sketch note of the talk, reproduced with permission.&lt;br /&gt;
&lt;img src=&quot;https://www.lytzen.name/assets/2018-04-29-security-talk-sketch-note.jpg&quot; alt=&quot;Sketch note&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/2tR5sEk46v0&quot; frameborder=&quot;0&quot; allow=&quot;autoplay; encrypted-media&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;The source code is on &lt;a href=&quot;https://github.com/flytzen/SecurityTalk&quot;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;some-key-timings&quot;&gt;Some key timings&lt;/h2&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Use SSL&lt;/td&gt;
      &lt;td&gt;11:03&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Virus scanning&lt;/td&gt;
      &lt;td&gt;20:01&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;WAF&lt;/td&gt;
      &lt;td&gt;21:00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Vnet&lt;/td&gt;
      &lt;td&gt;23:20&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Azure Key Vault&lt;/td&gt;
      &lt;td&gt;26:10&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Managed Service Identity&lt;/td&gt;
      &lt;td&gt;27:50&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Use Key Vault and managed identify to store secrets&lt;/td&gt;
      &lt;td&gt;29:55&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;ASP.Net Core configuration with Key Vault&lt;/td&gt;
      &lt;td&gt;31:55&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Connect to Azure SQL with Managed Identity (or not)&lt;/td&gt;
      &lt;td&gt;36:27&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Encrypt data at rest&lt;/td&gt;
      &lt;td&gt;38:00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Require secure transport&lt;/td&gt;
      &lt;td&gt;40:30&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;SQL Always Encrypted&lt;/td&gt;
      &lt;td&gt;41:40&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Storage client-side encryption (not shown)&lt;/td&gt;
      &lt;td&gt;52:00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Use Azure AD to access Azure&lt;/td&gt;
      &lt;td&gt;53:25&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Use Azure AD to access Azure SQL&lt;/td&gt;
      &lt;td&gt;54:05&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Supporting Security tools in Azure&lt;/td&gt;
      &lt;td&gt;56:50&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Detection&lt;/td&gt;
      &lt;td&gt;57:45&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;</content><author><name>Frans Lytzen</name></author><category term="Azure" /><category term="Security" /><summary type="html">A video overview of some of the Azure technologies that you can use to better protect your web applications in Azure - all depending on your required security level, of course.</summary></entry><entry><title type="html">Azure Failover and Resilience</title><link href="https://www.lytzen.name/2017/06/29/azure-failover-and-resilience.html" rel="alternate" type="text/html" title="Azure Failover and Resilience" /><published>2017-06-29T17:54:00+01:00</published><updated>2017-06-29T17:54:00+01:00</updated><id>https://www.lytzen.name/2017/06/29/azure-failover-and-resilience</id><content type="html" xml:base="https://www.lytzen.name/2017/06/29/azure-failover-and-resilience.html">Azure provides a highly resilient hosting platform, with significant built-in redundancy within a data centre, as well as the presence of more than 30 data centres across the world.&lt;br /&gt;&lt;br /&gt;When first coming to Azure, it can be hard to understand what resilience you get automatically and what you might have to set up yourself.&lt;br /&gt;&lt;br /&gt;This post provides a high-level overview of the principles. It is intended as an introduction to help you ask the right questions.&lt;br /&gt;&lt;br /&gt;The usual starting point for a system is to host it in a single data centre. Azure is highly resilient even within a single data centre, but even so, all the data is continually backed up to a secondary data centre.&lt;br /&gt;&lt;br /&gt;In the case of a complete failure of a data centre, the data can be restored to another data centre. This is not the same as automatic failover to another data centre; In order to get the data restored in the other data centre and get the system back up and running, you will have to do it yourself; Azure will (for the most part) not do this for you. How much work depends on how much preparatory work has been done and is primarily a business decision based on risk and cost.&lt;br /&gt;&lt;br /&gt;Any conversation about failover is complicated by the fact that a system consists of different components, which can fail independently, which have different probability and impact and which require different failover strategies.&lt;br /&gt;&lt;br /&gt;Before going into the details, it is important to understand that even the most basic setup in Azure has a very high level of resilience with each individual component typically having a guaranteed uptime of 99.95% or more. At the same time, data is continually backed up to a secondary data centre. In other words, even the most basic Azure setup has a level of resilience that is difficult and expensive to achieve with on-premise hosting.&lt;br /&gt;&lt;br /&gt;In this post “failover” will refer to failing over between data centres.&lt;br /&gt;&lt;br /&gt;&lt;h4&gt;Resilience in a single data centre&lt;/h4&gt;&lt;div&gt;Azure Data Centres are built in a modular way, meaning that each data centre can be thought of as many smaller data centres built next to each other. This means that your data and system will be physically spread over different parts of the data centre, in turn meaning that even if an entire part of the data centre fails, you are unlikely to notice.&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;Physically, all Azure data centres have multiple redundant power grid connections, multiple redundant internet connections, redundant backup generators, batteries and so on and so forth.&lt;br /&gt;&lt;br /&gt;As a general rule, any data you save in Azure, in databases, to disk or to other types of storage, is written to three different locations inside that one data centre and a single copy is written to another remote data centre as a backup. For example the London data centre backs up to Cardiff and the Amsterdam data centre backs up to Dublin etc.&lt;br /&gt;&lt;br /&gt;Azure App Service has some built-in resilience so even with only a single compute node in your app hosting plan, you are pretty well protected from outages. With Cloud Services, you must ensure that you have at least two instances running at all times to ensure resilience. With Virtual Machines – you are much more on your own, though there are a number of things you can configure, such as Availability Sets etc. As a &lt;i&gt;very&lt;/i&gt; general rule, to make reliability easier, avoid using VMs, use one of the managed options instead, when you can.&lt;br /&gt;&lt;br /&gt;When you want to be able to fail over to another data centre, there are several options available to you. I have grouped them here under “Cold”, “Warm” and “Hot”. These are just convenience labels and may not correlate to other people’s definitions.&lt;br /&gt;&lt;br /&gt;&lt;h4&gt;Cold&lt;/h4&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://4.bp.blogspot.com/-wV3fbqWMgeU/WVUssFhNeqI/AAAAAAAALuA/n5RZdmPeCqM4Jr6snqblsOxVj7fUfEyvQCLcBGAs/s1600/Failover%2BOptions%2B-%2BCold.png&quot; imageanchor=&quot;1&quot; style=&quot;clear: right; float: right; margin-bottom: 1em; margin-left: 1em;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;560&quot; data-original-width=&quot;1180&quot; height=&quot;188&quot; src=&quot;https://4.bp.blogspot.com/-wV3fbqWMgeU/WVUssFhNeqI/AAAAAAAALuA/n5RZdmPeCqM4Jr6snqblsOxVj7fUfEyvQCLcBGAs/s400/Failover%2BOptions%2B-%2BCold.png&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;div&gt;A Cold failover is what you get by default.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;Your data is automatically backed up to another data centre. In the case of a failure of the primary data centre, you can go to the other data centre, set up your systems again, deploy everything and restore your database. Of course, the more automated your deployment is, the easier this will be.&lt;br /&gt;&lt;br /&gt;You should be aware that while you can manually trigger a restore of SQL and CosmosDB databases, you cannot yourself trigger a “restore” of anything you put into Azure Storage. Microsoft has to do that by changing DNS entries and their SLA on that is up to 48 hours, last time I checked. There are things you can do to improve this, such as using read-access geo-redundant storage, but you will need to develop specifically to take advantage of that. Often, though, the data in Azure Storage is secondary to the main system and you may be able to live without that data for a day or two.&lt;br /&gt;&lt;br /&gt;The exact frequency of database backups depends on the chosen database but is generally four hours or less.&lt;br /&gt;&lt;br /&gt;&lt;h4&gt;Warm&lt;/h4&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://4.bp.blogspot.com/-gk1abFDNI6A/WVUtOZGJaeI/AAAAAAAALuE/gEM_835uULUHb8zijsbskKBwbgclBf2zgCLcBGAs/s1600/Failover%2BOptions%2B-%2BWarm.png&quot; imageanchor=&quot;1&quot; style=&quot;clear: right; float: right; margin-bottom: 1em; margin-left: 1em;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;560&quot; data-original-width=&quot;1180&quot; height=&quot;188&quot; src=&quot;https://4.bp.blogspot.com/-gk1abFDNI6A/WVUtOZGJaeI/AAAAAAAALuE/gEM_835uULUHb8zijsbskKBwbgclBf2zgCLcBGAs/s400/Failover%2BOptions%2B-%2BWarm.png&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/div&gt;A Warm failover allows faster fail-over to a secondary data centre, but still requires manual intervention.&lt;br /&gt;&lt;br /&gt;In order to reduce the time it takes to move to a secondary data centre, it is possible to prepare the infrastructure and have detailed plans in place. You can do this by configuring the whole system in the secondary data centre but not deploy anything to it; For many services you can define it but just not deploy anything to it. Similarly, you can deploy VMs and then de-allocate them etc. An alternative is to create an ARM template, which will allow you to quickly create a whole environment in Azure.&lt;br /&gt;&lt;br /&gt;You should also write plans and scripts so you can quickly restore the databases and direct traffic to the other data centre etc. It may also require periodic testing of the plans.&lt;br /&gt;&lt;br /&gt;Finally, you should make sure your DNS is set up with a short enough TTL that you can quickly move traffic to the new websites.&lt;br /&gt;&lt;br /&gt;Document storage, such as files, may in the Warm scenario still take up to 48 hours to be made available in the other data centre.&lt;br /&gt;&lt;br /&gt;&lt;h4&gt;Hot&lt;/h4&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-jR6utAG2ch0/WVUtUWoKCoI/AAAAAAAALuI/uwkuqU-cVuIUfrzgbYqs1QsXDfgPzYemgCLcBGAs/s1600/Failover%2BOptions%2B-%2BHot.png&quot; imageanchor=&quot;1&quot; style=&quot;clear: right; float: right; margin-bottom: 1em; margin-left: 1em;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;980&quot; data-original-width=&quot;1180&quot; height=&quot;331&quot; src=&quot;https://1.bp.blogspot.com/-jR6utAG2ch0/WVUtUWoKCoI/AAAAAAAALuI/uwkuqU-cVuIUfrzgbYqs1QsXDfgPzYemgCLcBGAs/s400/Failover%2BOptions%2B-%2BHot.png&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/div&gt;A Hot failover will automatically fail over to a secondary data centre if the primary data centre fails, in whole or in part.&lt;br /&gt;&lt;br /&gt;In practice, there are many different components to a system and it usually makes sense to only have hot failover in place for some of the components. A bespoke cost/benefit exercise should be carried out where hot failover is desired.&lt;br /&gt;&lt;br /&gt;The primary things to consider;&lt;br /&gt;&lt;br /&gt;&lt;h3&gt;Web site failover&lt;/h3&gt;It is possible to deploy the front-end web servers &amp;nbsp;to more than one data centre and use Traffic Manager to automatically direct traffic between the two. This works with most kinds of web hosting you can do in Azure. This means that if the websites in one data centre fails, requests will automatically be served by the other data centre, usually within 1 minute. The main costs are in paying for the extra server(s) and the added complexity in every deployment.&lt;br /&gt;&lt;br /&gt;&lt;h3&gt;Database failover&lt;/h3&gt;Azure offers hot failover for both Azure SQL and CosmosDB, the two main databases. With this failover, Azure will dynamically fail over to a secondary data centre in case of a failure and/or serve requests from both data centres. The mechanisms used by SQL Azure and CosmosDB are fundamentally different and will require different approaches.&lt;br /&gt;&lt;br /&gt;&amp;nbsp;As a general rule, you have to pay for two copies of your database and you may have to use a more expensive service tier.&lt;br /&gt;&lt;br /&gt;In the case of CosmosDB, it may be required to consider consistency levels and the system may need to be adapted to deal with this.&lt;br /&gt;&lt;br /&gt;&lt;h3&gt;Storage failover&lt;/h3&gt;It is common to store files and certain other types of data in Azure Storage. By default, data is backed up to another data centre (though this can be disabled when not required). However, Azure is in control of enabling access to the backup in case of a failure and the SLA is up to 48 hours. In many cases, this is acceptable as the loss of access to historical files may be considered a service degradation rather than a failure.&lt;br /&gt;&lt;br /&gt;Where required, Azure do provide ways to have direct access to a read-only copy in the secondary data centre. This can be utilised to build a very high level of resilience, but it requires explicit programming in your software to take advantage of this.&lt;br /&gt;&lt;br /&gt;&lt;h3&gt;Queue failover&lt;/h3&gt;In an effort to increase resilience and scalability, it is common to use queues in systems; Rather than do something straight away, the system will put a message on a queue and a background job will then process this. This design has many benefits, including automatic retrying, resilience to external systems being down and significant scale benefits as sudden peaks in demand just causes queues to get longer for a little while.&lt;br /&gt;This does, however, mean that the queues can be a single point of failure; if the queue service fails, you can no longer enqueue messages.&lt;br /&gt;&lt;br /&gt;From NewOrbit’s many years of working with Azure, it is clear that Microsoft are very aware of the crucial importance queues play in many systems and they have worked very hard to make them extremely resilient; Despite very extensive usage, NewOrbit has never experienced a failure with “storage queues” and has only experienced an issue with “service bus queues” on a single occasion in 2013.&lt;br /&gt;&lt;br /&gt;It is possible to implement failover for queues and NewOrbit has done that before. There are different approaches that can be taken and Service Bus Queues have some native support for failover, though it does require programming to take full advantage of it.&lt;br /&gt;&lt;br /&gt;&lt;h3&gt;Other items&lt;/h3&gt;There are many other items that can be used in Azure, including virtual machines. For most of these items, a bespoke failover strategy is required to achieve hot failover.&lt;br /&gt;&lt;br /&gt;&lt;h4&gt;More SLA details&lt;/h4&gt;The individual SLAs for all Azure services can be found at https://azure.microsoft.com/en-gb/support/legal/sla/&lt;br /&gt;If you need to report on your overall SLA, it is important to understand how to combine them. If you have, say, an Azure App Service with 99.95% SLA and an Azure SQL database with a 99.99% SLA then the overall SLA for both to be up is (99.95% x 99.99%) = 99.94%. This obviously compounds with more components.&lt;br /&gt;&lt;br /&gt;On the other hand, adding a hot failover App Service in another data centre using Traffic Manager means you now have a better than 99.95% expected SLA for the App Service component. However, calculating the actual SLA is not practical due to the presence of “systemic risk”; There is one risk of a single data centre going down and a separate risk of a worldwide outage of Azure App Services.&lt;br /&gt;&lt;br /&gt;&lt;h4&gt;Help?&lt;/h4&gt;&lt;div&gt;If you have a quick question, ping me on &lt;a href=&quot;https://www.twitter.com/flytzen&quot; target=&quot;_blank&quot;&gt;twitter&lt;/a&gt;.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;If you want more extensive advice and guidance, my company &lt;a href=&quot;http://www.neworbit.co.uk/&quot; target=&quot;_blank&quot;&gt;NewOrbit&lt;/a&gt; offers help to other companies who are moving to Azure. We have been building systems on Azure since 2011 and are a Microsoft Cloud Gold Partner.&lt;/div&gt;&lt;br /&gt;</content><author><name>Frans Lytzen</name></author><category term="Azure" /><summary type="html">Azure provides a highly resilient hosting platform, with significant built-in redundancy, but it can be hard to understand what resilience you get automatically and what you might have to set up yourself.</summary></entry><entry><title type="html">Combine documents with other data in Azure Search</title><link href="https://www.lytzen.name/2017/01/30/combine-documents-with-other-data-in.html" rel="alternate" type="text/html" title="Combine documents with other data in Azure Search" /><published>2017-01-30T13:07:00+00:00</published><updated>2017-01-30T13:07:00+00:00</updated><id>https://www.lytzen.name/2017/01/30/combine-documents-with-other-data-in</id><content type="html" xml:base="https://www.lytzen.name/2017/01/30/combine-documents-with-other-data-in.html">Azure Search has built-in support for indexing documents in blob storage, which works great. But what if you want to &lt;i&gt;combine&lt;/i&gt;&amp;nbsp;the documents with other data, for example if you are building a recruitment system and want to search on, say, location and CV content at the same time?&lt;br /&gt;&lt;br /&gt;&lt;b&gt;TL;DR; &lt;/b&gt;Indexers in Azure Search uses a &quot;create or update&quot; approach; as long as your different indexers use the same &lt;i&gt;id, &lt;/i&gt;they can all write data to the same document in the index.&lt;br /&gt;&lt;br /&gt;&lt;h2&gt;Terminology in Azure Search&lt;/h2&gt;&lt;div&gt;&lt;i&gt;(deliberately simplified and made Azure Search specific)&lt;/i&gt;&lt;br /&gt;&lt;i&gt;&lt;br /&gt;&lt;/i&gt;&lt;/div&gt;&lt;div&gt;An &lt;b&gt;index&lt;/b&gt;&amp;nbsp;in Azure Search is more like a table in an RDBMS or a collection in a document-based database. It has a specific &quot;schema&quot; (the index definition) that specifies the structure of the &lt;b&gt;documents&lt;/b&gt;&lt;i&gt;&amp;nbsp;&lt;/i&gt;stored in the index.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;A &lt;b&gt;document&lt;/b&gt;&amp;nbsp;is a single entity in Azure Search, think &quot;record&quot; or (indeed) document in a document database. It has an &lt;i&gt;id&lt;/i&gt;&amp;nbsp;that is unique within that index.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;An &lt;b&gt;indexer&lt;/b&gt;&amp;nbsp;is something that takes data from a source, like Blob storage, SQL or something else, re-formats it and writes it to documents in an index in Azure Search.&lt;br /&gt;&lt;br /&gt;A &lt;b&gt;data source&lt;/b&gt;&amp;nbsp;is a definition in Azure Search of somewhere that an &lt;b&gt;indexer&lt;/b&gt;&amp;nbsp;can read data from. It's sort of like a connection string.&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;&lt;h2&gt;Scenario&lt;/h2&gt;&lt;div&gt;The specific scenario I have is the need to build a database of candidates for a recruitment system. I want to be able to search the contents of CVs, but I also want to be able to filter and use faceted search on various bits of data about the candidate themselves, such as location, qualification and so on.&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;Azure Search provides built-in functionality to read the contents from a range of files stored in Azure Blob Storage, so that sounded pretty ideal; just upload the CVs to blob storage, point Azure search to it and be done. Of course, that would not combine it with the other candidate data. Initially I thought about several options;&lt;br /&gt;&lt;ul&gt;&lt;li&gt;I could extract the content of the files myself and write code to combine it with candidate data in code. But extracting content out of files is not a trivial problem.&lt;/li&gt;&lt;li&gt;I could let Azure Search index the files, then write code to extract the content back out of Azure Search and write it back to another index. But that seemed like a very long-winded solution.&lt;/li&gt;&lt;li&gt;I asked Microsoft if I could somehow access the content-extraction feature directly. But you can't.&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;&lt;br /&gt;&lt;h2&gt;The solution&lt;/h2&gt;&lt;div&gt;&lt;a href=&quot;https://twitter.com/liamca&quot;&gt;Liam Cavanagh&lt;/a&gt;&amp;nbsp;gave me the outline solution with this statement;&lt;/div&gt;&lt;blockquote class=&quot;tr_bq&quot;&gt;Leverage Blob Storage with Data Store (such as Azure SQL or DocumentDB):&amp;nbsp; If you store your files in Azure Blob storage, and your structured data in something like Azure SQL or DocumentDB, you can use both the Azure Search Blob Indexer as well as the Azure Search SQL or DocumentDB Indexer together.&amp;nbsp; &lt;b&gt;The only trick is to make sure that the unique key that is defined for each document matches between Blob and the structured data store&lt;/b&gt;.&amp;nbsp; For example, if you choose to use the Base64 Encoded filename as the key, then you would need to make sure that the matching content in your structured datastore also contains this value.&amp;nbsp; That way the Indexer which does something called MergeOrUpdate, will first take a row from say the Blob Storage and insert it as a new row in Azure Search and then it will bring in the data from the structured data store and update the existing row with the new fields.&lt;/blockquote&gt;&lt;br /&gt;&lt;div&gt;With a bit more help from Liam I put together the sample solution outlined below. I should also mention that in this solution I am using the built-in tools in Azure Search to read data directly from SQL and Blob storage. If you wanted to use code to manually write to the index instead of letting Azure Search read from SQL then you can do that too, of course. Or read from some of the other supported data sources, or combine it all or...&lt;/div&gt;&lt;br /&gt;&lt;h2&gt;Implementation&lt;/h2&gt;&lt;div&gt;You'll need to create an Azure Search account and whatever data sources you want to use. You can do a number of the steps directly in the Azure Portal, though not all of them. For the ones you can't I will show the appropriate JSON payload you need to send. It is ridiculously easy with something like &lt;a href=&quot;https://www.getpostman.com/&quot;&gt;Postman&lt;/a&gt;. You just need to make sure you add two headers to your requests;&lt;/div&gt;&lt;div&gt;&lt;ul&gt;&lt;li&gt;Content-Type : application/json&lt;/li&gt;&lt;li&gt;api-key : [an admin key for your Azure Search instance]&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;&lt;div style=&quot;height: 480px; margin: 10px; position: relative; width: 640px;&quot;&gt;In summary, this is what we are going to build:&lt;br /&gt;&lt;iframe allowfullscreen=&quot;&quot; frameborder=&quot;0&quot; id=&quot;wGZtguhjzZug&quot; src=&quot;https://www.lucidchart.com/documents/embeddedchart/ce4e64ec-6570-48a9-a412-756e445e9d84&quot; style=&quot;height: 480px; width: 640px;&quot;&gt;&lt;/iframe&gt;&lt;br /&gt;&lt;/div&gt;&lt;br /&gt;&lt;br /&gt;&lt;h4&gt;Create the Index&lt;/h4&gt;&lt;div&gt;You can create a new index directly in the Azure Portal, which is what I did. It has a very nice editor that makes it quite easy. I ended up with something looking like this;&lt;br /&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://2.bp.blogspot.com/-yV3arlG8Z-c/WI41d2CtLnI/AAAAAAAAKSo/Rjj-6L0M0sQvH4EyVoTqMYFKIp0NBMNUACLcB/s1600/CandidatesIndex.PNG&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;268&quot; src=&quot;https://2.bp.blogspot.com/-yV3arlG8Z-c/WI41d2CtLnI/AAAAAAAAKSo/Rjj-6L0M0sQvH4EyVoTqMYFKIp0NBMNUACLcB/s640/CandidatesIndex.PNG&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;Please note the &quot;&lt;b&gt;content&lt;/b&gt;&quot; field; When Azure Search indexes files, it will place the content of those files in the &lt;b&gt;content&lt;/b&gt;&amp;nbsp;field. &lt;b&gt;Id&lt;/b&gt;&amp;nbsp;is there by default and we need that, so leave it alone, but the remaining fields can be whatever you like. I put a &lt;b&gt;name&lt;/b&gt; in there for the Candidate Name and the last two fields are some fields I added so I could experiment with faceted search.&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;&lt;h4&gt;Create the data sources&lt;/h4&gt;&lt;div&gt;Next we need to tell Azure Search where it can get the data - we need to create the Data Sources.&lt;/div&gt;&lt;div&gt;You can't create a Data Source on it's own in the Portal; there is an option to &quot;import data&quot; but that combines setting up the data source with the index and the indexer so we can't use it for our purposes.&lt;br /&gt;Time to start posting JSON (see above).&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;POST these to https://&lt;i&gt;yoursearchservice&lt;/i&gt;.search.windows.net/datasources?api-version=2016-09-01 and replace the XXX with valid connection strings.&lt;br /&gt;&lt;pre class=&quot;prettyprint&quot;&gt;{&lt;br /&gt;    &quot;name&quot; : &quot;blobcvs&quot;,&lt;br /&gt;    &quot;type&quot; : &quot;azureblob&quot;,&lt;br /&gt;    &quot;credentials&quot; : { &quot;connectionString&quot; : &quot;XXX&quot; },&lt;br /&gt;    &quot;container&quot; : { &quot;name&quot; : &quot;cvs&quot; }&lt;br /&gt;}&lt;br /&gt;&lt;br /&gt;{&lt;br /&gt;    &quot;name&quot; : &quot;candidates&quot;,&lt;br /&gt;    &quot;type&quot; : &quot;azuresql&quot;,&lt;br /&gt;    &quot;credentials&quot; : { &quot;connectionString&quot; : &quot;XXX&quot; },&lt;br /&gt;    &quot;container&quot; : { &quot;name&quot; : &quot;Candidates&quot; }&lt;br /&gt;} &lt;br /&gt;&lt;/pre&gt;This tells Azure Search how to access your data.&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;&lt;h4&gt;Create the indexers&lt;/h4&gt;&lt;div&gt;POST these to&amp;nbsp;https://&lt;i&gt;yoursearchservice&lt;/i&gt;.search.windows.net/indexers?api-version=2016-09-01&lt;br /&gt;&lt;pre class=&quot;prettyprint&quot;&gt;{&lt;br /&gt;    &quot;name&quot; : &quot;candidateindexer&quot;,&lt;br /&gt;    &quot;dataSourceName&quot; : &quot;candidates&quot;,&lt;br /&gt;    &quot;targetIndexName&quot; : &quot;candidates&quot;,&lt;br /&gt;    &quot;fieldMappings&quot; : [ { &quot;sourceFieldName&quot; : &quot;Thingiemajics&quot;, &lt;br /&gt;                          &quot;mappingFunction&quot; : { &quot;name&quot; : &quot;jsonArrayToStringCollection&quot; } &lt;br /&gt;                      } ]&lt;br /&gt;}&lt;br /&gt;&lt;/pre&gt;This tells Azure Search to take the data in the SQL database specified in the SQL data source and create a document in Azure Search for each row. Azure Search will &lt;i&gt;automatically &lt;/i&gt;match fields with the same names; I've got an Id field as well as Name, Type and Thingiemajics columns in SQL. The only one that is a bit special is Thingiemajics; I'm storing an array of tag values in that field in SQL in the format [&quot;red&quot;, &quot;white&quot;, &quot;blue&quot;] and putting that mapping function in there tells Azure Search to make them individual tags that can be filtered individually. See &lt;a href=&quot;https://docs.microsoft.com/en-us/azure/search/search-indexer-field-mappings#jsonArrayToStringCollectionFunction&quot;&gt;the docs&lt;/a&gt;&amp;nbsp;for more details.&lt;br /&gt;&lt;br /&gt;Before I create the indexer for the &lt;i&gt;files&lt;/i&gt;, let me just take a little detour. If you remember, the original statement was that for this to work, the two different indexers need to use the &lt;i&gt;same&lt;/i&gt;&amp;nbsp;id for data about the same candidate. The SQL indexer in my example uses the database ID of the candidate and we need to ensure that when Azure Search indexes the CV for a candidate it returns the same index. By &lt;i&gt;default&lt;/i&gt;&amp;nbsp;Azure Search will use the filename, which is obviously no good in this situation. The way I solved this was to add a custom meta data property to the blob when I uploaded it to Azure Blob Storage, something like this:&lt;br /&gt;&lt;br /&gt;&lt;pre class=&quot;prettyprint&quot;&gt;using (var fileStream = System.IO.File.OpenRead(file))&lt;br /&gt;{&lt;br /&gt;   await blob.UploadFromStreamAsync(fileStream);&lt;br /&gt;}&lt;br /&gt;blob.Metadata.Add(&quot;mykey&quot;, identifier);&lt;br /&gt;await blob.SetMetadataAsync();&lt;br /&gt;&lt;/pre&gt;Here I have called it &quot;mykey&quot;, but it could be called anything.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;On to the indexer, which is created with this:&lt;br /&gt;&lt;pre class=&quot;prettyprint&quot;&gt;{&lt;br /&gt;    &quot;name&quot; : &quot;cvindexer&quot;,&lt;br /&gt;    &quot;dataSourceName&quot; : &quot;blobcvs&quot;,&lt;br /&gt;    &quot;targetIndexName&quot; : &quot;candidates&quot;,&lt;br /&gt;    &quot;fieldMappings&quot; : [ { &quot;sourceFieldName&quot; : &quot;mykey&quot;, &quot;targetFieldName&quot; : &quot;id&quot; } ],&lt;br /&gt;    &quot;parameters&quot; : { &quot;configuration&quot; : { &quot;failOnUnsupportedContentType&quot; : false } }&lt;br /&gt;}&lt;br /&gt;&lt;/pre&gt;The most important thing here is that I tell Azure Search to take the metadata property &quot;mykey&quot; and map it to the &quot;id&quot; field in my index. That's all that is required to ensure the contents of the CV ends up in the same search document as the other Candidate information.&lt;/div&gt;&lt;br /&gt;&lt;h2&gt;Notes&lt;/h2&gt;&lt;div&gt;In my solution here I added an ID property to the blob meta data. I could, of course, have gone the other way instead and added the blob reference to the SQL data and then remapped that reference to &quot;id&quot; in the SQL indexer. Both approaches are equally valid, both have pros and cons, including the need to manually maintain uniqueness of the ID property and the need to base64 encode filenames.&lt;br /&gt;&lt;br /&gt;You should also note that this works because the different datasources do not have any of the same field names; if the two indexers both return the same field, one will win and delete the data from the other one.&lt;/div&gt;</content><author><name>Frans Lytzen</name></author><category term="Azure" /><summary type="html">Azure Search has built-in support for indexing documents in blob storage, which works great. But what if you want to combine&amp;nbsp;the documents with other data, for example if you are building a recruitment system and want to search on, say, location and CV content at the same time?TL;DR; Indexers in Azure Search uses a &quot;create or update&quot; approach; as long as your different indexers use the same id, they can all write data to the same document in the index.Terminology in Azure Search(deliberately simplified and made Azure Search specific)An index&amp;nbsp;in Azure Search is more like a table in an RDBMS or a collection in a document-based database. It has a specific &quot;schema&quot; (the index definition) that specifies the structure of the documents&amp;nbsp;stored in the index.A document&amp;nbsp;is a single entity in Azure Search, think &quot;record&quot; or (indeed) document in a document database. It has an id&amp;nbsp;that is unique within that index.An indexer&amp;nbsp;is something that takes data from a source, like Blob storage, SQL or something else, re-formats it and writes it to documents in an index in Azure Search.A data source&amp;nbsp;is a definition in Azure Search of somewhere that an indexer&amp;nbsp;can read data from. It's sort of like a connection string.ScenarioThe specific scenario I have is the need to build a database of candidates for a recruitment system. I want to be able to search the contents of CVs, but I also want to be able to filter and use faceted search on various bits of data about the candidate themselves, such as location, qualification and so on.Azure Search provides built-in functionality to read the contents from a range of files stored in Azure Blob Storage, so that sounded pretty ideal; just upload the CVs to blob storage, point Azure search to it and be done. Of course, that would not combine it with the other candidate data. Initially I thought about several options;I could extract the content of the files myself and write code to combine it with candidate data in code. But extracting content out of files is not a trivial problem.I could let Azure Search index the files, then write code to extract the content back out of Azure Search and write it back to another index. But that seemed like a very long-winded solution.I asked Microsoft if I could somehow access the content-extraction feature directly. But you can't.The solutionLiam Cavanagh&amp;nbsp;gave me the outline solution with this statement;Leverage Blob Storage with Data Store (such as Azure SQL or DocumentDB):&amp;nbsp; If you store your files in Azure Blob storage, and your structured data in something like Azure SQL or DocumentDB, you can use both the Azure Search Blob Indexer as well as the Azure Search SQL or DocumentDB Indexer together.&amp;nbsp; The only trick is to make sure that the unique key that is defined for each document matches between Blob and the structured data store.&amp;nbsp; For example, if you choose to use the Base64 Encoded filename as the key, then you would need to make sure that the matching content in your structured datastore also contains this value.&amp;nbsp; That way the Indexer which does something called MergeOrUpdate, will first take a row from say the Blob Storage and insert it as a new row in Azure Search and then it will bring in the data from the structured data store and update the existing row with the new fields.With a bit more help from Liam I put together the sample solution outlined below. I should also mention that in this solution I am using the built-in tools in Azure Search to read data directly from SQL and Blob storage. If you wanted to use code to manually write to the index instead of letting Azure Search read from SQL then you can do that too, of course. Or read from some of the other supported data sources, or combine it all or...ImplementationYou'll need to create an Azure Search account and whatever data sources you want to use. You can do a number of the steps directly in the Azure Portal, though not all of them. For the ones you can't I will show the appropriate JSON payload you need to send. It is ridiculously easy with something like Postman. You just need to make sure you add two headers to your requests;Content-Type : application/jsonapi-key : [an admin key for your Azure Search instance]In summary, this is what we are going to build:Create the IndexYou can create a new index directly in the Azure Portal, which is what I did. It has a very nice editor that makes it quite easy. I ended up with something looking like this;Please note the &quot;content&quot; field; When Azure Search indexes files, it will place the content of those files in the content&amp;nbsp;field. Id&amp;nbsp;is there by default and we need that, so leave it alone, but the remaining fields can be whatever you like. I put a name in there for the Candidate Name and the last two fields are some fields I added so I could experiment with faceted search.Create the data sourcesNext we need to tell Azure Search where it can get the data - we need to create the Data Sources.You can't create a Data Source on it's own in the Portal; there is an option to &quot;import data&quot; but that combines setting up the data source with the index and the indexer so we can't use it for our purposes.Time to start posting JSON (see above).POST these to https://yoursearchservice.search.windows.net/datasources?api-version=2016-09-01 and replace the XXX with valid connection strings.{ &quot;name&quot; : &quot;blobcvs&quot;, &quot;type&quot; : &quot;azureblob&quot;, &quot;credentials&quot; : { &quot;connectionString&quot; : &quot;XXX&quot; }, &quot;container&quot; : { &quot;name&quot; : &quot;cvs&quot; }}{ &quot;name&quot; : &quot;candidates&quot;, &quot;type&quot; : &quot;azuresql&quot;, &quot;credentials&quot; : { &quot;connectionString&quot; : &quot;XXX&quot; }, &quot;container&quot; : { &quot;name&quot; : &quot;Candidates&quot; }} This tells Azure Search how to access your data.Create the indexersPOST these to&amp;nbsp;https://yoursearchservice.search.windows.net/indexers?api-version=2016-09-01{ &quot;name&quot; : &quot;candidateindexer&quot;, &quot;dataSourceName&quot; : &quot;candidates&quot;, &quot;targetIndexName&quot; : &quot;candidates&quot;, &quot;fieldMappings&quot; : [ { &quot;sourceFieldName&quot; : &quot;Thingiemajics&quot;, &quot;mappingFunction&quot; : { &quot;name&quot; : &quot;jsonArrayToStringCollection&quot; } } ]}This tells Azure Search to take the data in the SQL database specified in the SQL data source and create a document in Azure Search for each row. Azure Search will automatically match fields with the same names; I've got an Id field as well as Name, Type and Thingiemajics columns in SQL. The only one that is a bit special is Thingiemajics; I'm storing an array of tag values in that field in SQL in the format [&quot;red&quot;, &quot;white&quot;, &quot;blue&quot;] and putting that mapping function in there tells Azure Search to make them individual tags that can be filtered individually. See the docs&amp;nbsp;for more details.Before I create the indexer for the files, let me just take a little detour. If you remember, the original statement was that for this to work, the two different indexers need to use the same&amp;nbsp;id for data about the same candidate. The SQL indexer in my example uses the database ID of the candidate and we need to ensure that when Azure Search indexes the CV for a candidate it returns the same index. By default&amp;nbsp;Azure Search will use the filename, which is obviously no good in this situation. The way I solved this was to add a custom meta data property to the blob when I uploaded it to Azure Blob Storage, something like this:using (var fileStream = System.IO.File.OpenRead(file)){ await blob.UploadFromStreamAsync(fileStream);}blob.Metadata.Add(&quot;mykey&quot;, identifier);await blob.SetMetadataAsync();Here I have called it &quot;mykey&quot;, but it could be called anything.On to the indexer, which is created with this:{ &quot;name&quot; : &quot;cvindexer&quot;, &quot;dataSourceName&quot; : &quot;blobcvs&quot;, &quot;targetIndexName&quot; : &quot;candidates&quot;, &quot;fieldMappings&quot; : [ { &quot;sourceFieldName&quot; : &quot;mykey&quot;, &quot;targetFieldName&quot; : &quot;id&quot; } ], &quot;parameters&quot; : { &quot;configuration&quot; : { &quot;failOnUnsupportedContentType&quot; : false } }}The most important thing here is that I tell Azure Search to take the metadata property &quot;mykey&quot; and map it to the &quot;id&quot; field in my index. That's all that is required to ensure the contents of the CV ends up in the same search document as the other Candidate information.NotesIn my solution here I added an ID property to the blob meta data. I could, of course, have gone the other way instead and added the blob reference to the SQL data and then remapped that reference to &quot;id&quot; in the SQL indexer. Both approaches are equally valid, both have pros and cons, including the need to manually maintain uniqueness of the ID property and the need to base64 encode filenames.You should also note that this works because the different datasources do not have any of the same field names; if the two indexers both return the same field, one will win and delete the data from the other one.</summary></entry><entry><title type="html">How many ways can I host a web app in Azure?</title><link href="https://www.lytzen.name/2017/01/06/how-many-ways-can-i-host-web-app-in.html" rel="alternate" type="text/html" title="How many ways can I host a web app in Azure?" /><published>2017-01-06T20:17:00+00:00</published><updated>2017-01-06T20:17:00+00:00</updated><id>https://www.lytzen.name/2017/01/06/how-many-ways-can-i-host-web-app-in</id><content type="html" xml:base="https://www.lytzen.name/2017/01/06/how-many-ways-can-i-host-web-app-in.html">I got talking to a colleague about how many ways there are to host web apps in Azure and even managed to surprise myself by just how many ways I could think of. It inspired me to compile this list, which is just off the top of my head. I'm sure there are more - if you can think of other ways, please leave a comment.&lt;br /&gt;&lt;br /&gt;For the purposes of this, I am defining a web app as something that has a user-facing UI &lt;i&gt;and &lt;/i&gt;some server-side functionality. I have listed a few additional options at the bottom if you only need one of those things.&lt;br /&gt;&lt;br /&gt;I put this list together real quick like and the description of each service is just a very quick summary, mainly just from memory so please do not view this as a definitive or highly accurate document; It's mainly just a fun exercise. That said, do please point out any factual errors in the comments so I can correct them.&lt;br /&gt;&lt;br /&gt;&lt;h3&gt;&lt;a href=&quot;https://azure.microsoft.com/en-gb/services/app-service/web/&quot;&gt;Web Apps&lt;/a&gt;&lt;/h3&gt;&lt;div&gt;This is the default option and probably what you should choose if in doubt. It's getting a lot of love from Microsoft at the moment and is constantly getting new features. You get fail-over and auto-scaling by default, plans from free to expensive and it is available in both Windows and Linux flavours (in preview). It's very easy to get started with and supports pretty much anything from static pages to complex deployment processes, staging slots and even a built-in basic CD pipeline.&lt;br /&gt;The main thing to be aware of is that you don't get admin access to the server, so if you need to, say, customise which SSL protocols are available or you need to install fonts, you are out of luck. From experience, it is very rare that you need this, though.&amp;nbsp;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;h3&gt;&lt;a href=&quot;https://azure.microsoft.com/en-gb/services/cloud-services/&quot;&gt;Cloud Services&lt;/a&gt;&lt;/h3&gt;&lt;div&gt;This is the original Platform-as-a-Service option in Azure. It doesn't get much love these days, but I am still fond of it for those few situations where I need more than Web Apps can give me. Essentially, you provide Azure with an application package and Azure will take care of deploying that to one or more servers for you. You do get full admin access to the servers so you can do what you like - as long as you script it as part of your package. Patching, fail-over, load-balancing, auto-scaling, health monitoring etc is all taken care of for you.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;h3&gt;&lt;a href=&quot;https://azure.microsoft.com/en-gb/services/functions/&quot;&gt;Functions&lt;/a&gt;&lt;/h3&gt;&lt;div&gt;This isn't really meant for doing a proper web app, but you &lt;i&gt;can &lt;/i&gt;write a collection of functions that acts as an API or return some HTML etc, so you could certainly do it if you really wanted. Understand me right, though, Functions are brilliant for what they are meant to do, even if building whole web apps isn't it. That said, if you just need a couple of simple APIs to support a front-end app then it is certainly something you should consider.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;h3&gt;&lt;a href=&quot;https://azure.microsoft.com/en-gb/services/service-fabric/&quot;&gt;Service Fabric&lt;/a&gt;&lt;/h3&gt;&lt;div&gt;Not very well known, but Azure provides a pretty advanced micro-services framework that you can use for building sophisticated, large-scale applications. It supports both a service model and a basic Actor model out of the box. It's got a pretty high base cost relative to other Azure services due to the minimum number of nodes you have to use, but if you have a need for lots of scale then you should definitely look at this. Azure uses it to power a lot of their own architecture, including both SQL Azure and DocumentDb.&amp;nbsp;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;h3&gt;&lt;a href=&quot;https://azure.microsoft.com/en-gb/services/virtual-machines/&quot;&gt;Virtual Machine&lt;/a&gt;&lt;/h3&gt;&lt;div&gt;You can, of course, deploy good old-fashined virtual machines and run your web app on them. You are then responsible for patching and some level of maintenance yourself. You can define images so Azure can do auto scaling for you as well. I personally try to avoid using VMs as far as I can as I don't like to be responsible for patching and maintenance etc - yet, I still have about 40 of them :).&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;h3&gt;&lt;a href=&quot;https://azure.microsoft.com/en-gb/services/container-service/&quot;&gt;Container Services / Docker&lt;/a&gt;&lt;/h3&gt;&lt;div&gt;If you are one of those cool kids who like Docker, you are in luck. Azure has native support for Docker and support DC/OS, Docker Swarm and Kubernetes out of the box.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;So, how many was that? Six, I think, though I probably shouldn't count the Functions one :)&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;h4&gt;Related&lt;/h4&gt;&lt;div&gt;As if all the above wasn't enough, there are a couple of other technologies that can also be used to deal with web sites or APIs.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;h3&gt;&lt;a href=&quot;https://azure.microsoft.com/en-gb/services/storage/blobs/&quot;&gt;Blob Storage&lt;/a&gt; / &lt;a href=&quot;https://azure.microsoft.com/en-gb/services/cdn/&quot;&gt;CDN&lt;/a&gt;&lt;/h3&gt;&lt;div&gt;Blob storage is Azure's file storage system (okay, it's more than that, but let that suffice for now). You can share a container publicly and put html, js, css and whatever other files you like in it. I quite often use it for static assets, though in theory you could host a whole website in there. Azure CDN &lt;i&gt;can&lt;/i&gt; sit on top of blob storage and gives you geo-replication of the files. You can map custom domains to blob storage as well. Of course, this is all static so I couldn't quite include it in my list above.&amp;nbsp;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;h3&gt;&lt;a href=&quot;https://azure.microsoft.com/en-gb/services/traffic-manager/&quot;&gt;Traffic Manager&lt;/a&gt;&lt;/h3&gt;&lt;div&gt;Traffic Manager doesn't host anything, but it is worth understanding where it fits in. Pretty much all of the above options include a load balancer (automatic, you don't really have to worry about it) and fail-over within a data centre. If you need fail-over between data centres or want to route traffic to servers close to your users, you can use Traffic Manager. It works at the DNS level and is used to direct user requests for a given domain name to the nearest available data centre.&amp;nbsp;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;h3&gt;&lt;a href=&quot;https://azure.microsoft.com/en-gb/services/api-management/&quot;&gt;API Management&lt;/a&gt;&lt;/h3&gt;&lt;div&gt;Not strongly related to web app hosting, but I just thought I'd mention it; Basically, if you develop an API and you want to give external users access to it you probably want to do things like controlling who can use it (maybe so you can bill them), rate limiting, authentication and so on. Azure API management deals with all that as a service that just sits in front of your naked API so you don't have to write it all yourself.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;h4&gt;Takeaway&lt;/h4&gt;&lt;div&gt;When I started this list, I really didn't expect it would be this long. I think it's great there is so much choice and I know that each option has it's own set of strengths and weaknesses. We use most of the technologies listed here on different projects and for different reasons, and I'm very happy that I can choose. At the same time, I think it is probably quite tough for someone new to Azure to even get started on figuring out which of the many options are right for their particular scenario. And I have only dealt with web app hosting here, not the multitude of other things you can do.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;I just meant for this post to be a fun little exercise but, having written it, I should mention that at &lt;a href=&quot;http://www.neworbit.co.uk/&quot;&gt;NewOrbit&lt;/a&gt; we have recently started helping other companies move to the cloud and sharing our years of Azure experience with them.&amp;nbsp;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;If you have any questions or want to talk more, &lt;a href=&quot;https://twitter.com/flytzen&quot;&gt;ping me on twitter&lt;/a&gt;&amp;nbsp;or add a comment below.&lt;/div&gt;</content><author><name>Frans Lytzen</name></author><category term="Azure" /><summary type="html">I got talking to a colleague about how many ways there are to host web apps in Azure and even managed to surprise myself by just how many ways I could think of. It inspired me to compile this list, which is just off the top of my head. I'm sure there are more - if you can think of other ways, please leave a comment.For the purposes of this, I am defining a web app as something that has a user-facing UI and some server-side functionality. I have listed a few additional options at the bottom if you only need one of those things.I put this list together real quick like and the description of each service is just a very quick summary, mainly just from memory so please do not view this as a definitive or highly accurate document; It's mainly just a fun exercise. That said, do please point out any factual errors in the comments so I can correct them.Web AppsThis is the default option and probably what you should choose if in doubt. It's getting a lot of love from Microsoft at the moment and is constantly getting new features. You get fail-over and auto-scaling by default, plans from free to expensive and it is available in both Windows and Linux flavours (in preview). It's very easy to get started with and supports pretty much anything from static pages to complex deployment processes, staging slots and even a built-in basic CD pipeline.The main thing to be aware of is that you don't get admin access to the server, so if you need to, say, customise which SSL protocols are available or you need to install fonts, you are out of luck. From experience, it is very rare that you need this, though.&amp;nbsp;Cloud ServicesThis is the original Platform-as-a-Service option in Azure. It doesn't get much love these days, but I am still fond of it for those few situations where I need more than Web Apps can give me. Essentially, you provide Azure with an application package and Azure will take care of deploying that to one or more servers for you. You do get full admin access to the servers so you can do what you like - as long as you script it as part of your package. Patching, fail-over, load-balancing, auto-scaling, health monitoring etc is all taken care of for you.FunctionsThis isn't really meant for doing a proper web app, but you can write a collection of functions that acts as an API or return some HTML etc, so you could certainly do it if you really wanted. Understand me right, though, Functions are brilliant for what they are meant to do, even if building whole web apps isn't it. That said, if you just need a couple of simple APIs to support a front-end app then it is certainly something you should consider.Service FabricNot very well known, but Azure provides a pretty advanced micro-services framework that you can use for building sophisticated, large-scale applications. It supports both a service model and a basic Actor model out of the box. It's got a pretty high base cost relative to other Azure services due to the minimum number of nodes you have to use, but if you have a need for lots of scale then you should definitely look at this. Azure uses it to power a lot of their own architecture, including both SQL Azure and DocumentDb.&amp;nbsp;Virtual MachineYou can, of course, deploy good old-fashined virtual machines and run your web app on them. You are then responsible for patching and some level of maintenance yourself. You can define images so Azure can do auto scaling for you as well. I personally try to avoid using VMs as far as I can as I don't like to be responsible for patching and maintenance etc - yet, I still have about 40 of them :).Container Services / DockerIf you are one of those cool kids who like Docker, you are in luck. Azure has native support for Docker and support DC/OS, Docker Swarm and Kubernetes out of the box.So, how many was that? Six, I think, though I probably shouldn't count the Functions one :)RelatedAs if all the above wasn't enough, there are a couple of other technologies that can also be used to deal with web sites or APIs.Blob Storage / CDNBlob storage is Azure's file storage system (okay, it's more than that, but let that suffice for now). You can share a container publicly and put html, js, css and whatever other files you like in it. I quite often use it for static assets, though in theory you could host a whole website in there. Azure CDN can sit on top of blob storage and gives you geo-replication of the files. You can map custom domains to blob storage as well. Of course, this is all static so I couldn't quite include it in my list above.&amp;nbsp;Traffic ManagerTraffic Manager doesn't host anything, but it is worth understanding where it fits in. Pretty much all of the above options include a load balancer (automatic, you don't really have to worry about it) and fail-over within a data centre. If you need fail-over between data centres or want to route traffic to servers close to your users, you can use Traffic Manager. It works at the DNS level and is used to direct user requests for a given domain name to the nearest available data centre.&amp;nbsp;API ManagementNot strongly related to web app hosting, but I just thought I'd mention it; Basically, if you develop an API and you want to give external users access to it you probably want to do things like controlling who can use it (maybe so you can bill them), rate limiting, authentication and so on. Azure API management deals with all that as a service that just sits in front of your naked API so you don't have to write it all yourself.TakeawayWhen I started this list, I really didn't expect it would be this long. I think it's great there is so much choice and I know that each option has it's own set of strengths and weaknesses. We use most of the technologies listed here on different projects and for different reasons, and I'm very happy that I can choose. At the same time, I think it is probably quite tough for someone new to Azure to even get started on figuring out which of the many options are right for their particular scenario. And I have only dealt with web app hosting here, not the multitude of other things you can do.I just meant for this post to be a fun little exercise but, having written it, I should mention that at NewOrbit we have recently started helping other companies move to the cloud and sharing our years of Azure experience with them.&amp;nbsp;If you have any questions or want to talk more, ping me on twitter&amp;nbsp;or add a comment below.</summary></entry><entry><title type="html">Find docs with no PartitionKey in Azure DocumentDb</title><link href="https://www.lytzen.name/2016/12/06/find-docs-with-no-partitionkey-in-azure.html" rel="alternate" type="text/html" title="Find docs with no PartitionKey in Azure DocumentDb" /><published>2016-12-06T21:31:00+00:00</published><updated>2016-12-06T21:31:00+00:00</updated><id>https://www.lytzen.name/2016/12/06/find-docs-with-no-partitionkey-in-azure</id><content type="html" xml:base="https://www.lytzen.name/2016/12/06/find-docs-with-no-partitionkey-in-azure.html">When you are using Partitioned Collections in Azure DocumentDb you need to specify a &lt;i&gt;Partition Key&lt;/i&gt; on each Document. At least, I thought you did. But, it turns out that you actually &lt;i&gt;can&lt;/i&gt;&amp;nbsp;save documents without a partitionkey. But if you do, you'll have a hard time retrieving or deleting them - until you meet Undefined.Value.&lt;br /&gt;&lt;i&gt;Note: This post is written for C#, I am not sure about the equivalent for other languages.&lt;/i&gt;&lt;br /&gt;&lt;i&gt;&lt;br /&gt;&lt;/i&gt;&lt;br /&gt;&lt;h4&gt;Details&lt;/h4&gt;&lt;div&gt;If you create a Partitioned Collection in Azure DocumentDb you probably think that every document you save must have a partitionkey property and probably also that it must have a value. In this post I am dealing with the situation where you don't have a partition key property on your document at all, not the situation where you have one but you set it to null or an empty string.&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;For example, if you have created your collection with code similar to this;&lt;br /&gt;&lt;pre class=&quot;prettyprint&quot;&gt;var docCollection = new DocumentCollection()&lt;br /&gt;{&lt;br /&gt;   Id = this.collectionName&lt;br /&gt;};&lt;br /&gt;docCollection.PartitionKey.Paths.Add(&quot;/partitionKey&quot;);&lt;br /&gt;await docClient.CreateDocumentCollectionAsync(&lt;br /&gt;                    UriFactory.CreateDatabaseUri(this.dbName), &lt;br /&gt;                    docCollection);&lt;/pre&gt;and you then try to save an instance of a class that looks like this:&lt;br /&gt;&lt;pre class=&quot;prettyprint&quot;&gt;public class MyItem&lt;br /&gt;{&lt;br /&gt;    [JsonProperty(&quot;id&quot;)]&lt;br /&gt;    public string Id { get; set; }&lt;br /&gt;&lt;br /&gt;    public string SomeValue { get; set; }&lt;br /&gt;}&lt;br /&gt;&lt;/pre&gt;then you may expect to get an error. But, in fact, it will save just fine as I found out to my detriment after a major refactoring.&lt;/div&gt;&lt;br /&gt;&lt;div&gt;Now that you have that item in the database you will find it hard to retrieve it and even harder to delete it - until you meet your new friend &lt;b&gt;Undefined.Value&lt;/b&gt;.&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;h3&gt;How to read the document:&lt;/h3&gt;&lt;pre class=&quot;prettyprint&quot;&gt;MyItem item = (dynamic)client.ReadDocumentAsync(&lt;br /&gt;                           UriFactory.CreateDocumentUri(DbName, CollectionName, id),&lt;br /&gt;                           new RequestOptions() {&lt;br /&gt;                             PartitionKey = new PartitionKey(Undefined.Value)&lt;br /&gt;                           })&lt;br /&gt;                         .Result.Resource;&lt;br /&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div&gt;&lt;h3&gt;How to delete the document:&lt;/h3&gt;&lt;pre class=&quot;prettyprint&quot;&gt;client.DeleteDocumentAsync(&lt;br /&gt;          UriFactory.CreateDocumentUri(DbName, CollectionName, id), &lt;br /&gt;          new RequestOptions() { PartitionKey = new PartitionKey(Undefined.Value) });&lt;br /&gt;&lt;/pre&gt;&lt;/div&gt;Many thanks to &lt;a href=&quot;https://twitter.com/arkramac&quot;&gt;Aravind Ramachandran&lt;/a&gt; for telling me about Undefined.Value.&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;</content><author><name>Frans Lytzen</name></author><category term="CosmosDB" /><category term="Azure" /><summary type="html">When you are using Partitioned Collections in Azure DocumentDb you need to specify a Partition Key on each Document. At least, I thought you did. But, it turns out that you actually can&amp;nbsp;save documents without a partitionkey. But if you do, you'll have a hard time retrieving or deleting them - until you meet Undefined.Value.Note: This post is written for C#, I am not sure about the equivalent for other languages.DetailsIf you create a Partitioned Collection in Azure DocumentDb you probably think that every document you save must have a partitionkey property and probably also that it must have a value. In this post I am dealing with the situation where you don't have a partition key property on your document at all, not the situation where you have one but you set it to null or an empty string.For example, if you have created your collection with code similar to this;var docCollection = new DocumentCollection(){ Id = this.collectionName};docCollection.PartitionKey.Paths.Add(&quot;/partitionKey&quot;);await docClient.CreateDocumentCollectionAsync( UriFactory.CreateDatabaseUri(this.dbName), docCollection);and you then try to save an instance of a class that looks like this:public class MyItem{ [JsonProperty(&quot;id&quot;)] public string Id { get; set; } public string SomeValue { get; set; }}then you may expect to get an error. But, in fact, it will save just fine as I found out to my detriment after a major refactoring.Now that you have that item in the database you will find it hard to retrieve it and even harder to delete it - until you meet your new friend Undefined.Value.How to read the document:MyItem item = (dynamic)client.ReadDocumentAsync( UriFactory.CreateDocumentUri(DbName, CollectionName, id), new RequestOptions() { PartitionKey = new PartitionKey(Undefined.Value) }) .Result.Resource;How to delete the document:client.DeleteDocumentAsync( UriFactory.CreateDocumentUri(DbName, CollectionName, id), new RequestOptions() { PartitionKey = new PartitionKey(Undefined.Value) });Many thanks to Aravind Ramachandran for telling me about Undefined.Value.</summary></entry><entry><title type="html">Find Documents with missing properties in Azure DocumentDb with the .Net SDK</title><link href="https://www.lytzen.name/2016/09/29/find-documents-with-missing-properties.html" rel="alternate" type="text/html" title="Find Documents with missing properties in Azure DocumentDb with the .Net SDK" /><published>2016-09-29T21:06:00+01:00</published><updated>2016-09-29T21:06:00+01:00</updated><id>https://www.lytzen.name/2016/09/29/find-documents-with-missing-properties</id><content type="html" xml:base="https://www.lytzen.name/2016/09/29/find-documents-with-missing-properties.html">Azure DocumentDb stores documents as JSON. One of the effects of this is that sometimes you may end up with documents in the database that have missing properties and it can be quite tricky to search for them with the .Net SDK. This blog post has an approach to doing it - and quite simply too.&lt;br /&gt;&lt;br /&gt;&lt;h3&gt;Background&lt;/h3&gt;&lt;div&gt;Most commonly, you would encounter the issue of the missing property when you add a new property to an existing class in your .Net code. There is no automatic method of adding this new property to all the existing entries in the database, short of re-saving them all.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;Alternatively, you can explicitly configure Json.Net to not store properties that have null values like this:&lt;/div&gt;&lt;pre class=&quot;prettyprint&quot;&gt;JsonConvert.DefaultSettings = () =&amp;gt; &lt;br /&gt;  new JsonSerializerSettings&lt;br /&gt;       {&lt;br /&gt;          NullValueHandling = NullValueHandling.Ignore&lt;br /&gt;       };&lt;br /&gt;&lt;/pre&gt;&lt;br /&gt;&lt;div&gt;You can use this configuration option to test the behaviour I am describing here or to save space in the database.&lt;br /&gt;&lt;br /&gt;For example, imagine you have a class called MyItem looking like this:&lt;/div&gt;&lt;pre class=&quot;prettyprint&quot;&gt;public class MyItem&lt;br /&gt;{&lt;br /&gt;    [JsonProperty(&quot;id&quot;)]&lt;br /&gt;    public string Id { get; set; }&lt;br /&gt;    public string SomeValue { get; set; }&lt;br /&gt;}&lt;br /&gt;&lt;/pre&gt;&lt;br /&gt;&lt;div&gt;If you have an item where SomeValue is null, by default that will be serialised and stored in DocumentDb like this:&lt;/div&gt;&lt;pre class=&quot;prettyprint&quot;&gt;{&lt;br /&gt;  &quot;id&quot; : &quot;1&quot;,&lt;br /&gt;  &quot;SomeValue&quot; : null&lt;br /&gt;}&lt;br /&gt;&lt;/pre&gt;&lt;br /&gt;&lt;div&gt;However, if you configure Json.Net to not store null values (or the SomeValue field was added to your .Net code after you stored this item in DocumentDb) it will look like this in the database:&lt;/div&gt;&lt;pre class=&quot;prettyprint&quot;&gt;{&lt;br /&gt;  &quot;id&quot; : &quot;1&quot;,&lt;br /&gt;}&lt;br /&gt;&lt;/pre&gt;&lt;br /&gt;&lt;br /&gt;&lt;h3&gt;Selecting missing properties with SQL&lt;/h3&gt;&lt;div&gt;According to&amp;nbsp;&lt;a href=&quot;https://azure.microsoft.com/en-gb/documentation/articles/documentdb-sql-query/&quot;&gt;the documentation&lt;/a&gt;&amp;nbsp;you can use SQL to select missing properties like this:&lt;/div&gt;&lt;pre class=&quot;prettyprint&quot;&gt;SELECT f.lastName ?? f.surname AS familyName&lt;br /&gt;FROM Families f&lt;br /&gt;&lt;/pre&gt;&lt;div&gt;You can then extrapolate from that example to, for example, select items etc.&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;h3&gt;Finding items with null &lt;i&gt;or&lt;/i&gt;&amp;nbsp;missing with the .Net SDK&lt;/h3&gt;&lt;/div&gt;&lt;div&gt;Imagine you have added the SomeValue property to the MyItem class after you had already saved some items. Further, sometimes you store a null in the SomeValue property. Or you have configured Json.Net to ignore null values. And now you want to find all the items where SomeValue is either missing or null.&lt;/div&gt;&lt;div&gt;You might try this:&amp;nbsp;&lt;/div&gt;&lt;pre class=&quot;prettyprint&quot;&gt;var query1 = client.CreateDocumentQuery&amp;lt;MyItem&amp;gt;(collectionUrl)&lt;br /&gt;              .Where(i =&amp;gt; i.SomeValue == null);&lt;br /&gt;&lt;/pre&gt;&lt;div&gt;But you will find that this will not actually return any results - at least, it won't return any documents where SomeValue is not present at all. However, this odd-looking statement will work:&lt;/div&gt;&lt;pre class=&quot;prettyprint&quot;&gt;var query2 = client.CreateDocumentQuery&amp;lt;MyItem&amp;gt;(collectionUrl)&lt;br /&gt;              .Where(i =&amp;gt; (i.SomeValue ?? null) == null);&lt;br /&gt;&lt;/pre&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;It is using the null coalescor to make DocumentDb return a null value for the property for the property if it does not exist, which we can then compare to null.&lt;br /&gt;&lt;br /&gt;I have tested this with version 1.6, 1.8 and 1.10 of the SDK, but I would advise you to put an integration test in your code if you are going to rely on it, just in case the behaviour changes in the future. You'll probably also want to put a comment wherever you use this syntax as R# is quite keen to tell you that you should get rid of the &quot;?? null&quot; part.&lt;br /&gt;Finally, I have not done any performance testing on this, but I suspect DocumentDb won't be able to use any indexes to execute this query; it will probably have to evaluate each document in a scan so use with caution.&lt;/div&gt;&lt;br /&gt;&lt;h3&gt;A full sample&lt;/h3&gt;&lt;div&gt;If you want to try this out, there is a full example here:&amp;nbsp;&lt;a href=&quot;https://gist.github.com/flytzen/b8dbcb2079a66b1c8a67005ef5198052&quot;&gt;https://gist.github.com/flytzen/b8dbcb2079a66b1c8a67005ef5198052&lt;/a&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;</content><author><name>Frans Lytzen</name></author><category term="CosmosDB" /><category term="Azure" /><summary type="html">Azure DocumentDb stores documents as JSON. One of the effects of this is that sometimes you may end up with documents in the database that have missing properties and it can be quite tricky to search for them with the .Net SDK. This blog post has an approach to doing it - and quite simply too.BackgroundMost commonly, you would encounter the issue of the missing property when you add a new property to an existing class in your .Net code. There is no automatic method of adding this new property to all the existing entries in the database, short of re-saving them all.Alternatively, you can explicitly configure Json.Net to not store properties that have null values like this:JsonConvert.DefaultSettings = () =&amp;gt; new JsonSerializerSettings { NullValueHandling = NullValueHandling.Ignore };You can use this configuration option to test the behaviour I am describing here or to save space in the database.For example, imagine you have a class called MyItem looking like this:public class MyItem{ [JsonProperty(&quot;id&quot;)] public string Id { get; set; } public string SomeValue { get; set; }}If you have an item where SomeValue is null, by default that will be serialised and stored in DocumentDb like this:{ &quot;id&quot; : &quot;1&quot;, &quot;SomeValue&quot; : null}However, if you configure Json.Net to not store null values (or the SomeValue field was added to your .Net code after you stored this item in DocumentDb) it will look like this in the database:{ &quot;id&quot; : &quot;1&quot;,}Selecting missing properties with SQLAccording to&amp;nbsp;the documentation&amp;nbsp;you can use SQL to select missing properties like this:SELECT f.lastName ?? f.surname AS familyNameFROM Families fYou can then extrapolate from that example to, for example, select items etc.Finding items with null or&amp;nbsp;missing with the .Net SDKImagine you have added the SomeValue property to the MyItem class after you had already saved some items. Further, sometimes you store a null in the SomeValue property. Or you have configured Json.Net to ignore null values. And now you want to find all the items where SomeValue is either missing or null.You might try this:&amp;nbsp;var query1 = client.CreateDocumentQuery&amp;lt;MyItem&amp;gt;(collectionUrl) .Where(i =&amp;gt; i.SomeValue == null);But you will find that this will not actually return any results - at least, it won't return any documents where SomeValue is not present at all. However, this odd-looking statement will work:var query2 = client.CreateDocumentQuery&amp;lt;MyItem&amp;gt;(collectionUrl) .Where(i =&amp;gt; (i.SomeValue ?? null) == null);It is using the null coalescor to make DocumentDb return a null value for the property for the property if it does not exist, which we can then compare to null.I have tested this with version 1.6, 1.8 and 1.10 of the SDK, but I would advise you to put an integration test in your code if you are going to rely on it, just in case the behaviour changes in the future. You'll probably also want to put a comment wherever you use this syntax as R# is quite keen to tell you that you should get rid of the &quot;?? null&quot; part.Finally, I have not done any performance testing on this, but I suspect DocumentDb won't be able to use any indexes to execute this query; it will probably have to evaluate each document in a scan so use with caution.A full sampleIf you want to try this out, there is a full example here:&amp;nbsp;https://gist.github.com/flytzen/b8dbcb2079a66b1c8a67005ef5198052</summary></entry></feed>