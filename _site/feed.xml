<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.7.3">Jekyll</generator><link href="https://www.lytzen.name/feed.xml" rel="self" type="application/atom+xml" /><link href="https://www.lytzen.name/" rel="alternate" type="text/html" /><updated>2018-06-27T12:39:04+01:00</updated><id>https://www.lytzen.name/</id><title type="html">Frans’ Randomness</title><subtitle>My very infrequent thoughts on the world of software development</subtitle><entry><title type="html">GDPR for Software and how Azure can help</title><link href="https://www.lytzen.name/2018/06/25/GDPR-for-software.html" rel="alternate" type="text/html" title="GDPR for Software and how Azure can help" /><published>2018-06-25T00:00:00+01:00</published><updated>2018-06-25T00:00:00+01:00</updated><id>https://www.lytzen.name/2018/06/25/GDPR-for-software</id><content type="html" xml:base="https://www.lytzen.name/2018/06/25/GDPR-for-software.html">&lt;p&gt;When you develop software, whether for other people or for running your business, there are many things you have to consider which are quite different from the things you have to do to make your business GDPR compliant.&lt;br /&gt;
In the spring of 2018 I recorded a video with Microsoft that gives an overview of this as well looks at some of the ways in which Azure can help.&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/KlZhAG351Bs&quot; frameborder=&quot;0&quot; allow=&quot;autoplay; encrypted-media&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;For some more specific tips on how to lock down your software on Azure, including Managed Identity and KeyVault, see &lt;a href=&quot;/2018/04/29/securing-your-webapp-in-azure.html&quot;&gt;Securing your web app in Azure&lt;/a&gt;`&lt;/p&gt;</content><author><name>Frans Lytzen</name></author><category term="Azure" /><category term="GDPR" /><category term="Security" /><summary type="html">A video recording I made with Microsoft about how GDPR applies to Software development and how Azure can help.</summary></entry><entry><title type="html">CosmosDB token has wrong time</title><link href="https://www.lytzen.name/2018/05/08/Comos-db-token-failure.html" rel="alternate" type="text/html" title="CosmosDB token has wrong time" /><published>2018-05-08T12:00:00+01:00</published><updated>2018-05-08T12:00:00+01:00</updated><id>https://www.lytzen.name/2018/05/08/Comos-db-token-failure</id><content type="html" xml:base="https://www.lytzen.name/2018/05/08/Comos-db-token-failure.html">&lt;p&gt;Out of the blue, we started receiving the following error in our web app that uses CosmosDB:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;The authorization token is not valid at the current time. Please create another token and retry (token start time: Tue, 08 May 2018 09:17:33 GMT, token expiry time: Tue, 08 May 2018 09:32:33 GMT, current server time: Tue, 08 May 2018 09:11:00 GMT
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The important things to note here are &lt;code class=&quot;highlighter-rouge&quot;&gt;token start time: Tue, 08 May 2018 09:17:33 GMT&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;current server time: Tue, 08 May 2018 09:11:00 GMT&lt;/code&gt;.&lt;br /&gt;
The token has a start time &lt;em&gt;in the future&lt;/em&gt;. As a consequence, any request to CosmosDB made with this token fails.&lt;/p&gt;

&lt;p&gt;It took us a while to diagnose this because we just assumed that &lt;code class=&quot;highlighter-rouge&quot;&gt;current server time&lt;/code&gt; referred to the time on the Web server, but it actually refers to the time on the CosmosDB server.&lt;br /&gt;
We could see that the &lt;code class=&quot;highlighter-rouge&quot;&gt;current server time&lt;/code&gt; was correct, in that it matched the known correct time on other servers.&lt;/p&gt;

&lt;p&gt;What it boils down to is that the time on our webserver was wrong, specifically it was in the future, and the .Net client SDK was dutifully creating access tokens that started in the future.&lt;/p&gt;

&lt;p&gt;If you are running your app on your own server or a VM, you can stop reading now and go reset the time on your server and all will be well. What was really weird for us was that we are running this in a Web App in Azure App Services. They are supposed to be time synchronised and it simply shouldn’t be possible for their clock to drift. Except it did. We triangulated the problem by running the same code from another location and eventually from a web app deployed to a different App Service Plan.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;If you are experiencing this problem in Azure Web App, the TL;DR; is to scale your site &lt;em&gt;up&lt;/em&gt;, for example from an S1 to an S2 and then down again later. Details as to why are below.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The thing to remember here is that an App Service Plan is what encapsulates the underlying hardware that actually hosts the App Services. We had an App Service Plan with a single node in it and &lt;em&gt;all&lt;/em&gt; Web Apps deployed to this App Service Plan failed with the same error. Which is logical when you think about it, but easy to forget in the heat of battle.&lt;br /&gt;
We confirmed the issue by going to the “Advanced Tools” and, using the Powershell console, ran &lt;code class=&quot;highlighter-rouge&quot;&gt;Get-Date&lt;/code&gt;, which showed us that the web-server time was wrong.&lt;/p&gt;

&lt;p&gt;In our case we had just a single node in the App Service Plan. If you had multiple nodes and only one had time-drifted, you’d probably see the error intermittently and the &lt;code class=&quot;highlighter-rouge&quot;&gt;Get-Date&lt;/code&gt; would just return the time from whatever server the console happened to be running on. If you suspect this situation, it may be worthwhile scaling your app service plan down to a single node, testing it and scaling out again.&lt;/p&gt;

&lt;p&gt;What we needed was for Azure to kill our faulty node so we would automatically roll on to another node with the correct time. Unfortunately there are no tools to do this (and you wouldn’t expect to have to). However, there is a way you can do it and it was literally the process of writing this post that made me think about it, so I’ve just stopped writing to go fix the problem; Simply scale your site &lt;em&gt;up&lt;/em&gt; and then down again. For example, if you are on an S1, scale to an S2 as this will force Azure to deploy your site on a new underlying machine. You can scale back down again when you are happy.&lt;/p&gt;</content><author><name>Frans Lytzen</name></author><category term="Azure," /><category term="CosmosDB" /><summary type="html">CosmosDB error &quot;The authorization token is not valid at the current time&quot; and how to fix it.</summary></entry><entry><title type="html">Securing your web app in Azure</title><link href="https://www.lytzen.name/2018/04/29/securing-your-webapp-in-azure.html" rel="alternate" type="text/html" title="Securing your web app in Azure" /><published>2018-04-29T21:40:00+01:00</published><updated>2018-04-29T21:40:00+01:00</updated><id>https://www.lytzen.name/2018/04/29/securing-your-webapp-in-azure</id><content type="html" xml:base="https://www.lytzen.name/2018/04/29/securing-your-webapp-in-azure.html">&lt;p&gt;So you have deployed your web app to Azure. Now, how do you go about making it secure?
I gave a talk on this topic at  &lt;a href=&quot;https://www.meetup.com/dotnetoxford/&quot;&gt;DotNet Oxford&lt;/a&gt; on 24 April 2018 and recorded it. You can view the video below.&lt;/p&gt;

&lt;p&gt;The video runs through a scenario using an ASP.Net Web App hosted on Azure App Service and covers a number of features you can use to improve your security - as well as a number of features that are not available for App Services.&lt;/p&gt;

&lt;p&gt;The talk covers a lot of ground in an hour and everything is kept at a high level, but is nonetheless heavy on examples and code.&lt;br /&gt;
Watching the video myself, I realised I say “Okay” and “So” way, way too much. Sorry…&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;a href=&quot;https://twitter.com/jamesw0rld&quot;&gt;James World&lt;/a&gt; made this nice sketch note of the talk, reproduced with permission.&lt;br /&gt;
&lt;img src=&quot;https://www.lytzen.name/assets/2018-04-29-security-talk-sketch-note.jpg&quot; alt=&quot;Sketch note&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/2tR5sEk46v0&quot; frameborder=&quot;0&quot; allow=&quot;autoplay; encrypted-media&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;The source code is on &lt;a href=&quot;https://github.com/flytzen/SecurityTalk&quot;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;some-key-timings&quot;&gt;Some key timings&lt;/h2&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Use SSL&lt;/td&gt;
      &lt;td&gt;11:03&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Virus scanning&lt;/td&gt;
      &lt;td&gt;20:01&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;WAF&lt;/td&gt;
      &lt;td&gt;21:00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Vnet&lt;/td&gt;
      &lt;td&gt;23:20&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Azure Key Vault&lt;/td&gt;
      &lt;td&gt;26:10&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Managed Service Identity&lt;/td&gt;
      &lt;td&gt;27:50&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Use Key Vault and managed identify to store secrets&lt;/td&gt;
      &lt;td&gt;29:55&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;ASP.Net Core configuration with Key Vault&lt;/td&gt;
      &lt;td&gt;31:55&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Connect to Azure SQL with Managed Identity (or not)&lt;/td&gt;
      &lt;td&gt;36:27&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Encrypt data at rest&lt;/td&gt;
      &lt;td&gt;38:00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Require secure transport&lt;/td&gt;
      &lt;td&gt;40:30&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;SQL Always Encrypted&lt;/td&gt;
      &lt;td&gt;41:40&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Storage client-side encryption (not shown)&lt;/td&gt;
      &lt;td&gt;52:00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Use Azure AD to access Azure&lt;/td&gt;
      &lt;td&gt;53:25&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Use Azure AD to access Azure SQL&lt;/td&gt;
      &lt;td&gt;54:05&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Supporting Security tools in Azure&lt;/td&gt;
      &lt;td&gt;56:50&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Detection&lt;/td&gt;
      &lt;td&gt;57:45&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;</content><author><name>Frans Lytzen</name></author><category term="Azure" /><category term="Security" /><summary type="html">A video overview of some of the Azure technologies that you can use to better protect your web applications in Azure - all depending on your required security level, of course.</summary></entry><entry><title type="html">Azure Failover and Resilience</title><link href="https://www.lytzen.name/2017/06/29/azure-failover-and-resilience.html" rel="alternate" type="text/html" title="Azure Failover and Resilience" /><published>2017-06-29T17:54:00+01:00</published><updated>2017-06-29T17:54:00+01:00</updated><id>https://www.lytzen.name/2017/06/29/azure-failover-and-resilience</id><content type="html" xml:base="https://www.lytzen.name/2017/06/29/azure-failover-and-resilience.html">Azure provides a highly resilient hosting platform, with significant built-in redundancy within a data centre, as well as the presence of more than 30 data centres across the world.&lt;br /&gt;&lt;br /&gt;When first coming to Azure, it can be hard to understand what resilience you get automatically and what you might have to set up yourself.&lt;br /&gt;&lt;br /&gt;This post provides a high-level overview of the principles. It is intended as an introduction to help you ask the right questions.&lt;br /&gt;&lt;br /&gt;The usual starting point for a system is to host it in a single data centre. Azure is highly resilient even within a single data centre, but even so, all the data is continually backed up to a secondary data centre.&lt;br /&gt;&lt;br /&gt;In the case of a complete failure of a data centre, the data can be restored to another data centre. This is not the same as automatic failover to another data centre; In order to get the data restored in the other data centre and get the system back up and running, you will have to do it yourself; Azure will (for the most part) not do this for you. How much work depends on how much preparatory work has been done and is primarily a business decision based on risk and cost.&lt;br /&gt;&lt;br /&gt;Any conversation about failover is complicated by the fact that a system consists of different components, which can fail independently, which have different probability and impact and which require different failover strategies.&lt;br /&gt;&lt;br /&gt;Before going into the details, it is important to understand that even the most basic setup in Azure has a very high level of resilience with each individual component typically having a guaranteed uptime of 99.95% or more. At the same time, data is continually backed up to a secondary data centre. In other words, even the most basic Azure setup has a level of resilience that is difficult and expensive to achieve with on-premise hosting.&lt;br /&gt;&lt;br /&gt;In this post “failover” will refer to failing over between data centres.&lt;br /&gt;&lt;br /&gt;&lt;h4&gt;Resilience in a single data centre&lt;/h4&gt;&lt;div&gt;Azure Data Centres are built in a modular way, meaning that each data centre can be thought of as many smaller data centres built next to each other. This means that your data and system will be physically spread over different parts of the data centre, in turn meaning that even if an entire part of the data centre fails, you are unlikely to notice.&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;Physically, all Azure data centres have multiple redundant power grid connections, multiple redundant internet connections, redundant backup generators, batteries and so on and so forth.&lt;br /&gt;&lt;br /&gt;As a general rule, any data you save in Azure, in databases, to disk or to other types of storage, is written to three different locations inside that one data centre and a single copy is written to another remote data centre as a backup. For example the London data centre backs up to Cardiff and the Amsterdam data centre backs up to Dublin etc.&lt;br /&gt;&lt;br /&gt;Azure App Service has some built-in resilience so even with only a single compute node in your app hosting plan, you are pretty well protected from outages. With Cloud Services, you must ensure that you have at least two instances running at all times to ensure resilience. With Virtual Machines – you are much more on your own, though there are a number of things you can configure, such as Availability Sets etc. As a &lt;i&gt;very&lt;/i&gt; general rule, to make reliability easier, avoid using VMs, use one of the managed options instead, when you can.&lt;br /&gt;&lt;br /&gt;When you want to be able to fail over to another data centre, there are several options available to you. I have grouped them here under “Cold”, “Warm” and “Hot”. These are just convenience labels and may not correlate to other people’s definitions.&lt;br /&gt;&lt;br /&gt;&lt;h4&gt;Cold&lt;/h4&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://4.bp.blogspot.com/-wV3fbqWMgeU/WVUssFhNeqI/AAAAAAAALuA/n5RZdmPeCqM4Jr6snqblsOxVj7fUfEyvQCLcBGAs/s1600/Failover%2BOptions%2B-%2BCold.png&quot; imageanchor=&quot;1&quot; style=&quot;clear: right; float: right; margin-bottom: 1em; margin-left: 1em;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;560&quot; data-original-width=&quot;1180&quot; height=&quot;188&quot; src=&quot;https://4.bp.blogspot.com/-wV3fbqWMgeU/WVUssFhNeqI/AAAAAAAALuA/n5RZdmPeCqM4Jr6snqblsOxVj7fUfEyvQCLcBGAs/s400/Failover%2BOptions%2B-%2BCold.png&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;div&gt;A Cold failover is what you get by default.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;Your data is automatically backed up to another data centre. In the case of a failure of the primary data centre, you can go to the other data centre, set up your systems again, deploy everything and restore your database. Of course, the more automated your deployment is, the easier this will be.&lt;br /&gt;&lt;br /&gt;You should be aware that while you can manually trigger a restore of SQL and CosmosDB databases, you cannot yourself trigger a “restore” of anything you put into Azure Storage. Microsoft has to do that by changing DNS entries and their SLA on that is up to 48 hours, last time I checked. There are things you can do to improve this, such as using read-access geo-redundant storage, but you will need to develop specifically to take advantage of that. Often, though, the data in Azure Storage is secondary to the main system and you may be able to live without that data for a day or two.&lt;br /&gt;&lt;br /&gt;The exact frequency of database backups depends on the chosen database but is generally four hours or less.&lt;br /&gt;&lt;br /&gt;&lt;h4&gt;Warm&lt;/h4&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://4.bp.blogspot.com/-gk1abFDNI6A/WVUtOZGJaeI/AAAAAAAALuE/gEM_835uULUHb8zijsbskKBwbgclBf2zgCLcBGAs/s1600/Failover%2BOptions%2B-%2BWarm.png&quot; imageanchor=&quot;1&quot; style=&quot;clear: right; float: right; margin-bottom: 1em; margin-left: 1em;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;560&quot; data-original-width=&quot;1180&quot; height=&quot;188&quot; src=&quot;https://4.bp.blogspot.com/-gk1abFDNI6A/WVUtOZGJaeI/AAAAAAAALuE/gEM_835uULUHb8zijsbskKBwbgclBf2zgCLcBGAs/s400/Failover%2BOptions%2B-%2BWarm.png&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/div&gt;A Warm failover allows faster fail-over to a secondary data centre, but still requires manual intervention.&lt;br /&gt;&lt;br /&gt;In order to reduce the time it takes to move to a secondary data centre, it is possible to prepare the infrastructure and have detailed plans in place. You can do this by configuring the whole system in the secondary data centre but not deploy anything to it; For many services you can define it but just not deploy anything to it. Similarly, you can deploy VMs and then de-allocate them etc. An alternative is to create an ARM template, which will allow you to quickly create a whole environment in Azure.&lt;br /&gt;&lt;br /&gt;You should also write plans and scripts so you can quickly restore the databases and direct traffic to the other data centre etc. It may also require periodic testing of the plans.&lt;br /&gt;&lt;br /&gt;Finally, you should make sure your DNS is set up with a short enough TTL that you can quickly move traffic to the new websites.&lt;br /&gt;&lt;br /&gt;Document storage, such as files, may in the Warm scenario still take up to 48 hours to be made available in the other data centre.&lt;br /&gt;&lt;br /&gt;&lt;h4&gt;Hot&lt;/h4&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-jR6utAG2ch0/WVUtUWoKCoI/AAAAAAAALuI/uwkuqU-cVuIUfrzgbYqs1QsXDfgPzYemgCLcBGAs/s1600/Failover%2BOptions%2B-%2BHot.png&quot; imageanchor=&quot;1&quot; style=&quot;clear: right; float: right; margin-bottom: 1em; margin-left: 1em;&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;980&quot; data-original-width=&quot;1180&quot; height=&quot;331&quot; src=&quot;https://1.bp.blogspot.com/-jR6utAG2ch0/WVUtUWoKCoI/AAAAAAAALuI/uwkuqU-cVuIUfrzgbYqs1QsXDfgPzYemgCLcBGAs/s400/Failover%2BOptions%2B-%2BHot.png&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&lt;/div&gt;A Hot failover will automatically fail over to a secondary data centre if the primary data centre fails, in whole or in part.&lt;br /&gt;&lt;br /&gt;In practice, there are many different components to a system and it usually makes sense to only have hot failover in place for some of the components. A bespoke cost/benefit exercise should be carried out where hot failover is desired.&lt;br /&gt;&lt;br /&gt;The primary things to consider;&lt;br /&gt;&lt;br /&gt;&lt;h3&gt;Web site failover&lt;/h3&gt;It is possible to deploy the front-end web servers &amp;nbsp;to more than one data centre and use Traffic Manager to automatically direct traffic between the two. This works with most kinds of web hosting you can do in Azure. This means that if the websites in one data centre fails, requests will automatically be served by the other data centre, usually within 1 minute. The main costs are in paying for the extra server(s) and the added complexity in every deployment.&lt;br /&gt;&lt;br /&gt;&lt;h3&gt;Database failover&lt;/h3&gt;Azure offers hot failover for both Azure SQL and CosmosDB, the two main databases. With this failover, Azure will dynamically fail over to a secondary data centre in case of a failure and/or serve requests from both data centres. The mechanisms used by SQL Azure and CosmosDB are fundamentally different and will require different approaches.&lt;br /&gt;&lt;br /&gt;&amp;nbsp;As a general rule, you have to pay for two copies of your database and you may have to use a more expensive service tier.&lt;br /&gt;&lt;br /&gt;In the case of CosmosDB, it may be required to consider consistency levels and the system may need to be adapted to deal with this.&lt;br /&gt;&lt;br /&gt;&lt;h3&gt;Storage failover&lt;/h3&gt;It is common to store files and certain other types of data in Azure Storage. By default, data is backed up to another data centre (though this can be disabled when not required). However, Azure is in control of enabling access to the backup in case of a failure and the SLA is up to 48 hours. In many cases, this is acceptable as the loss of access to historical files may be considered a service degradation rather than a failure.&lt;br /&gt;&lt;br /&gt;Where required, Azure do provide ways to have direct access to a read-only copy in the secondary data centre. This can be utilised to build a very high level of resilience, but it requires explicit programming in your software to take advantage of this.&lt;br /&gt;&lt;br /&gt;&lt;h3&gt;Queue failover&lt;/h3&gt;In an effort to increase resilience and scalability, it is common to use queues in systems; Rather than do something straight away, the system will put a message on a queue and a background job will then process this. This design has many benefits, including automatic retrying, resilience to external systems being down and significant scale benefits as sudden peaks in demand just causes queues to get longer for a little while.&lt;br /&gt;This does, however, mean that the queues can be a single point of failure; if the queue service fails, you can no longer enqueue messages.&lt;br /&gt;&lt;br /&gt;From NewOrbit’s many years of working with Azure, it is clear that Microsoft are very aware of the crucial importance queues play in many systems and they have worked very hard to make them extremely resilient; Despite very extensive usage, NewOrbit has never experienced a failure with “storage queues” and has only experienced an issue with “service bus queues” on a single occasion in 2013.&lt;br /&gt;&lt;br /&gt;It is possible to implement failover for queues and NewOrbit has done that before. There are different approaches that can be taken and Service Bus Queues have some native support for failover, though it does require programming to take full advantage of it.&lt;br /&gt;&lt;br /&gt;&lt;h3&gt;Other items&lt;/h3&gt;There are many other items that can be used in Azure, including virtual machines. For most of these items, a bespoke failover strategy is required to achieve hot failover.&lt;br /&gt;&lt;br /&gt;&lt;h4&gt;More SLA details&lt;/h4&gt;The individual SLAs for all Azure services can be found at https://azure.microsoft.com/en-gb/support/legal/sla/&lt;br /&gt;If you need to report on your overall SLA, it is important to understand how to combine them. If you have, say, an Azure App Service with 99.95% SLA and an Azure SQL database with a 99.99% SLA then the overall SLA for both to be up is (99.95% x 99.99%) = 99.94%. This obviously compounds with more components.&lt;br /&gt;&lt;br /&gt;On the other hand, adding a hot failover App Service in another data centre using Traffic Manager means you now have a better than 99.95% expected SLA for the App Service component. However, calculating the actual SLA is not practical due to the presence of “systemic risk”; There is one risk of a single data centre going down and a separate risk of a worldwide outage of Azure App Services.&lt;br /&gt;&lt;br /&gt;&lt;h4&gt;Help?&lt;/h4&gt;&lt;div&gt;If you have a quick question, ping me on &lt;a href=&quot;https://www.twitter.com/flytzen&quot; target=&quot;_blank&quot;&gt;twitter&lt;/a&gt;.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;If you want more extensive advice and guidance, my company &lt;a href=&quot;http://www.neworbit.co.uk/&quot; target=&quot;_blank&quot;&gt;NewOrbit&lt;/a&gt; offers help to other companies who are moving to Azure. We have been building systems on Azure since 2011 and are a Microsoft Cloud Gold Partner.&lt;/div&gt;&lt;br /&gt;</content><author><name>Frans Lytzen</name></author><category term="Azure" /><summary type="html">Azure provides a highly resilient hosting platform, with significant built-in redundancy, but it can be hard to understand what resilience you get automatically and what you might have to set up yourself.</summary></entry><entry><title type="html">Combine documents with other data in Azure Search</title><link href="https://www.lytzen.name/2017/01/30/combine-documents-with-other-data-in.html" rel="alternate" type="text/html" title="Combine documents with other data in Azure Search" /><published>2017-01-30T13:07:00+00:00</published><updated>2017-01-30T13:07:00+00:00</updated><id>https://www.lytzen.name/2017/01/30/combine-documents-with-other-data-in</id><content type="html" xml:base="https://www.lytzen.name/2017/01/30/combine-documents-with-other-data-in.html">Azure Search has built-in support for indexing documents in blob storage, which works great. But what if you want to &lt;i&gt;combine&lt;/i&gt;&amp;nbsp;the documents with other data, for example if you are building a recruitment system and want to search on, say, location and CV content at the same time?&lt;br /&gt;&lt;br /&gt;&lt;b&gt;TL;DR; &lt;/b&gt;Indexers in Azure Search uses a &quot;create or update&quot; approach; as long as your different indexers use the same &lt;i&gt;id, &lt;/i&gt;they can all write data to the same document in the index.&lt;br /&gt;&lt;br /&gt;&lt;h2&gt;Terminology in Azure Search&lt;/h2&gt;&lt;div&gt;&lt;i&gt;(deliberately simplified and made Azure Search specific)&lt;/i&gt;&lt;br /&gt;&lt;i&gt;&lt;br /&gt;&lt;/i&gt;&lt;/div&gt;&lt;div&gt;An &lt;b&gt;index&lt;/b&gt;&amp;nbsp;in Azure Search is more like a table in an RDBMS or a collection in a document-based database. It has a specific &quot;schema&quot; (the index definition) that specifies the structure of the &lt;b&gt;documents&lt;/b&gt;&lt;i&gt;&amp;nbsp;&lt;/i&gt;stored in the index.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;A &lt;b&gt;document&lt;/b&gt;&amp;nbsp;is a single entity in Azure Search, think &quot;record&quot; or (indeed) document in a document database. It has an &lt;i&gt;id&lt;/i&gt;&amp;nbsp;that is unique within that index.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;An &lt;b&gt;indexer&lt;/b&gt;&amp;nbsp;is something that takes data from a source, like Blob storage, SQL or something else, re-formats it and writes it to documents in an index in Azure Search.&lt;br /&gt;&lt;br /&gt;A &lt;b&gt;data source&lt;/b&gt;&amp;nbsp;is a definition in Azure Search of somewhere that an &lt;b&gt;indexer&lt;/b&gt;&amp;nbsp;can read data from. It's sort of like a connection string.&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;&lt;h2&gt;Scenario&lt;/h2&gt;&lt;div&gt;The specific scenario I have is the need to build a database of candidates for a recruitment system. I want to be able to search the contents of CVs, but I also want to be able to filter and use faceted search on various bits of data about the candidate themselves, such as location, qualification and so on.&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;Azure Search provides built-in functionality to read the contents from a range of files stored in Azure Blob Storage, so that sounded pretty ideal; just upload the CVs to blob storage, point Azure search to it and be done. Of course, that would not combine it with the other candidate data. Initially I thought about several options;&lt;br /&gt;&lt;ul&gt;&lt;li&gt;I could extract the content of the files myself and write code to combine it with candidate data in code. But extracting content out of files is not a trivial problem.&lt;/li&gt;&lt;li&gt;I could let Azure Search index the files, then write code to extract the content back out of Azure Search and write it back to another index. But that seemed like a very long-winded solution.&lt;/li&gt;&lt;li&gt;I asked Microsoft if I could somehow access the content-extraction feature directly. But you can't.&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;&lt;br /&gt;&lt;h2&gt;The solution&lt;/h2&gt;&lt;div&gt;&lt;a href=&quot;https://twitter.com/liamca&quot;&gt;Liam Cavanagh&lt;/a&gt;&amp;nbsp;gave me the outline solution with this statement;&lt;/div&gt;&lt;blockquote class=&quot;tr_bq&quot;&gt;Leverage Blob Storage with Data Store (such as Azure SQL or DocumentDB):&amp;nbsp; If you store your files in Azure Blob storage, and your structured data in something like Azure SQL or DocumentDB, you can use both the Azure Search Blob Indexer as well as the Azure Search SQL or DocumentDB Indexer together.&amp;nbsp; &lt;b&gt;The only trick is to make sure that the unique key that is defined for each document matches between Blob and the structured data store&lt;/b&gt;.&amp;nbsp; For example, if you choose to use the Base64 Encoded filename as the key, then you would need to make sure that the matching content in your structured datastore also contains this value.&amp;nbsp; That way the Indexer which does something called MergeOrUpdate, will first take a row from say the Blob Storage and insert it as a new row in Azure Search and then it will bring in the data from the structured data store and update the existing row with the new fields.&lt;/blockquote&gt;&lt;br /&gt;&lt;div&gt;With a bit more help from Liam I put together the sample solution outlined below. I should also mention that in this solution I am using the built-in tools in Azure Search to read data directly from SQL and Blob storage. If you wanted to use code to manually write to the index instead of letting Azure Search read from SQL then you can do that too, of course. Or read from some of the other supported data sources, or combine it all or...&lt;/div&gt;&lt;br /&gt;&lt;h2&gt;Implementation&lt;/h2&gt;&lt;div&gt;You'll need to create an Azure Search account and whatever data sources you want to use. You can do a number of the steps directly in the Azure Portal, though not all of them. For the ones you can't I will show the appropriate JSON payload you need to send. It is ridiculously easy with something like &lt;a href=&quot;https://www.getpostman.com/&quot;&gt;Postman&lt;/a&gt;. You just need to make sure you add two headers to your requests;&lt;/div&gt;&lt;div&gt;&lt;ul&gt;&lt;li&gt;Content-Type : application/json&lt;/li&gt;&lt;li&gt;api-key : [an admin key for your Azure Search instance]&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;&lt;div style=&quot;height: 480px; margin: 10px; position: relative; width: 640px;&quot;&gt;In summary, this is what we are going to build:&lt;br /&gt;&lt;iframe allowfullscreen=&quot;&quot; frameborder=&quot;0&quot; id=&quot;wGZtguhjzZug&quot; src=&quot;https://www.lucidchart.com/documents/embeddedchart/ce4e64ec-6570-48a9-a412-756e445e9d84&quot; style=&quot;height: 480px; width: 640px;&quot;&gt;&lt;/iframe&gt;&lt;br /&gt;&lt;/div&gt;&lt;br /&gt;&lt;br /&gt;&lt;h4&gt;Create the Index&lt;/h4&gt;&lt;div&gt;You can create a new index directly in the Azure Portal, which is what I did. It has a very nice editor that makes it quite easy. I ended up with something looking like this;&lt;br /&gt;&lt;div class=&quot;separator&quot; style=&quot;clear: both; text-align: center;&quot;&gt;&lt;a href=&quot;https://2.bp.blogspot.com/-yV3arlG8Z-c/WI41d2CtLnI/AAAAAAAAKSo/Rjj-6L0M0sQvH4EyVoTqMYFKIp0NBMNUACLcB/s1600/CandidatesIndex.PNG&quot; imageanchor=&quot;1&quot; style=&quot;margin-left: 1em; margin-right: 1em;&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;268&quot; src=&quot;https://2.bp.blogspot.com/-yV3arlG8Z-c/WI41d2CtLnI/AAAAAAAAKSo/Rjj-6L0M0sQvH4EyVoTqMYFKIp0NBMNUACLcB/s640/CandidatesIndex.PNG&quot; width=&quot;640&quot; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;Please note the &quot;&lt;b&gt;content&lt;/b&gt;&quot; field; When Azure Search indexes files, it will place the content of those files in the &lt;b&gt;content&lt;/b&gt;&amp;nbsp;field. &lt;b&gt;Id&lt;/b&gt;&amp;nbsp;is there by default and we need that, so leave it alone, but the remaining fields can be whatever you like. I put a &lt;b&gt;name&lt;/b&gt; in there for the Candidate Name and the last two fields are some fields I added so I could experiment with faceted search.&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;&lt;h4&gt;Create the data sources&lt;/h4&gt;&lt;div&gt;Next we need to tell Azure Search where it can get the data - we need to create the Data Sources.&lt;/div&gt;&lt;div&gt;You can't create a Data Source on it's own in the Portal; there is an option to &quot;import data&quot; but that combines setting up the data source with the index and the indexer so we can't use it for our purposes.&lt;br /&gt;Time to start posting JSON (see above).&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;POST these to https://&lt;i&gt;yoursearchservice&lt;/i&gt;.search.windows.net/datasources?api-version=2016-09-01 and replace the XXX with valid connection strings.&lt;br /&gt;&lt;pre class=&quot;prettyprint&quot;&gt;{&lt;br /&gt;    &quot;name&quot; : &quot;blobcvs&quot;,&lt;br /&gt;    &quot;type&quot; : &quot;azureblob&quot;,&lt;br /&gt;    &quot;credentials&quot; : { &quot;connectionString&quot; : &quot;XXX&quot; },&lt;br /&gt;    &quot;container&quot; : { &quot;name&quot; : &quot;cvs&quot; }&lt;br /&gt;}&lt;br /&gt;&lt;br /&gt;{&lt;br /&gt;    &quot;name&quot; : &quot;candidates&quot;,&lt;br /&gt;    &quot;type&quot; : &quot;azuresql&quot;,&lt;br /&gt;    &quot;credentials&quot; : { &quot;connectionString&quot; : &quot;XXX&quot; },&lt;br /&gt;    &quot;container&quot; : { &quot;name&quot; : &quot;Candidates&quot; }&lt;br /&gt;} &lt;br /&gt;&lt;/pre&gt;This tells Azure Search how to access your data.&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;&lt;h4&gt;Create the indexers&lt;/h4&gt;&lt;div&gt;POST these to&amp;nbsp;https://&lt;i&gt;yoursearchservice&lt;/i&gt;.search.windows.net/indexers?api-version=2016-09-01&lt;br /&gt;&lt;pre class=&quot;prettyprint&quot;&gt;{&lt;br /&gt;    &quot;name&quot; : &quot;candidateindexer&quot;,&lt;br /&gt;    &quot;dataSourceName&quot; : &quot;candidates&quot;,&lt;br /&gt;    &quot;targetIndexName&quot; : &quot;candidates&quot;,&lt;br /&gt;    &quot;fieldMappings&quot; : [ { &quot;sourceFieldName&quot; : &quot;Thingiemajics&quot;, &lt;br /&gt;                          &quot;mappingFunction&quot; : { &quot;name&quot; : &quot;jsonArrayToStringCollection&quot; } &lt;br /&gt;                      } ]&lt;br /&gt;}&lt;br /&gt;&lt;/pre&gt;This tells Azure Search to take the data in the SQL database specified in the SQL data source and create a document in Azure Search for each row. Azure Search will &lt;i&gt;automatically &lt;/i&gt;match fields with the same names; I've got an Id field as well as Name, Type and Thingiemajics columns in SQL. The only one that is a bit special is Thingiemajics; I'm storing an array of tag values in that field in SQL in the format [&quot;red&quot;, &quot;white&quot;, &quot;blue&quot;] and putting that mapping function in there tells Azure Search to make them individual tags that can be filtered individually. See &lt;a href=&quot;https://docs.microsoft.com/en-us/azure/search/search-indexer-field-mappings#jsonArrayToStringCollectionFunction&quot;&gt;the docs&lt;/a&gt;&amp;nbsp;for more details.&lt;br /&gt;&lt;br /&gt;Before I create the indexer for the &lt;i&gt;files&lt;/i&gt;, let me just take a little detour. If you remember, the original statement was that for this to work, the two different indexers need to use the &lt;i&gt;same&lt;/i&gt;&amp;nbsp;id for data about the same candidate. The SQL indexer in my example uses the database ID of the candidate and we need to ensure that when Azure Search indexes the CV for a candidate it returns the same index. By &lt;i&gt;default&lt;/i&gt;&amp;nbsp;Azure Search will use the filename, which is obviously no good in this situation. The way I solved this was to add a custom meta data property to the blob when I uploaded it to Azure Blob Storage, something like this:&lt;br /&gt;&lt;br /&gt;&lt;pre class=&quot;prettyprint&quot;&gt;using (var fileStream = System.IO.File.OpenRead(file))&lt;br /&gt;{&lt;br /&gt;   await blob.UploadFromStreamAsync(fileStream);&lt;br /&gt;}&lt;br /&gt;blob.Metadata.Add(&quot;mykey&quot;, identifier);&lt;br /&gt;await blob.SetMetadataAsync();&lt;br /&gt;&lt;/pre&gt;Here I have called it &quot;mykey&quot;, but it could be called anything.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;On to the indexer, which is created with this:&lt;br /&gt;&lt;pre class=&quot;prettyprint&quot;&gt;{&lt;br /&gt;    &quot;name&quot; : &quot;cvindexer&quot;,&lt;br /&gt;    &quot;dataSourceName&quot; : &quot;blobcvs&quot;,&lt;br /&gt;    &quot;targetIndexName&quot; : &quot;candidates&quot;,&lt;br /&gt;    &quot;fieldMappings&quot; : [ { &quot;sourceFieldName&quot; : &quot;mykey&quot;, &quot;targetFieldName&quot; : &quot;id&quot; } ],&lt;br /&gt;    &quot;parameters&quot; : { &quot;configuration&quot; : { &quot;failOnUnsupportedContentType&quot; : false } }&lt;br /&gt;}&lt;br /&gt;&lt;/pre&gt;The most important thing here is that I tell Azure Search to take the metadata property &quot;mykey&quot; and map it to the &quot;id&quot; field in my index. That's all that is required to ensure the contents of the CV ends up in the same search document as the other Candidate information.&lt;/div&gt;&lt;br /&gt;&lt;h2&gt;Notes&lt;/h2&gt;&lt;div&gt;In my solution here I added an ID property to the blob meta data. I could, of course, have gone the other way instead and added the blob reference to the SQL data and then remapped that reference to &quot;id&quot; in the SQL indexer. Both approaches are equally valid, both have pros and cons, including the need to manually maintain uniqueness of the ID property and the need to base64 encode filenames.&lt;br /&gt;&lt;br /&gt;You should also note that this works because the different datasources do not have any of the same field names; if the two indexers both return the same field, one will win and delete the data from the other one.&lt;/div&gt;</content><author><name>Frans Lytzen</name></author><category term="Azure" /><summary type="html">Azure Search has built-in support for indexing documents in blob storage, which works great. But what if you want to combine&amp;nbsp;the documents with other data, for example if you are building a recruitment system and want to search on, say, location and CV content at the same time?TL;DR; Indexers in Azure Search uses a &quot;create or update&quot; approach; as long as your different indexers use the same id, they can all write data to the same document in the index.Terminology in Azure Search(deliberately simplified and made Azure Search specific)An index&amp;nbsp;in Azure Search is more like a table in an RDBMS or a collection in a document-based database. It has a specific &quot;schema&quot; (the index definition) that specifies the structure of the documents&amp;nbsp;stored in the index.A document&amp;nbsp;is a single entity in Azure Search, think &quot;record&quot; or (indeed) document in a document database. It has an id&amp;nbsp;that is unique within that index.An indexer&amp;nbsp;is something that takes data from a source, like Blob storage, SQL or something else, re-formats it and writes it to documents in an index in Azure Search.A data source&amp;nbsp;is a definition in Azure Search of somewhere that an indexer&amp;nbsp;can read data from. It's sort of like a connection string.ScenarioThe specific scenario I have is the need to build a database of candidates for a recruitment system. I want to be able to search the contents of CVs, but I also want to be able to filter and use faceted search on various bits of data about the candidate themselves, such as location, qualification and so on.Azure Search provides built-in functionality to read the contents from a range of files stored in Azure Blob Storage, so that sounded pretty ideal; just upload the CVs to blob storage, point Azure search to it and be done. Of course, that would not combine it with the other candidate data. Initially I thought about several options;I could extract the content of the files myself and write code to combine it with candidate data in code. But extracting content out of files is not a trivial problem.I could let Azure Search index the files, then write code to extract the content back out of Azure Search and write it back to another index. But that seemed like a very long-winded solution.I asked Microsoft if I could somehow access the content-extraction feature directly. But you can't.The solutionLiam Cavanagh&amp;nbsp;gave me the outline solution with this statement;Leverage Blob Storage with Data Store (such as Azure SQL or DocumentDB):&amp;nbsp; If you store your files in Azure Blob storage, and your structured data in something like Azure SQL or DocumentDB, you can use both the Azure Search Blob Indexer as well as the Azure Search SQL or DocumentDB Indexer together.&amp;nbsp; The only trick is to make sure that the unique key that is defined for each document matches between Blob and the structured data store.&amp;nbsp; For example, if you choose to use the Base64 Encoded filename as the key, then you would need to make sure that the matching content in your structured datastore also contains this value.&amp;nbsp; That way the Indexer which does something called MergeOrUpdate, will first take a row from say the Blob Storage and insert it as a new row in Azure Search and then it will bring in the data from the structured data store and update the existing row with the new fields.With a bit more help from Liam I put together the sample solution outlined below. I should also mention that in this solution I am using the built-in tools in Azure Search to read data directly from SQL and Blob storage. If you wanted to use code to manually write to the index instead of letting Azure Search read from SQL then you can do that too, of course. Or read from some of the other supported data sources, or combine it all or...ImplementationYou'll need to create an Azure Search account and whatever data sources you want to use. You can do a number of the steps directly in the Azure Portal, though not all of them. For the ones you can't I will show the appropriate JSON payload you need to send. It is ridiculously easy with something like Postman. You just need to make sure you add two headers to your requests;Content-Type : application/jsonapi-key : [an admin key for your Azure Search instance]In summary, this is what we are going to build:Create the IndexYou can create a new index directly in the Azure Portal, which is what I did. It has a very nice editor that makes it quite easy. I ended up with something looking like this;Please note the &quot;content&quot; field; When Azure Search indexes files, it will place the content of those files in the content&amp;nbsp;field. Id&amp;nbsp;is there by default and we need that, so leave it alone, but the remaining fields can be whatever you like. I put a name in there for the Candidate Name and the last two fields are some fields I added so I could experiment with faceted search.Create the data sourcesNext we need to tell Azure Search where it can get the data - we need to create the Data Sources.You can't create a Data Source on it's own in the Portal; there is an option to &quot;import data&quot; but that combines setting up the data source with the index and the indexer so we can't use it for our purposes.Time to start posting JSON (see above).POST these to https://yoursearchservice.search.windows.net/datasources?api-version=2016-09-01 and replace the XXX with valid connection strings.{ &quot;name&quot; : &quot;blobcvs&quot;, &quot;type&quot; : &quot;azureblob&quot;, &quot;credentials&quot; : { &quot;connectionString&quot; : &quot;XXX&quot; }, &quot;container&quot; : { &quot;name&quot; : &quot;cvs&quot; }}{ &quot;name&quot; : &quot;candidates&quot;, &quot;type&quot; : &quot;azuresql&quot;, &quot;credentials&quot; : { &quot;connectionString&quot; : &quot;XXX&quot; }, &quot;container&quot; : { &quot;name&quot; : &quot;Candidates&quot; }} This tells Azure Search how to access your data.Create the indexersPOST these to&amp;nbsp;https://yoursearchservice.search.windows.net/indexers?api-version=2016-09-01{ &quot;name&quot; : &quot;candidateindexer&quot;, &quot;dataSourceName&quot; : &quot;candidates&quot;, &quot;targetIndexName&quot; : &quot;candidates&quot;, &quot;fieldMappings&quot; : [ { &quot;sourceFieldName&quot; : &quot;Thingiemajics&quot;, &quot;mappingFunction&quot; : { &quot;name&quot; : &quot;jsonArrayToStringCollection&quot; } } ]}This tells Azure Search to take the data in the SQL database specified in the SQL data source and create a document in Azure Search for each row. Azure Search will automatically match fields with the same names; I've got an Id field as well as Name, Type and Thingiemajics columns in SQL. The only one that is a bit special is Thingiemajics; I'm storing an array of tag values in that field in SQL in the format [&quot;red&quot;, &quot;white&quot;, &quot;blue&quot;] and putting that mapping function in there tells Azure Search to make them individual tags that can be filtered individually. See the docs&amp;nbsp;for more details.Before I create the indexer for the files, let me just take a little detour. If you remember, the original statement was that for this to work, the two different indexers need to use the same&amp;nbsp;id for data about the same candidate. The SQL indexer in my example uses the database ID of the candidate and we need to ensure that when Azure Search indexes the CV for a candidate it returns the same index. By default&amp;nbsp;Azure Search will use the filename, which is obviously no good in this situation. The way I solved this was to add a custom meta data property to the blob when I uploaded it to Azure Blob Storage, something like this:using (var fileStream = System.IO.File.OpenRead(file)){ await blob.UploadFromStreamAsync(fileStream);}blob.Metadata.Add(&quot;mykey&quot;, identifier);await blob.SetMetadataAsync();Here I have called it &quot;mykey&quot;, but it could be called anything.On to the indexer, which is created with this:{ &quot;name&quot; : &quot;cvindexer&quot;, &quot;dataSourceName&quot; : &quot;blobcvs&quot;, &quot;targetIndexName&quot; : &quot;candidates&quot;, &quot;fieldMappings&quot; : [ { &quot;sourceFieldName&quot; : &quot;mykey&quot;, &quot;targetFieldName&quot; : &quot;id&quot; } ], &quot;parameters&quot; : { &quot;configuration&quot; : { &quot;failOnUnsupportedContentType&quot; : false } }}The most important thing here is that I tell Azure Search to take the metadata property &quot;mykey&quot; and map it to the &quot;id&quot; field in my index. That's all that is required to ensure the contents of the CV ends up in the same search document as the other Candidate information.NotesIn my solution here I added an ID property to the blob meta data. I could, of course, have gone the other way instead and added the blob reference to the SQL data and then remapped that reference to &quot;id&quot; in the SQL indexer. Both approaches are equally valid, both have pros and cons, including the need to manually maintain uniqueness of the ID property and the need to base64 encode filenames.You should also note that this works because the different datasources do not have any of the same field names; if the two indexers both return the same field, one will win and delete the data from the other one.</summary></entry><entry><title type="html">How many ways can I host a web app in Azure?</title><link href="https://www.lytzen.name/2017/01/06/how-many-ways-can-i-host-web-app-in.html" rel="alternate" type="text/html" title="How many ways can I host a web app in Azure?" /><published>2017-01-06T20:17:00+00:00</published><updated>2017-01-06T20:17:00+00:00</updated><id>https://www.lytzen.name/2017/01/06/how-many-ways-can-i-host-web-app-in</id><content type="html" xml:base="https://www.lytzen.name/2017/01/06/how-many-ways-can-i-host-web-app-in.html">I got talking to a colleague about how many ways there are to host web apps in Azure and even managed to surprise myself by just how many ways I could think of. It inspired me to compile this list, which is just off the top of my head. I'm sure there are more - if you can think of other ways, please leave a comment.&lt;br /&gt;&lt;br /&gt;For the purposes of this, I am defining a web app as something that has a user-facing UI &lt;i&gt;and &lt;/i&gt;some server-side functionality. I have listed a few additional options at the bottom if you only need one of those things.&lt;br /&gt;&lt;br /&gt;I put this list together real quick like and the description of each service is just a very quick summary, mainly just from memory so please do not view this as a definitive or highly accurate document; It's mainly just a fun exercise. That said, do please point out any factual errors in the comments so I can correct them.&lt;br /&gt;&lt;br /&gt;&lt;h3&gt;&lt;a href=&quot;https://azure.microsoft.com/en-gb/services/app-service/web/&quot;&gt;Web Apps&lt;/a&gt;&lt;/h3&gt;&lt;div&gt;This is the default option and probably what you should choose if in doubt. It's getting a lot of love from Microsoft at the moment and is constantly getting new features. You get fail-over and auto-scaling by default, plans from free to expensive and it is available in both Windows and Linux flavours (in preview). It's very easy to get started with and supports pretty much anything from static pages to complex deployment processes, staging slots and even a built-in basic CD pipeline.&lt;br /&gt;The main thing to be aware of is that you don't get admin access to the server, so if you need to, say, customise which SSL protocols are available or you need to install fonts, you are out of luck. From experience, it is very rare that you need this, though.&amp;nbsp;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;h3&gt;&lt;a href=&quot;https://azure.microsoft.com/en-gb/services/cloud-services/&quot;&gt;Cloud Services&lt;/a&gt;&lt;/h3&gt;&lt;div&gt;This is the original Platform-as-a-Service option in Azure. It doesn't get much love these days, but I am still fond of it for those few situations where I need more than Web Apps can give me. Essentially, you provide Azure with an application package and Azure will take care of deploying that to one or more servers for you. You do get full admin access to the servers so you can do what you like - as long as you script it as part of your package. Patching, fail-over, load-balancing, auto-scaling, health monitoring etc is all taken care of for you.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;h3&gt;&lt;a href=&quot;https://azure.microsoft.com/en-gb/services/functions/&quot;&gt;Functions&lt;/a&gt;&lt;/h3&gt;&lt;div&gt;This isn't really meant for doing a proper web app, but you &lt;i&gt;can &lt;/i&gt;write a collection of functions that acts as an API or return some HTML etc, so you could certainly do it if you really wanted. Understand me right, though, Functions are brilliant for what they are meant to do, even if building whole web apps isn't it. That said, if you just need a couple of simple APIs to support a front-end app then it is certainly something you should consider.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;h3&gt;&lt;a href=&quot;https://azure.microsoft.com/en-gb/services/service-fabric/&quot;&gt;Service Fabric&lt;/a&gt;&lt;/h3&gt;&lt;div&gt;Not very well known, but Azure provides a pretty advanced micro-services framework that you can use for building sophisticated, large-scale applications. It supports both a service model and a basic Actor model out of the box. It's got a pretty high base cost relative to other Azure services due to the minimum number of nodes you have to use, but if you have a need for lots of scale then you should definitely look at this. Azure uses it to power a lot of their own architecture, including both SQL Azure and DocumentDb.&amp;nbsp;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;h3&gt;&lt;a href=&quot;https://azure.microsoft.com/en-gb/services/virtual-machines/&quot;&gt;Virtual Machine&lt;/a&gt;&lt;/h3&gt;&lt;div&gt;You can, of course, deploy good old-fashined virtual machines and run your web app on them. You are then responsible for patching and some level of maintenance yourself. You can define images so Azure can do auto scaling for you as well. I personally try to avoid using VMs as far as I can as I don't like to be responsible for patching and maintenance etc - yet, I still have about 40 of them :).&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;h3&gt;&lt;a href=&quot;https://azure.microsoft.com/en-gb/services/container-service/&quot;&gt;Container Services / Docker&lt;/a&gt;&lt;/h3&gt;&lt;div&gt;If you are one of those cool kids who like Docker, you are in luck. Azure has native support for Docker and support DC/OS, Docker Swarm and Kubernetes out of the box.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;So, how many was that? Six, I think, though I probably shouldn't count the Functions one :)&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;h4&gt;Related&lt;/h4&gt;&lt;div&gt;As if all the above wasn't enough, there are a couple of other technologies that can also be used to deal with web sites or APIs.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;h3&gt;&lt;a href=&quot;https://azure.microsoft.com/en-gb/services/storage/blobs/&quot;&gt;Blob Storage&lt;/a&gt; / &lt;a href=&quot;https://azure.microsoft.com/en-gb/services/cdn/&quot;&gt;CDN&lt;/a&gt;&lt;/h3&gt;&lt;div&gt;Blob storage is Azure's file storage system (okay, it's more than that, but let that suffice for now). You can share a container publicly and put html, js, css and whatever other files you like in it. I quite often use it for static assets, though in theory you could host a whole website in there. Azure CDN &lt;i&gt;can&lt;/i&gt; sit on top of blob storage and gives you geo-replication of the files. You can map custom domains to blob storage as well. Of course, this is all static so I couldn't quite include it in my list above.&amp;nbsp;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;h3&gt;&lt;a href=&quot;https://azure.microsoft.com/en-gb/services/traffic-manager/&quot;&gt;Traffic Manager&lt;/a&gt;&lt;/h3&gt;&lt;div&gt;Traffic Manager doesn't host anything, but it is worth understanding where it fits in. Pretty much all of the above options include a load balancer (automatic, you don't really have to worry about it) and fail-over within a data centre. If you need fail-over between data centres or want to route traffic to servers close to your users, you can use Traffic Manager. It works at the DNS level and is used to direct user requests for a given domain name to the nearest available data centre.&amp;nbsp;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;h3&gt;&lt;a href=&quot;https://azure.microsoft.com/en-gb/services/api-management/&quot;&gt;API Management&lt;/a&gt;&lt;/h3&gt;&lt;div&gt;Not strongly related to web app hosting, but I just thought I'd mention it; Basically, if you develop an API and you want to give external users access to it you probably want to do things like controlling who can use it (maybe so you can bill them), rate limiting, authentication and so on. Azure API management deals with all that as a service that just sits in front of your naked API so you don't have to write it all yourself.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;h4&gt;Takeaway&lt;/h4&gt;&lt;div&gt;When I started this list, I really didn't expect it would be this long. I think it's great there is so much choice and I know that each option has it's own set of strengths and weaknesses. We use most of the technologies listed here on different projects and for different reasons, and I'm very happy that I can choose. At the same time, I think it is probably quite tough for someone new to Azure to even get started on figuring out which of the many options are right for their particular scenario. And I have only dealt with web app hosting here, not the multitude of other things you can do.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;I just meant for this post to be a fun little exercise but, having written it, I should mention that at &lt;a href=&quot;http://www.neworbit.co.uk/&quot;&gt;NewOrbit&lt;/a&gt; we have recently started helping other companies move to the cloud and sharing our years of Azure experience with them.&amp;nbsp;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;If you have any questions or want to talk more, &lt;a href=&quot;https://twitter.com/flytzen&quot;&gt;ping me on twitter&lt;/a&gt;&amp;nbsp;or add a comment below.&lt;/div&gt;</content><author><name>Frans Lytzen</name></author><category term="Azure" /><summary type="html">I got talking to a colleague about how many ways there are to host web apps in Azure and even managed to surprise myself by just how many ways I could think of. It inspired me to compile this list, which is just off the top of my head. I'm sure there are more - if you can think of other ways, please leave a comment.For the purposes of this, I am defining a web app as something that has a user-facing UI and some server-side functionality. I have listed a few additional options at the bottom if you only need one of those things.I put this list together real quick like and the description of each service is just a very quick summary, mainly just from memory so please do not view this as a definitive or highly accurate document; It's mainly just a fun exercise. That said, do please point out any factual errors in the comments so I can correct them.Web AppsThis is the default option and probably what you should choose if in doubt. It's getting a lot of love from Microsoft at the moment and is constantly getting new features. You get fail-over and auto-scaling by default, plans from free to expensive and it is available in both Windows and Linux flavours (in preview). It's very easy to get started with and supports pretty much anything from static pages to complex deployment processes, staging slots and even a built-in basic CD pipeline.The main thing to be aware of is that you don't get admin access to the server, so if you need to, say, customise which SSL protocols are available or you need to install fonts, you are out of luck. From experience, it is very rare that you need this, though.&amp;nbsp;Cloud ServicesThis is the original Platform-as-a-Service option in Azure. It doesn't get much love these days, but I am still fond of it for those few situations where I need more than Web Apps can give me. Essentially, you provide Azure with an application package and Azure will take care of deploying that to one or more servers for you. You do get full admin access to the servers so you can do what you like - as long as you script it as part of your package. Patching, fail-over, load-balancing, auto-scaling, health monitoring etc is all taken care of for you.FunctionsThis isn't really meant for doing a proper web app, but you can write a collection of functions that acts as an API or return some HTML etc, so you could certainly do it if you really wanted. Understand me right, though, Functions are brilliant for what they are meant to do, even if building whole web apps isn't it. That said, if you just need a couple of simple APIs to support a front-end app then it is certainly something you should consider.Service FabricNot very well known, but Azure provides a pretty advanced micro-services framework that you can use for building sophisticated, large-scale applications. It supports both a service model and a basic Actor model out of the box. It's got a pretty high base cost relative to other Azure services due to the minimum number of nodes you have to use, but if you have a need for lots of scale then you should definitely look at this. Azure uses it to power a lot of their own architecture, including both SQL Azure and DocumentDb.&amp;nbsp;Virtual MachineYou can, of course, deploy good old-fashined virtual machines and run your web app on them. You are then responsible for patching and some level of maintenance yourself. You can define images so Azure can do auto scaling for you as well. I personally try to avoid using VMs as far as I can as I don't like to be responsible for patching and maintenance etc - yet, I still have about 40 of them :).Container Services / DockerIf you are one of those cool kids who like Docker, you are in luck. Azure has native support for Docker and support DC/OS, Docker Swarm and Kubernetes out of the box.So, how many was that? Six, I think, though I probably shouldn't count the Functions one :)RelatedAs if all the above wasn't enough, there are a couple of other technologies that can also be used to deal with web sites or APIs.Blob Storage / CDNBlob storage is Azure's file storage system (okay, it's more than that, but let that suffice for now). You can share a container publicly and put html, js, css and whatever other files you like in it. I quite often use it for static assets, though in theory you could host a whole website in there. Azure CDN can sit on top of blob storage and gives you geo-replication of the files. You can map custom domains to blob storage as well. Of course, this is all static so I couldn't quite include it in my list above.&amp;nbsp;Traffic ManagerTraffic Manager doesn't host anything, but it is worth understanding where it fits in. Pretty much all of the above options include a load balancer (automatic, you don't really have to worry about it) and fail-over within a data centre. If you need fail-over between data centres or want to route traffic to servers close to your users, you can use Traffic Manager. It works at the DNS level and is used to direct user requests for a given domain name to the nearest available data centre.&amp;nbsp;API ManagementNot strongly related to web app hosting, but I just thought I'd mention it; Basically, if you develop an API and you want to give external users access to it you probably want to do things like controlling who can use it (maybe so you can bill them), rate limiting, authentication and so on. Azure API management deals with all that as a service that just sits in front of your naked API so you don't have to write it all yourself.TakeawayWhen I started this list, I really didn't expect it would be this long. I think it's great there is so much choice and I know that each option has it's own set of strengths and weaknesses. We use most of the technologies listed here on different projects and for different reasons, and I'm very happy that I can choose. At the same time, I think it is probably quite tough for someone new to Azure to even get started on figuring out which of the many options are right for their particular scenario. And I have only dealt with web app hosting here, not the multitude of other things you can do.I just meant for this post to be a fun little exercise but, having written it, I should mention that at NewOrbit we have recently started helping other companies move to the cloud and sharing our years of Azure experience with them.&amp;nbsp;If you have any questions or want to talk more, ping me on twitter&amp;nbsp;or add a comment below.</summary></entry><entry><title type="html">Find docs with no PartitionKey in Azure DocumentDb</title><link href="https://www.lytzen.name/2016/12/06/find-docs-with-no-partitionkey-in-azure.html" rel="alternate" type="text/html" title="Find docs with no PartitionKey in Azure DocumentDb" /><published>2016-12-06T21:31:00+00:00</published><updated>2016-12-06T21:31:00+00:00</updated><id>https://www.lytzen.name/2016/12/06/find-docs-with-no-partitionkey-in-azure</id><content type="html" xml:base="https://www.lytzen.name/2016/12/06/find-docs-with-no-partitionkey-in-azure.html">When you are using Partitioned Collections in Azure DocumentDb you need to specify a &lt;i&gt;Partition Key&lt;/i&gt; on each Document. At least, I thought you did. But, it turns out that you actually &lt;i&gt;can&lt;/i&gt;&amp;nbsp;save documents without a partitionkey. But if you do, you'll have a hard time retrieving or deleting them - until you meet Undefined.Value.&lt;br /&gt;&lt;i&gt;Note: This post is written for C#, I am not sure about the equivalent for other languages.&lt;/i&gt;&lt;br /&gt;&lt;i&gt;&lt;br /&gt;&lt;/i&gt;&lt;br /&gt;&lt;h4&gt;Details&lt;/h4&gt;&lt;div&gt;If you create a Partitioned Collection in Azure DocumentDb you probably think that every document you save must have a partitionkey property and probably also that it must have a value. In this post I am dealing with the situation where you don't have a partition key property on your document at all, not the situation where you have one but you set it to null or an empty string.&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;For example, if you have created your collection with code similar to this;&lt;br /&gt;&lt;pre class=&quot;prettyprint&quot;&gt;var docCollection = new DocumentCollection()&lt;br /&gt;{&lt;br /&gt;   Id = this.collectionName&lt;br /&gt;};&lt;br /&gt;docCollection.PartitionKey.Paths.Add(&quot;/partitionKey&quot;);&lt;br /&gt;await docClient.CreateDocumentCollectionAsync(&lt;br /&gt;                    UriFactory.CreateDatabaseUri(this.dbName), &lt;br /&gt;                    docCollection);&lt;/pre&gt;and you then try to save an instance of a class that looks like this:&lt;br /&gt;&lt;pre class=&quot;prettyprint&quot;&gt;public class MyItem&lt;br /&gt;{&lt;br /&gt;    [JsonProperty(&quot;id&quot;)]&lt;br /&gt;    public string Id { get; set; }&lt;br /&gt;&lt;br /&gt;    public string SomeValue { get; set; }&lt;br /&gt;}&lt;br /&gt;&lt;/pre&gt;then you may expect to get an error. But, in fact, it will save just fine as I found out to my detriment after a major refactoring.&lt;/div&gt;&lt;br /&gt;&lt;div&gt;Now that you have that item in the database you will find it hard to retrieve it and even harder to delete it - until you meet your new friend &lt;b&gt;Undefined.Value&lt;/b&gt;.&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;h3&gt;How to read the document:&lt;/h3&gt;&lt;pre class=&quot;prettyprint&quot;&gt;MyItem item = (dynamic)client.ReadDocumentAsync(&lt;br /&gt;                           UriFactory.CreateDocumentUri(DbName, CollectionName, id),&lt;br /&gt;                           new RequestOptions() {&lt;br /&gt;                             PartitionKey = new PartitionKey(Undefined.Value)&lt;br /&gt;                           })&lt;br /&gt;                         .Result.Resource;&lt;br /&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div&gt;&lt;h3&gt;How to delete the document:&lt;/h3&gt;&lt;pre class=&quot;prettyprint&quot;&gt;client.DeleteDocumentAsync(&lt;br /&gt;          UriFactory.CreateDocumentUri(DbName, CollectionName, id), &lt;br /&gt;          new RequestOptions() { PartitionKey = new PartitionKey(Undefined.Value) });&lt;br /&gt;&lt;/pre&gt;&lt;/div&gt;Many thanks to &lt;a href=&quot;https://twitter.com/arkramac&quot;&gt;Aravind Ramachandran&lt;/a&gt; for telling me about Undefined.Value.&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;</content><author><name>Frans Lytzen</name></author><category term="CosmosDB" /><category term="Azure" /><summary type="html">When you are using Partitioned Collections in Azure DocumentDb you need to specify a Partition Key on each Document. At least, I thought you did. But, it turns out that you actually can&amp;nbsp;save documents without a partitionkey. But if you do, you'll have a hard time retrieving or deleting them - until you meet Undefined.Value.Note: This post is written for C#, I am not sure about the equivalent for other languages.DetailsIf you create a Partitioned Collection in Azure DocumentDb you probably think that every document you save must have a partitionkey property and probably also that it must have a value. In this post I am dealing with the situation where you don't have a partition key property on your document at all, not the situation where you have one but you set it to null or an empty string.For example, if you have created your collection with code similar to this;var docCollection = new DocumentCollection(){ Id = this.collectionName};docCollection.PartitionKey.Paths.Add(&quot;/partitionKey&quot;);await docClient.CreateDocumentCollectionAsync( UriFactory.CreateDatabaseUri(this.dbName), docCollection);and you then try to save an instance of a class that looks like this:public class MyItem{ [JsonProperty(&quot;id&quot;)] public string Id { get; set; } public string SomeValue { get; set; }}then you may expect to get an error. But, in fact, it will save just fine as I found out to my detriment after a major refactoring.Now that you have that item in the database you will find it hard to retrieve it and even harder to delete it - until you meet your new friend Undefined.Value.How to read the document:MyItem item = (dynamic)client.ReadDocumentAsync( UriFactory.CreateDocumentUri(DbName, CollectionName, id), new RequestOptions() { PartitionKey = new PartitionKey(Undefined.Value) }) .Result.Resource;How to delete the document:client.DeleteDocumentAsync( UriFactory.CreateDocumentUri(DbName, CollectionName, id), new RequestOptions() { PartitionKey = new PartitionKey(Undefined.Value) });Many thanks to Aravind Ramachandran for telling me about Undefined.Value.</summary></entry><entry><title type="html">Find Documents with missing properties in Azure DocumentDb with the .Net SDK</title><link href="https://www.lytzen.name/2016/09/29/find-documents-with-missing-properties.html" rel="alternate" type="text/html" title="Find Documents with missing properties in Azure DocumentDb with the .Net SDK" /><published>2016-09-29T21:06:00+01:00</published><updated>2016-09-29T21:06:00+01:00</updated><id>https://www.lytzen.name/2016/09/29/find-documents-with-missing-properties</id><content type="html" xml:base="https://www.lytzen.name/2016/09/29/find-documents-with-missing-properties.html">Azure DocumentDb stores documents as JSON. One of the effects of this is that sometimes you may end up with documents in the database that have missing properties and it can be quite tricky to search for them with the .Net SDK. This blog post has an approach to doing it - and quite simply too.&lt;br /&gt;&lt;br /&gt;&lt;h3&gt;Background&lt;/h3&gt;&lt;div&gt;Most commonly, you would encounter the issue of the missing property when you add a new property to an existing class in your .Net code. There is no automatic method of adding this new property to all the existing entries in the database, short of re-saving them all.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;Alternatively, you can explicitly configure Json.Net to not store properties that have null values like this:&lt;/div&gt;&lt;pre class=&quot;prettyprint&quot;&gt;JsonConvert.DefaultSettings = () =&amp;gt; &lt;br /&gt;  new JsonSerializerSettings&lt;br /&gt;       {&lt;br /&gt;          NullValueHandling = NullValueHandling.Ignore&lt;br /&gt;       };&lt;br /&gt;&lt;/pre&gt;&lt;br /&gt;&lt;div&gt;You can use this configuration option to test the behaviour I am describing here or to save space in the database.&lt;br /&gt;&lt;br /&gt;For example, imagine you have a class called MyItem looking like this:&lt;/div&gt;&lt;pre class=&quot;prettyprint&quot;&gt;public class MyItem&lt;br /&gt;{&lt;br /&gt;    [JsonProperty(&quot;id&quot;)]&lt;br /&gt;    public string Id { get; set; }&lt;br /&gt;    public string SomeValue { get; set; }&lt;br /&gt;}&lt;br /&gt;&lt;/pre&gt;&lt;br /&gt;&lt;div&gt;If you have an item where SomeValue is null, by default that will be serialised and stored in DocumentDb like this:&lt;/div&gt;&lt;pre class=&quot;prettyprint&quot;&gt;{&lt;br /&gt;  &quot;id&quot; : &quot;1&quot;,&lt;br /&gt;  &quot;SomeValue&quot; : null&lt;br /&gt;}&lt;br /&gt;&lt;/pre&gt;&lt;br /&gt;&lt;div&gt;However, if you configure Json.Net to not store null values (or the SomeValue field was added to your .Net code after you stored this item in DocumentDb) it will look like this in the database:&lt;/div&gt;&lt;pre class=&quot;prettyprint&quot;&gt;{&lt;br /&gt;  &quot;id&quot; : &quot;1&quot;,&lt;br /&gt;}&lt;br /&gt;&lt;/pre&gt;&lt;br /&gt;&lt;br /&gt;&lt;h3&gt;Selecting missing properties with SQL&lt;/h3&gt;&lt;div&gt;According to&amp;nbsp;&lt;a href=&quot;https://azure.microsoft.com/en-gb/documentation/articles/documentdb-sql-query/&quot;&gt;the documentation&lt;/a&gt;&amp;nbsp;you can use SQL to select missing properties like this:&lt;/div&gt;&lt;pre class=&quot;prettyprint&quot;&gt;SELECT f.lastName ?? f.surname AS familyName&lt;br /&gt;FROM Families f&lt;br /&gt;&lt;/pre&gt;&lt;div&gt;You can then extrapolate from that example to, for example, select items etc.&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;h3&gt;Finding items with null &lt;i&gt;or&lt;/i&gt;&amp;nbsp;missing with the .Net SDK&lt;/h3&gt;&lt;/div&gt;&lt;div&gt;Imagine you have added the SomeValue property to the MyItem class after you had already saved some items. Further, sometimes you store a null in the SomeValue property. Or you have configured Json.Net to ignore null values. And now you want to find all the items where SomeValue is either missing or null.&lt;/div&gt;&lt;div&gt;You might try this:&amp;nbsp;&lt;/div&gt;&lt;pre class=&quot;prettyprint&quot;&gt;var query1 = client.CreateDocumentQuery&amp;lt;MyItem&amp;gt;(collectionUrl)&lt;br /&gt;              .Where(i =&amp;gt; i.SomeValue == null);&lt;br /&gt;&lt;/pre&gt;&lt;div&gt;But you will find that this will not actually return any results - at least, it won't return any documents where SomeValue is not present at all. However, this odd-looking statement will work:&lt;/div&gt;&lt;pre class=&quot;prettyprint&quot;&gt;var query2 = client.CreateDocumentQuery&amp;lt;MyItem&amp;gt;(collectionUrl)&lt;br /&gt;              .Where(i =&amp;gt; (i.SomeValue ?? null) == null);&lt;br /&gt;&lt;/pre&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;It is using the null coalescor to make DocumentDb return a null value for the property for the property if it does not exist, which we can then compare to null.&lt;br /&gt;&lt;br /&gt;I have tested this with version 1.6, 1.8 and 1.10 of the SDK, but I would advise you to put an integration test in your code if you are going to rely on it, just in case the behaviour changes in the future. You'll probably also want to put a comment wherever you use this syntax as R# is quite keen to tell you that you should get rid of the &quot;?? null&quot; part.&lt;br /&gt;Finally, I have not done any performance testing on this, but I suspect DocumentDb won't be able to use any indexes to execute this query; it will probably have to evaluate each document in a scan so use with caution.&lt;/div&gt;&lt;br /&gt;&lt;h3&gt;A full sample&lt;/h3&gt;&lt;div&gt;If you want to try this out, there is a full example here:&amp;nbsp;&lt;a href=&quot;https://gist.github.com/flytzen/b8dbcb2079a66b1c8a67005ef5198052&quot;&gt;https://gist.github.com/flytzen/b8dbcb2079a66b1c8a67005ef5198052&lt;/a&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;</content><author><name>Frans Lytzen</name></author><category term="CosmosDB" /><category term="Azure" /><summary type="html">Azure DocumentDb stores documents as JSON. One of the effects of this is that sometimes you may end up with documents in the database that have missing properties and it can be quite tricky to search for them with the .Net SDK. This blog post has an approach to doing it - and quite simply too.BackgroundMost commonly, you would encounter the issue of the missing property when you add a new property to an existing class in your .Net code. There is no automatic method of adding this new property to all the existing entries in the database, short of re-saving them all.Alternatively, you can explicitly configure Json.Net to not store properties that have null values like this:JsonConvert.DefaultSettings = () =&amp;gt; new JsonSerializerSettings { NullValueHandling = NullValueHandling.Ignore };You can use this configuration option to test the behaviour I am describing here or to save space in the database.For example, imagine you have a class called MyItem looking like this:public class MyItem{ [JsonProperty(&quot;id&quot;)] public string Id { get; set; } public string SomeValue { get; set; }}If you have an item where SomeValue is null, by default that will be serialised and stored in DocumentDb like this:{ &quot;id&quot; : &quot;1&quot;, &quot;SomeValue&quot; : null}However, if you configure Json.Net to not store null values (or the SomeValue field was added to your .Net code after you stored this item in DocumentDb) it will look like this in the database:{ &quot;id&quot; : &quot;1&quot;,}Selecting missing properties with SQLAccording to&amp;nbsp;the documentation&amp;nbsp;you can use SQL to select missing properties like this:SELECT f.lastName ?? f.surname AS familyNameFROM Families fYou can then extrapolate from that example to, for example, select items etc.Finding items with null or&amp;nbsp;missing with the .Net SDKImagine you have added the SomeValue property to the MyItem class after you had already saved some items. Further, sometimes you store a null in the SomeValue property. Or you have configured Json.Net to ignore null values. And now you want to find all the items where SomeValue is either missing or null.You might try this:&amp;nbsp;var query1 = client.CreateDocumentQuery&amp;lt;MyItem&amp;gt;(collectionUrl) .Where(i =&amp;gt; i.SomeValue == null);But you will find that this will not actually return any results - at least, it won't return any documents where SomeValue is not present at all. However, this odd-looking statement will work:var query2 = client.CreateDocumentQuery&amp;lt;MyItem&amp;gt;(collectionUrl) .Where(i =&amp;gt; (i.SomeValue ?? null) == null);It is using the null coalescor to make DocumentDb return a null value for the property for the property if it does not exist, which we can then compare to null.I have tested this with version 1.6, 1.8 and 1.10 of the SDK, but I would advise you to put an integration test in your code if you are going to rely on it, just in case the behaviour changes in the future. You'll probably also want to put a comment wherever you use this syntax as R# is quite keen to tell you that you should get rid of the &quot;?? null&quot; part.Finally, I have not done any performance testing on this, but I suspect DocumentDb won't be able to use any indexes to execute this query; it will probably have to evaluate each document in a scan so use with caution.A full sampleIf you want to try this out, there is a full example here:&amp;nbsp;https://gist.github.com/flytzen/b8dbcb2079a66b1c8a67005ef5198052</summary></entry><entry><title type="html">Auto publish Azure Web Jobs with ASP.Net Core RTM</title><link href="https://www.lytzen.name/2016/08/26/auto-publish-azure-web-jobs-with-aspnet.html" rel="alternate" type="text/html" title="Auto publish Azure Web Jobs with ASP.Net Core RTM" /><published>2016-08-26T10:36:00+01:00</published><updated>2016-08-26T10:36:00+01:00</updated><id>https://www.lytzen.name/2016/08/26/auto-publish-azure-web-jobs-with-aspnet</id><content type="html" xml:base="https://www.lytzen.name/2016/08/26/auto-publish-azure-web-jobs-with-aspnet.html">&lt;i&gt;This is an update of my &lt;a href=&quot;http://blog.lytzen.name/2016/03/auto-deploy-azure-web-job-with-aspnet-5.html&quot;&gt;original post&lt;/a&gt; on how to do this with ASP.Net Core RC1&lt;/i&gt;&lt;br /&gt;At the time of this writing, there is no tooling to auto publish Azure Web Jobs with an ASP.Net Core website. You can do it with old-style websites, but not yet the new ones. The tooling is highly likely to appear eventually, but as of right now you have to take a few steps to set it up yourself, which is what I describe in this post. For clarity, what I am describing works only with source control deploy (git etc),&amp;nbsp;&lt;i&gt;not&lt;/i&gt;&amp;nbsp;with publishing from Visual Studio.&lt;br /&gt;&lt;br /&gt;&lt;b&gt;This information will eventually become obsolete, probably around the RTM of the ASP.Net Core Visual Studio tooling. If that has happened and I have not updated the post, please post a comment and I'll get on it.&lt;/b&gt;&lt;br /&gt;Note that for very simple, self-contained web jobs, there may be a simpler approach (see&amp;nbsp;&lt;a href=&quot;http://stackoverflow.com/a/33291097/11534&quot;&gt;http://stackoverflow.com/a/33291097/11534&lt;/a&gt;&amp;nbsp;for some thoughts). In this post I am catering for the scenario where your webjob references another project in your solution - though it'll work just as well if it doesn't.&lt;br /&gt;&lt;h3&gt;In summary&lt;/h3&gt;&lt;div&gt;You have to use a custom deployment script and insert an action to&amp;nbsp;&lt;i&gt;publish&lt;/i&gt;&amp;nbsp;your webjob to the path that Azure looks for webjobs in; App_Data/Jobs/Continuous for continuous jobs. Azure will automatically detect content in that folder and assume it's a webjob, so all we really have to do is make sure our webjob is copied there. And yes, it will happily overwrite a running webjob, the Kudu functionality handles that somehow.&lt;br /&gt;&lt;br /&gt;The reason we are&amp;nbsp;&lt;i&gt;publishing,&amp;nbsp;&lt;/i&gt;using &lt;i&gt;dotnet publish&lt;/i&gt;, is to ensure we get all the dependencies. It won't re-compile so it's just a copy operation.&lt;br /&gt;If we had a very simple webjob that was self contained, you could just copy&amp;nbsp;the files, as long as you added a&amp;nbsp;&lt;i&gt;run.cmd&lt;/i&gt;&amp;nbsp;(see link above).&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;&lt;h3&gt;In detail&lt;/h3&gt;&lt;div&gt;Prepare a site&lt;br /&gt;&lt;ol&gt;&lt;li&gt;Set up a solution with an ASP.Net 5 Web site and a webjob written as a ASP.Net 5 / Core console app.&lt;/li&gt;&lt;li&gt;Set up source control deploy to an Azure website. It shouldn't matter which type.&lt;/li&gt;&lt;li&gt;Wait for the initial deploy of the site.&amp;nbsp;&lt;/li&gt;&lt;/ol&gt;&lt;h4&gt;Set up a custom deployment script&lt;/h4&gt;&lt;/div&gt;&lt;div&gt;In this example, we will download the deployment script that Azure has created. There are ways to also do this with the Azure CLI, but the generated script is not quite the same at this point in time - which may or may not matter.&lt;/div&gt;&lt;div&gt;&lt;ol&gt;&lt;li&gt;Get the auto generated script&lt;/li&gt;&lt;ol&gt;&lt;li&gt;Log in to the web app console at https://[yoursite].scm.azurewebsites.net&lt;/li&gt;&lt;li&gt;Go to Tools - Download deployment script&lt;/li&gt;&lt;li&gt;Unzip the downloaded zip file to the root of your solution folder&lt;/li&gt;&lt;/ol&gt;&lt;li&gt;Modify your deploy.cmd file in the following ways&lt;br /&gt;&lt;/li&gt;&lt;ol&gt;&lt;li&gt;Add a line like this near the top - just to make it easier to check that your custom script is being used (whatever you put after echo will be output in the log file)&lt;br /&gt;&lt;pre class=&quot;prettyprint&quot;&gt;echo CUSTOM SCRIPT Start&lt;br /&gt;&lt;/pre&gt;&lt;/li&gt;&lt;li&gt;Near the middle of the file you will find a series of steps that are numbered. One of the steps will look like this;&lt;br /&gt;&lt;strong&gt;NOTE: The indented lines will be on a single line, the line breaks are only added here for readability&lt;/strong&gt;&lt;br /&gt;&lt;pre class=&quot;prettyprint&quot;&gt;:: 2. Build and publish&lt;br /&gt;call :ExecuteCmd &quot;%MSBUILD_PATH%&quot; &quot;%DEPLOYMENT_SOURCE%\MySolution.sln&quot; &lt;br /&gt;       /nologo &lt;br /&gt;       /verbosity:m &lt;br /&gt;       /p:deployOnBuild=True;&lt;br /&gt;          AutoParameterizationWebConfigConnectionStrings=false;&lt;br /&gt;          Configuration=Release;&lt;br /&gt;          UseSharedCompilation=false;&lt;br /&gt;          publishUrl=&quot;%DEPLOYMENT_TEMP%&quot; &lt;br /&gt;       %SCM_BUILD_ARGS%&lt;br /&gt;&lt;br /&gt;&lt;/pre&gt;&lt;/li&gt;&lt;li&gt;Below that insert these lines (updating the path with the actual path to your webjob in the solution)&lt;br /&gt;&lt;strong&gt;NOTE: The indented lines need to put on a single line, the line breaks are only here for readability&lt;/strong&gt;&lt;br /&gt;&lt;pre class=&quot;prettyprint&quot;&gt;:: 2.1 Publish webjobs&lt;br /&gt;echo STARTING TO PUBLISH WEBJOBS&lt;br /&gt;echo DEPLOYMENT_TEMP is %DEPLOYMENT_TEMP%&lt;br /&gt;call :ExecuteCmd dotnet publish &lt;br /&gt;       &quot;%DEPLOYMENT_SOURCE%\src\MyWebJob\project.json&quot; &lt;br /&gt;         -o &quot;%DEPLOYMENT_TEMP%\App_Data\Jobs\Continuous\MyWebJob&quot; &lt;br /&gt;       -c Release&lt;br /&gt;&lt;/pre&gt;If you have more than one web job, just add individual publish lines for each one&lt;/li&gt;&lt;/ol&gt;&lt;/ol&gt;&lt;/div&gt;&lt;br /&gt;&lt;h4&gt;Check that it worked&lt;/h4&gt;&lt;div&gt;Push your changes and wait for Azure to deploy, then look at the webjobs in the Azure portal. You should see your job there. In case you don't, have a look at the publish log file to see if there are any errors in there.&lt;/div&gt;</content><author><name>Frans Lytzen</name></author><category term="Azure" /><summary type="html">This is an update of my original post on how to do this with ASP.Net Core RC1At the time of this writing, there is no tooling to auto publish Azure Web Jobs with an ASP.Net Core website. You can do it with old-style websites, but not yet the new ones. The tooling is highly likely to appear eventually, but as of right now you have to take a few steps to set it up yourself, which is what I describe in this post. For clarity, what I am describing works only with source control deploy (git etc),&amp;nbsp;not&amp;nbsp;with publishing from Visual Studio.This information will eventually become obsolete, probably around the RTM of the ASP.Net Core Visual Studio tooling. If that has happened and I have not updated the post, please post a comment and I'll get on it.Note that for very simple, self-contained web jobs, there may be a simpler approach (see&amp;nbsp;http://stackoverflow.com/a/33291097/11534&amp;nbsp;for some thoughts). In this post I am catering for the scenario where your webjob references another project in your solution - though it'll work just as well if it doesn't.In summaryYou have to use a custom deployment script and insert an action to&amp;nbsp;publish&amp;nbsp;your webjob to the path that Azure looks for webjobs in; App_Data/Jobs/Continuous for continuous jobs. Azure will automatically detect content in that folder and assume it's a webjob, so all we really have to do is make sure our webjob is copied there. And yes, it will happily overwrite a running webjob, the Kudu functionality handles that somehow.The reason we are&amp;nbsp;publishing,&amp;nbsp;using dotnet publish, is to ensure we get all the dependencies. It won't re-compile so it's just a copy operation.If we had a very simple webjob that was self contained, you could just copy&amp;nbsp;the files, as long as you added a&amp;nbsp;run.cmd&amp;nbsp;(see link above).In detailPrepare a siteSet up a solution with an ASP.Net 5 Web site and a webjob written as a ASP.Net 5 / Core console app.Set up source control deploy to an Azure website. It shouldn't matter which type.Wait for the initial deploy of the site.&amp;nbsp;Set up a custom deployment scriptIn this example, we will download the deployment script that Azure has created. There are ways to also do this with the Azure CLI, but the generated script is not quite the same at this point in time - which may or may not matter.Get the auto generated scriptLog in to the web app console at https://[yoursite].scm.azurewebsites.netGo to Tools - Download deployment scriptUnzip the downloaded zip file to the root of your solution folderModify your deploy.cmd file in the following waysAdd a line like this near the top - just to make it easier to check that your custom script is being used (whatever you put after echo will be output in the log file)echo CUSTOM SCRIPT StartNear the middle of the file you will find a series of steps that are numbered. One of the steps will look like this;NOTE: The indented lines will be on a single line, the line breaks are only added here for readability:: 2. Build and publishcall :ExecuteCmd &quot;%MSBUILD_PATH%&quot; &quot;%DEPLOYMENT_SOURCE%\MySolution.sln&quot; /nologo /verbosity:m /p:deployOnBuild=True; AutoParameterizationWebConfigConnectionStrings=false; Configuration=Release; UseSharedCompilation=false; publishUrl=&quot;%DEPLOYMENT_TEMP%&quot; %SCM_BUILD_ARGS%Below that insert these lines (updating the path with the actual path to your webjob in the solution)NOTE: The indented lines need to put on a single line, the line breaks are only here for readability:: 2.1 Publish webjobsecho STARTING TO PUBLISH WEBJOBSecho DEPLOYMENT_TEMP is %DEPLOYMENT_TEMP%call :ExecuteCmd dotnet publish &quot;%DEPLOYMENT_SOURCE%\src\MyWebJob\project.json&quot; -o &quot;%DEPLOYMENT_TEMP%\App_Data\Jobs\Continuous\MyWebJob&quot; -c ReleaseIf you have more than one web job, just add individual publish lines for each oneCheck that it workedPush your changes and wait for Azure to deploy, then look at the webjobs in the Azure portal. You should see your job there. In case you don't, have a look at the publish log file to see if there are any errors in there.</summary></entry><entry><title type="html">InfoSec with SQL Azure</title><link href="https://www.lytzen.name/2016/08/25/infosec-with-sql-azure.html" rel="alternate" type="text/html" title="InfoSec with SQL Azure" /><published>2016-08-25T15:17:00+01:00</published><updated>2016-08-25T15:17:00+01:00</updated><id>https://www.lytzen.name/2016/08/25/infosec-with-sql-azure</id><content type="html" xml:base="https://www.lytzen.name/2016/08/25/infosec-with-sql-azure.html">I've been using SQL Azure since 2011 and it's been a journey. One of the big problems I used to have was passing security audits from some of our clients; We deal with a &amp;nbsp;lot of data that is highly sensitive so are under a lot of scrutiny to make sure it is protected.&lt;br /&gt;&lt;br /&gt;A lot of what you have to comply with when you start going into the areas of ISO27001 and PCI compliance is not just about technical security in the way we normally think about it, it’s just as much about people and processes. In my experience from filling in dozens of security questionnaires, I believe that SQL Azure can tick all the boxes, as long as you switch on the right features. Just to be clear, I'm not required to comply with PCI so can't vouch for that, but I do have to comply with a number of ISO27001 and Data Protection requirements.&lt;br /&gt;&lt;br /&gt;&lt;h2&gt;Box ticking&lt;/h2&gt;Most security audits will ask you if your data is &quot;encrypted at rest&quot;. If you understand how Azure really works and where &amp;nbsp;that requirement originally comes from, you'll know that this is a meaningless requirement when you're in Azure. But, just try to convince a security auditor of that. So go ahead and switch on &lt;a href=&quot;https://msdn.microsoft.com/en-gb/library/dn948096.aspx&quot;&gt;Transparent Data Encryption&lt;/a&gt;&amp;nbsp; and you can tick that box on the questionnaire.&lt;br /&gt;Incidentally, Azure also recently enabled you to do something similar for &lt;a href=&quot;https://azure.microsoft.com/en-gb/documentation/articles/storage-service-encryption/&quot;&gt;Blob storage&lt;/a&gt; - again mainly so you can tick the box.&lt;br /&gt;&lt;br /&gt;&lt;h2&gt;Intrusion detection&lt;/h2&gt;SQL Azure has a neat Threat Detection facility to monitor the use of your database and alert you to anomalous behaviour. I've seen it detect potential SQL injection attacks and allegedly it will detect &quot;unusual&quot; behaviour, though I have yet to actually see that (thankfully :)).&lt;br /&gt;This is &amp;nbsp;a useful feature to potentially detect both external and internal attacks (see below).&lt;br /&gt;You have nothing to lose by &lt;a href=&quot;https://azure.microsoft.com/en-gb/documentation/articles/sql-database-threat-detection-get-started/&quot;&gt;switching it on&lt;/a&gt;.&lt;br /&gt;&lt;br /&gt;&lt;h2&gt;Internal attacks&lt;/h2&gt;As a developer it's easy to think that once you pass penetration testing, your job is done. But, for the organisation, that's only half the battle. A proper security audit will look just as much at how you are preventing and detecting what they sometimes call &quot;Internal Data Leakage&quot;. Basically, they are worried that people inside your organisation may access the data and leak it. This may be deliberately, it may be a result of social engineering or it may be that an external agency decided to hack a person who works for you. It's the modern equivalent of sneaking in through the kitchen entrance instead of taking a battering ram to the front gates. It's hardly surprising there is a lot of focus on this, given that most of the big attacks recently seems to have come from some kind of inside job.&lt;br /&gt;You may well trust everyone in your organisation to not want to deliberately betray you and to be sensible about not having their laptops hacked - but an auditor won't, so you may as well buckle up and do the right thing. SQL Azure makes it surprisingly easy, though there are number of manual steps that could be made a lot easier with some better tooling, especially if you have many databases.&lt;br /&gt;&lt;br /&gt;&lt;h3&gt;Step 1 - Give users individual logins&lt;/h3&gt;Your application probably uses a master username and password to log in to SQL. It's tempting to just give that to the people who need to go and look stuff up in the database, including developers, devops, support etc. But this means you have no way of knowing which actual person did what. Worse, when someone leaves the organisation you may not find it that easy to change the application's SQL Password.&lt;br /&gt;&lt;br /&gt;You can create individual users in SQL Azure to give to the people who need access and I urge you to do so. You may want to give users read-only access while you are it. The tooling is lacking, in that you have to create the users using T-SQL from something like SQL Server Management Studio or similar; you can’t manage the users in the Azure Portal, which is quite an oversight IMO.&lt;br /&gt;&lt;br /&gt;If you use Azure AD for authentication you can &lt;a href=&quot;https://azure.microsoft.com/en-gb/documentation/articles/sql-database-aad-authentication/&quot;&gt;add Azure AD&lt;/a&gt; users to the SQL Databases. This has the benefit that user access will be automatically revoked from all databases when the user's AD Account is disabled - probably when they leave the organisation. It also means that, as of August 2016, you can use 2 Factor Authentication with the SQL logins - just download the latest version of SQL Server Management Studio 2016 and choose the Universal authentication method.&lt;br /&gt;At &lt;a href=&quot;https://www.neworbit.co.uk/&quot;&gt;NewOrbit&lt;/a&gt;, all user access to Azure and SQL &amp;nbsp;Azure is controlled through Azure AD, through our Office 365 subscription and all users are required to have 2FA enabled.&lt;br /&gt;&lt;br /&gt;&lt;h3&gt;Step 2 - hide the sign-in details for the application&lt;/h3&gt;Once you have given users individual logins, you'll need to change the password the application uses to login to SQL. And you need to ensure it's not available to developers or others. You can do this by specifying things in the portal, if you use web apps. However, there are two other methods you may want to consider;&lt;br /&gt;&lt;br /&gt;&lt;ul&gt;&lt;li&gt;You can let your Application itself authenticate to SQL with Azure AD using a certificate - see the same link as above about using Azure AD with SQL Azure. This way there literally is no password and as long as you delete the certificate after uploading it, there is no way for anyone to log in to SQL as the application.&lt;br /&gt;&lt;/li&gt;&lt;li&gt;You could store the SQL password in Azure Key Vault.&lt;/li&gt;&lt;/ul&gt;&lt;br /&gt;&lt;br /&gt;&lt;h3&gt;Step 3 - audit all the things&lt;/h3&gt;SQL Azure has an &lt;a href=&quot;https://azure.microsoft.com/en-gb/documentation/articles/sql-database-auditing-get-started/&quot;&gt;Auditing facility&lt;/a&gt; whereby you can audit all queries made against the database. Given that you have assigned an individual user name to each of your people, you can (and should) audit exactly which queries each of your people have run - and Azure will even flag up queries that return an unusually large number of records. This is gold dust for complying with InfoSec requirements about preventing internal data leakage.&lt;br /&gt;&lt;br /&gt;&lt;h3&gt;Step 4 - use data masking&lt;/h3&gt;SQL Azure has a &lt;a href=&quot;https://azure.microsoft.com/en-gb/documentation/articles/sql-database-dynamic-data-masking-get-started/&quot;&gt;facility to &quot;mask&quot;&lt;/a&gt; certain columns, such as email addresses, phone numbers and so on. I think the original design of this feature was as an aide to applications. But actually it's really useful in complying with InfoSec requirements about limiting access to sensitive data; You can set it up so your Application has normal access to all the data, but the logins your users have can automatically mask sensitive data. This means your users can write queries as normal and even see if data exists, but sensitive details that they don't need for troubleshooting will be masked in the output.&lt;br /&gt;Do understand that this feature is not completely tamper proof; you can still search on the underlying data, so with patience you can triangulate your way to the real data - but then that will show up in the audit logs.&lt;br /&gt;&lt;br /&gt;&lt;h2&gt;Conclusion&lt;/h2&gt;When properly configured, I believe SQL Azure offers a very compelling InfoSec story, one that should be able to satisfy most security audits you are likely to encounter.&lt;br /&gt;&lt;br /&gt;</content><author><name>Frans Lytzen</name></author><category term="Security" /><category term="Azure" /><category term="SQL" /><summary type="html">I've been using SQL Azure since 2011 and it's been a journey. One of the big problems I used to have was passing security audits from some of our clients; We deal with a &amp;nbsp;lot of data that is highly sensitive so are under a lot of scrutiny to make sure it is protected.A lot of what you have to comply with when you start going into the areas of ISO27001 and PCI compliance is not just about technical security in the way we normally think about it, it’s just as much about people and processes. In my experience from filling in dozens of security questionnaires, I believe that SQL Azure can tick all the boxes, as long as you switch on the right features. Just to be clear, I'm not required to comply with PCI so can't vouch for that, but I do have to comply with a number of ISO27001 and Data Protection requirements.Box tickingMost security audits will ask you if your data is &quot;encrypted at rest&quot;. If you understand how Azure really works and where &amp;nbsp;that requirement originally comes from, you'll know that this is a meaningless requirement when you're in Azure. But, just try to convince a security auditor of that. So go ahead and switch on Transparent Data Encryption&amp;nbsp; and you can tick that box on the questionnaire.Incidentally, Azure also recently enabled you to do something similar for Blob storage - again mainly so you can tick the box.Intrusion detectionSQL Azure has a neat Threat Detection facility to monitor the use of your database and alert you to anomalous behaviour. I've seen it detect potential SQL injection attacks and allegedly it will detect &quot;unusual&quot; behaviour, though I have yet to actually see that (thankfully :)).This is &amp;nbsp;a useful feature to potentially detect both external and internal attacks (see below).You have nothing to lose by switching it on.Internal attacksAs a developer it's easy to think that once you pass penetration testing, your job is done. But, for the organisation, that's only half the battle. A proper security audit will look just as much at how you are preventing and detecting what they sometimes call &quot;Internal Data Leakage&quot;. Basically, they are worried that people inside your organisation may access the data and leak it. This may be deliberately, it may be a result of social engineering or it may be that an external agency decided to hack a person who works for you. It's the modern equivalent of sneaking in through the kitchen entrance instead of taking a battering ram to the front gates. It's hardly surprising there is a lot of focus on this, given that most of the big attacks recently seems to have come from some kind of inside job.You may well trust everyone in your organisation to not want to deliberately betray you and to be sensible about not having their laptops hacked - but an auditor won't, so you may as well buckle up and do the right thing. SQL Azure makes it surprisingly easy, though there are number of manual steps that could be made a lot easier with some better tooling, especially if you have many databases.Step 1 - Give users individual loginsYour application probably uses a master username and password to log in to SQL. It's tempting to just give that to the people who need to go and look stuff up in the database, including developers, devops, support etc. But this means you have no way of knowing which actual person did what. Worse, when someone leaves the organisation you may not find it that easy to change the application's SQL Password.You can create individual users in SQL Azure to give to the people who need access and I urge you to do so. You may want to give users read-only access while you are it. The tooling is lacking, in that you have to create the users using T-SQL from something like SQL Server Management Studio or similar; you can’t manage the users in the Azure Portal, which is quite an oversight IMO.If you use Azure AD for authentication you can add Azure AD users to the SQL Databases. This has the benefit that user access will be automatically revoked from all databases when the user's AD Account is disabled - probably when they leave the organisation. It also means that, as of August 2016, you can use 2 Factor Authentication with the SQL logins - just download the latest version of SQL Server Management Studio 2016 and choose the Universal authentication method.At NewOrbit, all user access to Azure and SQL &amp;nbsp;Azure is controlled through Azure AD, through our Office 365 subscription and all users are required to have 2FA enabled.Step 2 - hide the sign-in details for the applicationOnce you have given users individual logins, you'll need to change the password the application uses to login to SQL. And you need to ensure it's not available to developers or others. You can do this by specifying things in the portal, if you use web apps. However, there are two other methods you may want to consider;You can let your Application itself authenticate to SQL with Azure AD using a certificate - see the same link as above about using Azure AD with SQL Azure. This way there literally is no password and as long as you delete the certificate after uploading it, there is no way for anyone to log in to SQL as the application.You could store the SQL password in Azure Key Vault.Step 3 - audit all the thingsSQL Azure has an Auditing facility whereby you can audit all queries made against the database. Given that you have assigned an individual user name to each of your people, you can (and should) audit exactly which queries each of your people have run - and Azure will even flag up queries that return an unusually large number of records. This is gold dust for complying with InfoSec requirements about preventing internal data leakage.Step 4 - use data maskingSQL Azure has a facility to &quot;mask&quot; certain columns, such as email addresses, phone numbers and so on. I think the original design of this feature was as an aide to applications. But actually it's really useful in complying with InfoSec requirements about limiting access to sensitive data; You can set it up so your Application has normal access to all the data, but the logins your users have can automatically mask sensitive data. This means your users can write queries as normal and even see if data exists, but sensitive details that they don't need for troubleshooting will be masked in the output.Do understand that this feature is not completely tamper proof; you can still search on the underlying data, so with patience you can triangulate your way to the real data - but then that will show up in the audit logs.ConclusionWhen properly configured, I believe SQL Azure offers a very compelling InfoSec story, one that should be able to satisfy most security audits you are likely to encounter.</summary></entry></feed>